{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Libero","metadata":{}},{"cell_type":"code","source":"import sys\nfrom unittest.mock import MagicMock\n\n# The Patch\nmock_mpl = MagicMock()\nsys.modules[\"matplotlib\"] = mock_mpl\nsys.modules[\"matplotlib.pyplot\"] = mock_mpl\nsys.modules[\"matplotlib.cm\"] = mock_mpl\nsys.modules[\"matplotlib.colors\"] = mock_mpl\nsys.modules[\"matplotlib.transforms\"] = mock_mpl\nsys.modules[\"matplotlib.ticker\"] = mock_mpl\nsys.modules[\"matplotlib._path\"] = mock_mpl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T16:33:35.591951Z","iopub.execute_input":"2025-12-03T16:33:35.592201Z","iopub.status.idle":"2025-12-03T16:33:35.609241Z","shell.execute_reply.started":"2025-12-03T16:33:35.592181Z","shell.execute_reply":"2025-12-03T16:33:35.608601Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# # Install compatible versions to avoid the \"API version 0x10\" error\n# !pip uninstall -y numpy\n# !pip install \"numpy>=1.24,<2.0\"\n# !pip install robosuite libero\n# # Force a restart of the python process after install\n# import os\n# os.kill(os.getpid(), 9)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T16:33:35.611114Z","iopub.execute_input":"2025-12-03T16:33:35.611747Z","iopub.status.idle":"2025-12-03T16:33:35.615324Z","shell.execute_reply.started":"2025-12-03T16:33:35.611729Z","shell.execute_reply":"2025-12-03T16:33:35.614579Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# # 1. Install the correct NumPy version to fix the \"0x10 vs 0xf\" error\n# !pip uninstall -y numpy\n# !pip install \"numpy>=1.24,<2.0\" \n\n# # 2. Re-install Robosuite/Libero to ensure they link to the new NumPy\n# !pip install robosuite libero\n\n# print(\"‚úÖ INSTALLATION DONE. NOW PLEASE RESTART THE KERNEL.\")\n# print(\"Menu -> Run -> Restart Kernel (or the circular arrow icon)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T16:33:35.616141Z","iopub.execute_input":"2025-12-03T16:33:35.616437Z","iopub.status.idle":"2025-12-03T16:33:35.630026Z","shell.execute_reply.started":"2025-12-03T16:33:35.616421Z","shell.execute_reply":"2025-12-03T16:33:35.629247Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!git clone https://github.com/Lifelong-Robot-Learning/LIBERO.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T16:33:35.630764Z","iopub.execute_input":"2025-12-03T16:33:35.630942Z","iopub.status.idle":"2025-12-03T16:33:47.216930Z","shell.execute_reply.started":"2025-12-03T16:33:35.630928Z","shell.execute_reply":"2025-12-03T16:33:47.216047Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'LIBERO'...\nremote: Enumerating objects: 1788, done.\u001b[K\nremote: Total 1788 (delta 0), reused 0 (delta 0), pack-reused 1788 (from 1)\u001b[K\nReceiving objects: 100% (1788/1788), 315.99 MiB | 41.90 MiB/s, done.\nResolving deltas: 100% (745/745), done.\nUpdating files: 100% (1116/1116), done.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# !pip install \"numpy>=1.24,<2.0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T16:33:47.218005Z","iopub.execute_input":"2025-12-03T16:33:47.218288Z","iopub.status.idle":"2025-12-03T16:33:47.222227Z","shell.execute_reply.started":"2025-12-03T16:33:47.218248Z","shell.execute_reply":"2025-12-03T16:33:47.221442Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"!pip install matplotlib\n!pip install tokenizers==0.15.2\n!pip install optuna","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T16:33:47.222993Z","iopub.execute_input":"2025-12-03T16:33:47.223217Z","iopub.status.idle":"2025-12-03T16:33:58.517213Z","shell.execute_reply.started":"2025-12-03T16:33:47.223201Z","shell.execute_reply":"2025-12-03T16:33:58.516347Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->matplotlib) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20->matplotlib) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20->matplotlib) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20->matplotlib) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.20->matplotlib) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.20->matplotlib) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.20->matplotlib) (2024.2.0)\nCollecting tokenizers==0.15.2\n  Downloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers==0.15.2) (0.36.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers==0.15.2) (3.20.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers==0.15.2) (2025.10.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers==0.15.2) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers==0.15.2) (6.0.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers==0.15.2) (2.32.5)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers==0.15.2) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers==0.15.2) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers==0.15.2) (1.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers==0.15.2) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers==0.15.2) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers==0.15.2) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers==0.15.2) (2025.10.5)\nDownloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.2\n    Uninstalling tokenizers-0.21.2:\n      Successfully uninstalled tokenizers-0.21.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntransformers 4.53.3 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.15.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed tokenizers-0.15.2\nRequirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.5.0)\nRequirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.17.1)\nRequirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.10.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (25.0)\nRequirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.3)\nRequirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\nRequirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->optuna) (2.4.1)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optuna) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optuna) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optuna) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->optuna) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->optuna) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->optuna) (2024.2.0)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!pip install -r <(grep -v -e \"matplotlib\" -e \"tokenizers\" /kaggle/working/LIBERO/requirements.txt) --no-deps","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T16:33:58.519325Z","iopub.execute_input":"2025-12-03T16:33:58.519702Z","iopub.status.idle":"2025-12-03T16:38:03.415546Z","shell.execute_reply.started":"2025-12-03T16:33:58.519675Z","shell.execute_reply":"2025-12-03T16:38:03.414863Z"}},"outputs":[{"name":"stdout","text":"Collecting hydra-core==1.2.0 (from -r /dev/fd/63 (line 1))\n  Downloading hydra_core-1.2.0-py3-none-any.whl.metadata (4.0 kB)\nCollecting numpy==1.22.4 (from -r /dev/fd/63 (line 2))\n  Downloading numpy-1.22.4.zip (11.5 MB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m127.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nCollecting wandb==0.13.1 (from -r /dev/fd/63 (line 3))\n  Downloading wandb-0.13.1-py2.py3-none-any.whl.metadata (7.4 kB)\nCollecting easydict==1.9 (from -r /dev/fd/63 (line 4))\n  Downloading easydict-1.9.tar.gz (6.4 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting transformers==4.21.1 (from -r /dev/fd/63 (line 5))\n  Downloading transformers-4.21.1-py3-none-any.whl.metadata (81 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting opencv-python==4.6.0.66 (from -r /dev/fd/63 (line 6))\n  Downloading opencv_python-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nCollecting robomimic==0.2.0 (from -r /dev/fd/63 (line 7))\n  Downloading robomimic-0.2.0.tar.gz (192 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m192.9/192.9 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting einops==0.4.1 (from -r /dev/fd/63 (line 8))\n  Downloading einops-0.4.1-py3-none-any.whl.metadata (10 kB)\nCollecting thop==0.1.1-2209072238 (from -r /dev/fd/63 (line 9))\n  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\nCollecting robosuite==1.4.0 (from -r /dev/fd/63 (line 10))\n  Downloading robosuite-1.4.0-py3-none-any.whl.metadata (5.6 kB)\nCollecting bddl==1.0.1 (from -r /dev/fd/63 (line 11))\n  Downloading bddl-1.0.1.tar.gz (164 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting future==0.18.2 (from -r /dev/fd/63 (line 12))\n  Downloading future-0.18.2.tar.gz (829 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m829.2/829.2 kB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting cloudpickle==2.1.0 (from -r /dev/fd/63 (line 13))\n  Downloading cloudpickle-2.1.0-py3-none-any.whl.metadata (6.9 kB)\nRequirement already satisfied: gym==0.25.2 in /usr/local/lib/python3.11/dist-packages (from -r /dev/fd/63 (line 14)) (0.25.2)\nDownloading hydra_core-1.2.0-py3-none-any.whl (151 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m151.1/151.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading wandb-0.13.1-py2.py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.21.1-py3-none-any.whl (4.7 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading opencv_python-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.9 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m60.9/60.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading einops-0.4.1-py3-none-any.whl (28 kB)\nDownloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\nDownloading robosuite-1.4.0-py3-none-any.whl (193.5 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m193.5/193.5 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading cloudpickle-2.1.0-py3-none-any.whl (25 kB)\nBuilding wheels for collected packages: numpy, easydict, robomimic, bddl, future\n  Building wheel for numpy (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for numpy: filename=numpy-1.22.4-cp311-cp311-linux_x86_64.whl size=17329501 sha256=36de5e31abb76346e74566ca4bee2918d20f26364c6fc12d8169c04520f26e5d\n  Stored in directory: /root/.cache/pip/wheels/8e/c0/7e/1583fa989ccf57e2059824c8783691f4927f2ce7b77cec9da2\n  Building wheel for easydict (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for easydict: filename=easydict-1.9-py3-none-any.whl size=6344 sha256=420c96c92670a821b1ee8ec88aa9d8c85858c3f8e4e0424feae1bd7b12287a81\n  Stored in directory: /root/.cache/pip/wheels/37/0d/a4/4a224efd03ae021fd8b33cf88d900d817aec7b2a982950933c\n  Building wheel for robomimic (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for robomimic: filename=robomimic-0.2.0-py3-none-any.whl size=223233 sha256=ddbb0f8db0c6f8af0ba88bf97876b22b911a586dc35ad5b61187fe6144b4378f\n  Stored in directory: /root/.cache/pip/wheels/72/24/cb/5d6150ab34f5cd6d37b20202e80461812dd0ee64a507c08d75\n  Building wheel for bddl (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for bddl: filename=bddl-1.0.1-py3-none-any.whl size=212350 sha256=331d209bd760ddb17b2f467f8c62aba8a8e9a53ba924fe94fc63f4662935652a\n  Stored in directory: /root/.cache/pip/wheels/a6/b7/78/1289e10725481b4ce866c296282d9f7edc16a23a37561e7a09\n  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491059 sha256=94a34c13292d9634b938b613294869976bfedd91cc096dc3079beaac5178437c\n  Stored in directory: /root/.cache/pip/wheels/ba/95/72/3a10afedafaa998b0ac6fc6df9ea553b0c419af420cff21347\nSuccessfully built numpy easydict robomimic bddl future\nInstalling collected packages: thop, hydra-core, einops, easydict, bddl, wandb, transformers, robosuite, robomimic, opencv-python, numpy, future, cloudpickle\n  Attempting uninstall: einops\n    Found existing installation: einops 0.8.1\n    Uninstalling einops-0.8.1:\n      Successfully uninstalled einops-0.8.1\n  Attempting uninstall: easydict\n    Found existing installation: easydict 1.13\n    Uninstalling easydict-1.13:\n      Successfully uninstalled easydict-1.13\n  Attempting uninstall: wandb\n    Found existing installation: wandb 0.21.0\n    Uninstalling wandb-0.21.0:\n      Successfully uninstalled wandb-0.21.0\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.53.3\n    Uninstalling transformers-4.53.3:\n      Successfully uninstalled transformers-4.53.3\n  Attempting uninstall: opencv-python\n    Found existing installation: opencv-python 4.12.0.88\n    Uninstalling opencv-python-4.12.0.88:\n      Successfully uninstalled opencv-python-4.12.0.88\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n  Attempting uninstall: future\n    Found existing installation: future 1.0.0\n    Uninstalling future-1.0.0:\n      Successfully uninstalled future-1.0.0\n  Attempting uninstall: cloudpickle\n    Found existing installation: cloudpickle 3.1.2\n    Uninstalling cloudpickle-3.1.2:\n      Successfully uninstalled cloudpickle-3.1.2\nSuccessfully installed bddl-1.0.1 cloudpickle-2.1.0 easydict-1.9 einops-0.4.1 future-0.18.2 hydra-core-1.2.0 numpy-1.22.4 opencv-python-4.6.0.66 robomimic-0.2.0 robosuite-1.4.0 thop-0.1.1.post2209072238 transformers-4.21.1 wandb-0.13.1\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!echo n | python /kaggle/working/LIBERO/benchmark_scripts/download_libero_datasets.py --datasets libero_spatial --use-huggingface > /dev/null 2>&1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T17:47:03.673084Z","iopub.execute_input":"2025-12-03T17:47:03.673379Z","iopub.status.idle":"2025-12-03T17:47:03.881136Z","shell.execute_reply.started":"2025-12-03T17:47:03.673354Z","shell.execute_reply":"2025-12-03T17:47:03.880133Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!mkdir /kaggle/working/dataset | mv /kaggle/working/LIBERO/libero/datasets/* /kaggle/working/dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T17:05:04.859567Z","iopub.execute_input":"2025-12-03T17:05:04.860416Z","iopub.status.idle":"2025-12-03T17:05:04.978458Z","shell.execute_reply.started":"2025-12-03T17:05:04.860335Z","shell.execute_reply":"2025-12-03T17:05:04.977511Z"}},"outputs":[{"name":"stdout","text":"mkdir: cannot create directory ‚Äò/kaggle/working/dataset‚Äô: File exists\nmv: cannot stat '/kaggle/working/LIBERO/libero/datasets/*': No such file or directory\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# import h5py\n# import matplotlib.pyplot as plt\n# import torch.multiprocessing as mp\n\n\n# file_path = \"/kaggle/working/dataset/libero_spatial/pick_up_the_black_bowl_on_the_cookie_box_and_place_it_on_the_plate_demo.hdf5\"\n\n# def print_hdf5_contents(name, obj):\n#     \"\"\"\n#     Stampa nome, tipo e forma di ogni elemento HDF5\n#     \"\"\"\n#     if isinstance(obj, h5py.Dataset):\n#         print(f\"Dataset: {name}, shape: {obj.shape}, dtype: {obj.dtype}\")\n#     elif isinstance(obj, h5py.Group):\n#         print(f\"Group: {name}\")\n\n# '''with h5py.File(file_path, \"r\") as f:\n#     f.visititems(print_hdf5_contents)'''\n\n# file_path = \"/kaggle/working/dataset/libero_spatial/pick_up_the_black_bowl_from_table_center_and_place_it_on_the_plate_demo.hdf5\"\n\n# with h5py.File(file_path, \"r\") as f:\n#     # Cicla su tutti i demo presenti\n#     for demo_name in f[\"data\"]:\n#         print(f\"Demo: {demo_name}\")\n#         images = f[f\"data/{demo_name}/obs/agentview_rgb\"]\n#         num_images = images.shape[0]\n        \n#         # Prendi immagini ogni 15 timestep\n#         indices = list(range(0, num_images, 15))\n        \n#         # Assicurati che l'ultima immagine sia inclusa\n#         if num_images - 1 not in indices:\n#             indices.append(num_images - 1)\n        \n#         fig, axes = plt.subplots(1, len(indices), figsize=(15, 3))\n        \n#         for ax, idx in zip(axes, indices):\n#             ax.imshow(images[idx])\n#             ax.axis('off')\n#             ax.set_title(f\"Timestep {idx}\")\n        \n#         plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import h5py\nimport numpy as np\nfrom PIL import Image\nfrom IPython.display import display, HTML\n\n# Path to your demo file\nfile_path = \"/kaggle/working/dataset/libero_spatial/pick_up_the_black_bowl_from_table_center_and_place_it_on_the_plate_demo.hdf5\"\n\nprint(f\"Opening file: {file_path}\")\n\ntry:\n    with h5py.File(file_path, \"r\") as f:\n        # Loop through all demos in the file\n        for demo_name in f[\"data\"]:\n            print(f\"\\n=== Demo: {demo_name} ===\")\n            \n            # Access the image data (AgentView RGB)\n            # Note: Ensure this path exists in your specific HDF5 structure\n            if \"obs/agentview_rgb\" in f[f\"data/{demo_name}\"]:\n                dataset = f[f\"data/{demo_name}/obs/agentview_rgb\"]\n                num_images = dataset.shape[0]\n                print(f\"Total frames: {num_images}\")\n                \n                # Pick indices: every 15th frame + the last one\n                indices = list(range(0, num_images, 15))\n                if num_images - 1 not in indices:\n                    indices.append(num_images - 1)\n                \n                # Display images horizontally using HTML/PIL (No Matplotlib)\n                images_html = []\n                for idx in indices:\n                    img_array = dataset[idx]\n                    \n                    # Convert numpy array to PIL Image\n                    # (Robosuite images are usually already correct, but sometimes flipped)\n                    img = Image.fromarray(img_array)\n                    \n                    # Resize for smaller display if needed\n                    img_small = img.resize((128, 128)) \n                    \n                    # Hack to display inline in loop\n                    print(f\"Frame {idx}:\")\n                    display(img)\n            else:\n                print(f\"Skipping {demo_name}: 'obs/agentview_rgb' not found.\")\n                \nexcept Exception as e:\n    print(f\"An error occurred: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Libraries","metadata":{}},{"cell_type":"code","source":"!pip install mujoco==2.3.7\n!pip install --force-reinstall numba==0.56.4 llvmlite==0.39.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Installa Einops: Indispensabile per manipolare tensori (reshape, permute)\n# in modo leggibile dentro il tuo blocco ricorsivo.\n!pip install einops\n\n# Installa WandB (Weights & Biases): Standard de-facto per tracciare\n# i grafici di loss e accuracy durante il training.\n!pip install wandb\n\n# Installa Transformers: Servir√† per scaricare CLIP o BERT\n# quando dovrai creare gli embedding del linguaggio (per LIBERO-Goal).\n!pip install transformers\n\n# Installa Robomimic (Opzionale ma consigliato):\n# Contiene molte utility per processare i file HDF5 di robosuite/libero\n# senza dover riscrivere tutto da zero.\n!pip install robomimic","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport os\nimport torch.nn as nn\nimport numpy as np\nimport h5py\nimport einops\nfrom torchvision import models\nfrom torchvision.models import resnet18, ResNet18_Weights\nfrom torchvision import transforms\n\n# Verifica disponibilit√† GPU\nprint(f\"PyTorch Version: {torch.__version__}\")\nif torch.cuda.is_available():\n    print(f\"‚úÖ GPU Disponibile: {torch.cuda.get_device_name(0)}\")\n    device = torch.device(\"cuda\")\nelse:\n    print(\"‚ö†Ô∏è ATTENZIONE: GPU non rilevata. Vai su 'Settings' > 'Accelerator' e seleziona GPU P100 o T4.\")\n    device = torch.device(\"cpu\")\n\n# Verifica Einops (lo useremo molto nel modello ricorsivo)\nprint(f\"Einops installato correttamente.\")\n\n# Configurazione base per riproducibilit√† (importante per la tesi)\ndef set_seed(seed=42):\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed(42)\nprint(\"‚úÖ Ambiente pronto per il modello TinyRecursive.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport h5py\n# import matplotlib.pyplot as plt\nfrom pathlib import Path\nimport cv2\nfrom typing import Dict, List, Tuple, Optional, Any\nfrom dataclasses import dataclass, asdict, replace\nimport json\nimport wandb\nimport optuna\nfrom datetime import datetime\nfrom transformers import CLIPTokenizer, CLIPTextModel\n\n# Setup device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Seed per riproducibilit√†\ndef set_seed(seed=42):\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed(42)\n\n# =========================\n# Configuration utilities\n# =========================\n\n@dataclass\nclass TrainingConfig:\n    \"\"\"Container strutturato per gli iperparametri di training/model.\"\"\"\n\n    lr: float = 3e-4\n    hidden_dim: int = 256\n    num_recursions: int = 8\n    epochs: int = 20\n    batch_size: int = 64\n    weight_decay: float = 1e-4\n    grad_clip: Optional[float] = 1.0\n    sched_T0: Optional[int] = None\n    sched_T_mult: int = 1\n    lr_min: float = 1e-6\n    warmup_epochs: int = 3\n    early_stop_patience: Optional[int] = None\n    save_path: str = 'best_model.pt'\n    use_pretrained_encoder: bool = True\n    freeze_backbone: bool = True\n    augmentation: bool = False\n    dropout: float = 0.1\n    encoder_dropout: float = 0.1\n    use_text_prompts: bool = True\n    text_encoder_name: str = 'openai/clip-vit-large-patch14'\n    train_text_encoder: bool = False\n    text_dropout: float = 0.1\n\n    def to_dict(self) -> Dict[str, Any]:\n        return asdict(self)\n\n    def label(self) -> str:\n        return f\"lr{self.lr}_h{self.hidden_dim}_rec{self.num_recursions}_bs{self.batch_size}\"\n\n\n@dataclass\nclass HyperparameterSearchSpace:\n    lr: List[float]\n    hidden_dim: List[int]\n    num_recursions: List[int]\n    batch_size: List[int]\n    weight_decay: List[float]\n    pretrained_encoder: List[bool]\n    freeze_backbone: List[bool]\n    augmentation: List[bool]\n    dropout: List[float]\n\n    def as_optuna_space(self) -> Dict[str, List[Any]]:\n        return {\n            'lr': self.lr,\n            'hidden_dim': self.hidden_dim,\n            'num_recursions': self.num_recursions,\n            'batch_size': self.batch_size,\n            'weight_decay': self.weight_decay,\n            'pretrained_encoder': self.pretrained_encoder,\n            'freeze_backbone': self.freeze_backbone,\n            'augmentation': self.augmentation,\n            'dropout': self.dropout,\n        }\n\n# Numero 1\ndef default_search_space() -> HyperparameterSearchSpace:\n    \"\"\"Restituisce lo spazio di ricerca richiesto dall'utente.\"\"\"\n\n    return HyperparameterSearchSpace(\n        lr=[1e-5, 1e-6, 1e-4, 1e-3],\n        hidden_dim=[128, 256, 512],\n        num_recursions=[8, 12, 16],\n        batch_size=[32, 64, 128, 256, 512],\n        weight_decay=[0.1, 0.5, 1.0],\n        pretrained_encoder=[False],\n        freeze_backbone=[False],\n        augmentation=[True, False],\n        dropout=[0.1, 0.3, 0.5, 0.7],\n    )\n#Numero 2\n'''\ndef default_search_space() -> HyperparameterSearchSpace:\n    \"\"\"Restituisce lo spazio di ricerca richiesto dall'utente.\"\"\"\n\n    return HyperparameterSearchSpace(\n        lr=[1e-5, 1e-6, 1e-4, 1e-3],\n        hidden_dim=[128, 256, 512],\n        num_recursions=[8, 12, 16],\n        batch_size=[32, 64, 128, 256, 512],\n        weight_decay=[0.1, 0.5, 1.0],\n        pretrained_encoder=[False],\n        freeze_backbone=[True],\n        augmentation=[True, False],\n        dropout=[0.1, 0.3, 0.5, 0.7],\n    )\n'''\n\n# Numero 3\n'''\ndef default_search_space() -> HyperparameterSearchSpace:\n    \"\"\"Restituisce lo spazio di ricerca richiesto dall'utente.\"\"\"\n\n    return HyperparameterSearchSpace(\n        lr=[1e-5, 1e-6, 1e-4, 1e-3],\n        hidden_dim=[128, 256, 512],\n        num_recursions=[8, 12, 16],\n        batch_size=[32, 64, 128, 256, 512],\n        weight_decay=[0.1, 0.5, 1.0],\n        pretrained_encoder=[True],\n        freeze_backbone=[False],\n        augmentation=[True, False],\n        dropout=[0.1, 0.3, 0.5, 0.7],\n    )\n'''\n\n# Numero 4\n'''\ndef default_search_space() -> HyperparameterSearchSpace:\n    \"\"\"Restituisce lo spazio di ricerca richiesto dall'utente.\"\"\"\n\n    return HyperparameterSearchSpace(\n        lr=[1e-5, 1e-6, 1e-4, 1e-3],\n        hidden_dim=[128, 256, 512],\n        num_recursions=[8, 12, 16],\n        batch_size=[32, 64, 128, 256, 512],\n        weight_decay=[0.1, 0.5, 1.0],\n        pretrained_encoder=[True],\n        freeze_backbone=[True],\n        augmentation=[True, False],\n        dropout=[0.1, 0.3, 0.5, 0.7],\n    )\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport h5py\nimport sys\nfrom pathlib import Path\nfrom PIL import Image\nfrom IPython.display import display\n\n# --- HELPER FUNCTIONS (Kept largely the same) ---\n\ndef load_images_robust(dataset):\n    \"\"\"\n    Carica immagini da dataset HDF5 usando metodo robusto.\n    \"\"\"\n    shape = dataset.shape\n    \n    # METODO 1: Lettura diretta uint8\n    try:\n        buffer = np.empty(shape, dtype=np.uint8)\n        dataset.read_direct(buffer)\n        return buffer\n    except Exception:\n        pass\n    \n    # METODO 2: Float32 -> Uint8\n    try:\n        buffer = np.empty(shape, dtype=np.float32)\n        dataset.read_direct(buffer)\n        if buffer.max() <= 1.0:\n            buffer = (buffer * 255).astype(np.uint8)\n        else:\n            buffer = np.clip(buffer, 0, 255).astype(np.uint8)\n        return buffer\n    except Exception:\n        pass\n    \n    # METODO 3: Float64 -> Uint8\n    try:\n        buffer = np.empty(shape, dtype=np.float64)\n        dataset.read_direct(buffer)\n        if buffer.max() <= 1.0:\n            buffer = (buffer * 255).astype(np.uint8)\n        else:\n            buffer = np.clip(buffer, 0, 255).astype(np.uint8)\n        return buffer\n    except Exception:\n        pass\n\n    # METODO 4: Fallback bytes\n    try:\n        buffer = np.empty(shape, dtype=np.uint8)\n        dataset.id.read(h5py.h5s.ALL, h5py.h5s.ALL, buffer)\n        return buffer\n    except Exception as e:\n        raise RuntimeError(f\"Impossibile leggere il dataset: {e}\")\n\ndef load_actions_robust(dataset):\n    \"\"\"\n    Carica azioni da dataset HDF5.\n    \"\"\"\n    shape = dataset.shape\n    try:\n        buffer = np.empty(shape, dtype=np.float32)\n        dataset.read_direct(buffer)\n        return buffer\n    except Exception:\n        pass\n    \n    try:\n        buffer = np.empty(shape, dtype=np.float64)\n        dataset.read_direct(buffer)\n        return buffer.astype(np.float32)\n    except Exception as e:\n        raise RuntimeError(f\"Impossibile leggere le azioni: {e}\")\n\n# --- MODIFIED EXPLORATION FUNCTION (No Matplotlib) ---\n\ndef explore_libero_dataset(data_path: Path):\n    \n    # Trova file\n    hdf5_files = list(data_path.glob('**/*.hdf5'))\n    \n    if not hdf5_files:\n        print(f\"‚ö†Ô∏è Nessun file HDF5 trovato in {data_path}\")\n        return []\n    \n    print(f\"‚úÖ Trovati {len(hdf5_files)} file HDF5\")\n    \n    # Analizza il primo file\n    demo_file = hdf5_files[0]\n    print(f\"\\nüìÑ Analizzando: {demo_file.name}\")\n    \n    try:\n        with h5py.File(demo_file, 'r') as f:\n            if 'data' not in f:\n                print(\"‚ö†Ô∏è Chiave 'data' non trovata\")\n                return hdf5_files\n            \n            data_group = f['data']\n            demo_keys = list(data_group.keys())\n            first_demo_key = demo_keys[0]\n            demo_0 = data_group[first_demo_key]\n            \n            imgs = None\n            \n            # 1. Caricamento Immagini\n            if 'obs' in demo_0:\n                obs_group = demo_0['obs']\n                \n                # Strategia di ricerca chiave immagine\n                image_keys = ['agentview_rgb', 'agentview_image', 'rgb', 'image', 'robot0_eye_in_hand_image']\n                img_key = next((k for k in image_keys if k in obs_group), None)\n                \n                # Fallback ricerca generica\n                if img_key is None:\n                    img_key = next((k for k in obs_group.keys() if 'rgb' in k.lower() or 'image' in k.lower()), None)\n                \n                if img_key:\n                    print(f\"\\nüñºÔ∏è Usando chiave immagini: '{img_key}'\")\n                    try:\n                        imgs = load_images_robust(obs_group[img_key])\n                        print(f\"  ‚úÖ Immagini caricate: {imgs.shape}\")\n                    except Exception as e:\n                        print(f\"  ‚ùå Errore immagini: {e}\")\n            \n            # 2. Caricamento Azioni\n            if 'actions' in demo_0:\n                try:\n                    actions = load_actions_robust(demo_0['actions'])\n                    print(f\"\\nüéÆ Azioni caricate: {actions.shape}\")\n                    print(f\"  Range: [{actions.min():.3f}, {actions.max():.3f}]\")\n                except Exception as e:\n                    print(f\"  ‚ùå Errore azioni: {e}\")\n\n            # 3. VISUALIZZAZIONE (Senza Matplotlib)\n            if imgs is not None and len(imgs) > 0:\n                print(\"\\nüé¨ Visualizzazione frame esempio (PIL/IPython):\")\n                \n                num_frames = min(4, len(imgs))\n                indices = np.linspace(0, len(imgs) - 1, num_frames, dtype=int)\n                \n                for idx in indices:\n                    img_array = imgs[idx]\n                    \n                    # Se l'immagine √® float [0,1], converti a uint8\n                    if img_array.dtype != np.uint8:\n                         img_array = (np.clip(img_array, 0, 1) * 255).astype(np.uint8)\n                    \n                    # Crea immagine PIL\n                    pil_img = Image.fromarray(img_array)\n                    \n                    # (Opzionale) Resize per non occupare troppo spazio\n                    # pil_img = pil_img.resize((128, 128))\n                    \n                    print(f\"--- Frame {idx} ---\")\n                    display(pil_img)\n            else:\n                print(\"\\n‚ö†Ô∏è Nessuna immagine valida da visualizzare\")\n\n    except Exception as e:\n        print(f\"Errore critico durante l'apertura del file: {e}\")\n    \n    return hdf5_files\n\n# Esegui\nhdf5_files = explore_libero_dataset(Path('/kaggle/working/dataset/libero_spatial'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LIBERODataset(Dataset):\n    \"\"\"\n    Dataset per dimostrazioni LIBERO\n    \n    Carica osservazioni visive e azioni da file HDF5.\n    Supporta data augmentation e normalizzazione.\n    Gestisce automaticamente problemi di dtype non standard nei file HDF5.\n    \n    Supporta demo-level split: invece di dividere i file, divide le demo\n    all'interno di ciascun file (es. 80% train, 20% val per ogni file).\n    \"\"\"\n    \n    def __init__(\n        self,\n        hdf5_files: List[Path],\n        sequence_length: int = 1,\n        image_size: Tuple[int, int] = (128, 128),\n        normalize_actions: bool = True,\n        augmentation: bool = False,\n        max_demos_per_task: Optional[int] = None,\n        demo_split_ratio: float = 0.8,\n        is_train: bool = True,\n        action_stats: Optional[Dict] = None\n    ):\n        \"\"\"\n        Args:\n            hdf5_files: lista di path ai file HDF5\n            sequence_length: lunghezza delle sequenze (1 = single-step prediction)\n            image_size: dimensioni delle immagini\n            normalize_actions: se True, normalizza le azioni con z-score\n            augmentation: se True, applica data augmentation\n            max_demos_per_task: limite massimo di demo per task (per debugging)\n            demo_split_ratio: percentuale di demo per training (default 0.8 = 80%)\n            is_train: se True, usa le prime demo_split_ratio% demo; se False, usa il resto\n            action_stats: statistiche azioni pre-calcolate (per validation set)\n        \"\"\"\n        self.hdf5_files = hdf5_files\n        self.sequence_length = sequence_length\n        self.image_size = (int(image_size[0]), int(image_size[1]))\n        self.augmentation = augmentation and is_train\n        self.normalize_actions = normalize_actions\n        self.demo_split_ratio = demo_split_ratio\n        self.is_train = is_train\n        \n        # Carica tutti i dati in memoria (assumendo dataset gestibile)\n        self.data = []\n        self.action_stats = action_stats if action_stats is not None else {'mean': None, 'std': None}\n        self.samples: List[Tuple[int, int]] = []  # (demo_idx, start_idx)\n        \n        split_name = \"TRAIN\" if is_train else \"VAL\"\n        print(f\"Loading {len(hdf5_files)} HDF5 files for {split_name} (demo split: {demo_split_ratio:.0%})...\")\n        all_actions = []\n        \n        for hdf5_file in hdf5_files:\n            try:\n                with h5py.File(hdf5_file, 'r') as f:\n                    if 'data' not in f:\n                        print(f\"‚ö†Ô∏è 'data' key not found in {hdf5_file.name}, skipping...\")\n                        continue\n                    \n                    demo_keys = list(f['data'].keys())\n                    \n                    # Limita numero di demo se richiesto\n                    if max_demos_per_task is not None:\n                        demo_keys = demo_keys[:max_demos_per_task]\n                    \n                    # Demo-level split: seleziona subset di demo in base a is_train\n                    n_demos = len(demo_keys)\n                    n_train_demos = int(n_demos * demo_split_ratio)\n                    \n                    if is_train:\n                        # Training: prime n_train_demos demo\n                        selected_demo_keys = demo_keys[:n_train_demos]\n                    else:\n                        # Validation: demo rimanenti\n                        selected_demo_keys = demo_keys[n_train_demos:]\n                    \n                    if len(selected_demo_keys) == 0:\n                        print(f\"‚ö†Ô∏è No demos selected from {hdf5_file.name} for {split_name}, skipping...\")\n                        continue\n                    \n                    task_prompt = self._prompt_from_filename(hdf5_file)\n\n                    for demo_key in selected_demo_keys:\n                        try:\n                            demo = f[f'data/{demo_key}']\n                            \n                            # Trova la chiave delle immagini\n                            obs_group = demo['obs']\n                            img_key = self._find_image_key(obs_group)\n                            \n                            if img_key is None:\n                                print(f\"‚ö†Ô∏è No image key found in {hdf5_file.name}/{demo_key}, skipping...\")\n                                continue\n                            \n                            # Carica osservazioni con metodo robusto\n                            obs = self._load_images_robust(obs_group[img_key])\n                            \n                            # Carica azioni con metodo robusto\n                            actions = self._load_actions_robust(demo['actions'])\n                            \n                            # Verifica che obs e actions abbiano lunghezze compatibili\n                            min_len = min(len(obs), len(actions))\n                            if min_len < self.sequence_length:\n                                print(f\"‚ö†Ô∏è Demo too short ({min_len} < {self.sequence_length}), skipping...\")\n                                continue\n                            \n                            obs = obs[:min_len]\n                            actions = actions[:min_len]\n                            \n                            # Aggiungi alla lista\n                            self.data.append({\n                                'observations': obs,\n                                'actions': actions,\n                                'prompt': task_prompt\n                            })\n                            \n                            all_actions.append(actions)\n                            \n                        except Exception as e:\n                            print(f\"‚ö†Ô∏è Error loading demo {demo_key} from {hdf5_file.name}: {e}\")\n                            continue\n                            \n            except Exception as e:\n                print(f\"‚ùå Error opening file {hdf5_file}: {e}\")\n                continue\n        \n        print(f\"‚úÖ Loaded {len(self.data)} demonstrations for {split_name}\")\n        \n        if len(self.data) == 0:\n            raise ValueError(f\"No valid demonstrations loaded for {split_name}! Check your data files.\")\n        \n        # Calcola statistiche azioni per normalizzazione (solo per training set o se non fornite)\n        if self.normalize_actions and len(all_actions) > 0 and action_stats is None:\n            all_actions_concat = np.concatenate(all_actions, axis=0)\n        \n            mean = all_actions_concat.mean(axis=0).astype(np.float32)\n            std  = all_actions_concat.std(axis=0).astype(np.float32)\n        \n            # ‚ö†Ô∏è Floor di sicurezza: evita std troppo piccole che esplodono la normalizzazione\n            std_clipped = np.clip(std, 0.1, None)\n        \n            # Log dettagliato\n            print(f\"üìä Action statistics computed from {split_name} set:\")\n            print(f\"   Mean:        {np.round(mean, 3)}\")\n            print(f\"   Std (raw):   {np.round(std, 3)}\")\n            print(f\"   Std (clipped to >=0.1): {np.round(std_clipped, 3)}\")\n        \n            self.action_stats['mean'] = mean\n            self.action_stats['std']  = std_clipped\n        \n        elif action_stats is not None:\n            print(f\"üìä Using provided action statistics\")\n            self.action_stats = {\n                'mean': action_stats['mean'].astype(np.float32),\n                'std':  np.clip(action_stats['std'], 0.1, None).astype(np.float32)\n            }\n\n        # Costruisci indice delle transizioni per accesso O(1)\n        self.samples = self._build_sample_index()\n        print(f\"üì¶ Generated {len(self.samples)} transitions for {split_name}\")\n\n    @staticmethod\n    def _prompt_from_filename(hdf5_file: Path) -> str:\n        \"\"\"Converte il nome del file HDF5 in un prompt naturale.\"\"\"\n        name = hdf5_file.stem\n        if name.endswith('_demo'):\n            name = name[:-5]\n        name = name.replace('_', ' ').replace('-', ' ')\n        return ' '.join(name.split()).strip()\n\n    \n    def _find_image_key(self, obs_group) -> Optional[str]:\n        \"\"\"Trova la chiave corretta per le immagini nel gruppo obs\"\"\"\n        # Lista di possibili chiavi per le immagini (in ordine di priorit√†)\n        possible_keys = [\n            'agentview_rgb',\n            'agentview_image', \n            'rgb',\n            'image',\n            'robot0_eye_in_hand_image',\n            'frontview_image',\n            'sideview_image'\n        ]\n        \n        obs_keys = list(obs_group.keys())\n        \n        # Prima cerca chiavi note\n        for key in possible_keys:\n            if key in obs_keys:\n                return key\n        \n        # Poi cerca qualsiasi chiave che contiene 'rgb' o 'image'\n        for key in obs_keys:\n            if 'rgb' in key.lower() or 'image' in key.lower():\n                return key\n        \n        return None\n    \n    def _load_images_robust(self, dataset) -> np.ndarray:\n        \"\"\"\n        Carica immagini da dataset HDF5 usando metodo robusto che bypassa problemi dtype.\n        \"\"\"\n        shape = dataset.shape\n        \n        # METODO 1: Prova lettura diretta come uint8\n        try:\n            buffer = np.empty(shape, dtype=np.uint8)\n            dataset.read_direct(buffer)\n            return buffer\n        except Exception:\n            pass\n        \n        # METODO 2: Prova come float32 e converti\n        try:\n            buffer = np.empty(shape, dtype=np.float32)\n            dataset.read_direct(buffer)\n            if buffer.max() <= 1.0:\n                buffer = (buffer * 255).astype(np.uint8)\n            else:\n                buffer = np.clip(buffer, 0, 255).astype(np.uint8)\n            return buffer\n        except Exception:\n            pass\n        \n        # METODO 3: Prova come float64\n        try:\n            buffer = np.empty(shape, dtype=np.float64)\n            dataset.read_direct(buffer)\n            if buffer.max() <= 1.0:\n                buffer = (buffer * 255).astype(np.uint8)\n            else:\n                buffer = np.clip(buffer, 0, 255).astype(np.uint8)\n            return buffer\n        except Exception:\n            pass\n        \n        # METODO 4: Lettura raw\n        try:\n            buffer = np.empty(shape, dtype=np.uint8)\n            dataset.id.read(h5py.h5s.ALL, h5py.h5s.ALL, buffer)\n            return buffer\n        except Exception as e:\n            raise RuntimeError(f\"Cannot read image dataset: {e}\")\n    \n    def _load_actions_robust(self, dataset) -> np.ndarray:\n        \"\"\"\n        Carica azioni da dataset HDF5 usando metodo robusto.\n        \"\"\"\n        shape = dataset.shape\n        \n        # METODO 1: Prova lettura diretta come float32\n        try:\n            buffer = np.empty(shape, dtype=np.float32)\n            dataset.read_direct(buffer)\n            return buffer\n        except Exception:\n            pass\n        \n        # METODO 2: Prova come float64 e converti\n        try:\n            buffer = np.empty(shape, dtype=np.float64)\n            dataset.read_direct(buffer)\n            return buffer.astype(np.float32)\n        except Exception as e:\n            raise RuntimeError(f\"Cannot read actions dataset: {e}\")\n    \n    def _build_sample_index(self) -> List[Tuple[int, int]]:\n        \"\"\"Pre-calcola gli indici (demo_idx, start_idx) per ogni transizione\"\"\"\n        indices: List[Tuple[int, int]] = []\n        for demo_idx, demo in enumerate(self.data):\n            demo_transitions = len(demo['observations']) - self.sequence_length + 1\n            if demo_transitions <= 0:\n                continue\n            indices.extend((demo_idx, start) for start in range(demo_transitions))\n        if not indices:\n            raise ValueError(\"Dataset index is empty after preprocessing\")\n        return indices\n\n    def __len__(self) -> int:\n        \"\"\"Numero totale di transizioni disponibili\"\"\"\n        return len(self.samples)\n    \n    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Restituisce una transizione (osservazione, azione).\n        \n        Args:\n            idx: indice della transizione\n            \n        Returns:\n            Dict con 'observations' e 'actions' come tensori\n        \"\"\"\n        demo_idx, start_idx = self.samples[idx]\n        demo = self.data[demo_idx]\n        end_idx = start_idx + self.sequence_length\n\n        obs = demo['observations'][start_idx:end_idx].copy()\n        actions = demo['actions'][start_idx:end_idx].copy()\n\n        # Preprocessing\n        obs = self._preprocess_obs(obs)\n        actions = self._preprocess_actions(actions)\n\n        # Per single-step prediction, restituisci solo primo elemento\n        if self.sequence_length == 1:\n            obs = obs[0]\n            actions = actions[0]\n\n        # Converti HWC -> CHW per PyTorch\n        if obs.ndim == 3:  # Single image: (H, W, C) -> (C, H, W)\n            obs = np.transpose(obs, (2, 0, 1))\n        elif obs.ndim == 4:  # Sequence: (T, H, W, C) -> (T, C, H, W)\n            obs = np.transpose(obs, (0, 3, 1, 2))\n\n        return {\n            'observations': torch.from_numpy(obs).float(),\n            'actions': torch.from_numpy(actions).float(),\n            'prompt': demo.get('prompt', '')\n        }\n    \n    def _preprocess_obs(self, obs: np.ndarray) -> np.ndarray:\n        \"\"\"Preprocessing delle osservazioni\"\"\"\n        processed = []\n        target_h, target_w = self.image_size\n        for img in obs:\n            if img.shape[0] != target_h or img.shape[1] != target_w:\n                img = cv2.resize(img, (target_w, target_h), interpolation=cv2.INTER_AREA)\n            processed.append(img)\n        obs = np.stack(processed, axis=0)\n\n        # Normalizza [0, 255] -> [0, 1]\n        obs = obs.astype(np.float32) / 255.0\n\n        # Data augmentation (se abilitato)\n        if self.augmentation:\n            obs = self._augment_obs(obs)\n        \n        return obs\n    def _augment_obs(self, obs: np.ndarray) -> np.ndarray:\n        \"\"\"Data augmentation per osservazioni\"\"\"\n        # Color jitter (brightness)\n        if np.random.rand() < 0.5:\n            brightness = np.random.uniform(0.8, 1.2)\n            obs = np.clip(obs * brightness, 0, 1)\n        \n        # Contrast adjustment\n        if np.random.rand() < 0.3:\n            contrast = np.random.uniform(0.8, 1.2)\n            mean = obs.mean(axis=(1, 2), keepdims=True)\n            obs = np.clip((obs - mean) * contrast + mean, 0, 1)\n        \n        # Random crop (con padding)\n        if np.random.rand() < 0.3:\n            crop_ratio = np.random.uniform(0.85, 0.95)\n            crop_size_h = int(self.image_size[0] * crop_ratio)\n            crop_size_w = int(self.image_size[1] * crop_ratio)\n            \n            start_y = np.random.randint(0, self.image_size[0] - crop_size_h + 1)\n            start_x = np.random.randint(0, self.image_size[1] - crop_size_w + 1)\n            \n            cropped = []\n            for img in obs:\n                img_crop = img[start_y:start_y+crop_size_h, start_x:start_x+crop_size_w]\n                img_resized = cv2.resize(img_crop, (self.image_size[1], self.image_size[0]))\n                cropped.append(img_resized)\n            obs = np.stack(cropped)\n        \n        return obs\n    \n    def _preprocess_actions(self, actions: np.ndarray) -> np.ndarray:\n        \"\"\"Preprocessing delle azioni con normalizzazione z-score\"\"\"\n        actions = actions.astype(np.float32)\n        if self.action_stats['mean'] is not None:\n            actions = (actions - self.action_stats['mean']) / self.action_stats['std']\n        \n        return actions\n    \n    def get_action_stats(self) -> Dict[str, np.ndarray]:\n        \"\"\"Restituisce le statistiche delle azioni per denormalizzazione\"\"\"\n        return self.action_stats.copy()\n    \n    def denormalize_actions(self, actions: np.ndarray) -> np.ndarray:\n        \"\"\"Denormalizza le azioni per l'esecuzione nel simulatore\"\"\"\n        if self.action_stats['mean'] is not None:\n            return actions * self.action_stats['std'] + self.action_stats['mean']\n        return actions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class PretrainedVisualEncoder(nn.Module):\n    \"\"\"Visual encoder basato su ResNet18 con testa adattiva.\"\"\"\n\n    def __init__(self, hidden_dim: int = 256, freeze_backbone: bool = True, dropout: float = 0.1):\n        super().__init__()\n\n        self.hidden_dim = hidden_dim\n\n        resnet = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n\n        if freeze_backbone:\n            for param in resnet.parameters():\n                param.requires_grad = False\n\n        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n\n        self.adapter = nn.Sequential(\n            nn.Linear(512, 512),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(512, hidden_dim)\n        )\n        self.ln = nn.LayerNorm(hidden_dim)\n\n        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n\n        self._init_weights()\n\n    def _init_weights(self):\n        for m in self.adapter.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x.float()\n\n        if x.shape[-1] != 224 or x.shape[-2] != 224:\n            x = nn.functional.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)\n\n        x = (x - self.mean) / self.std\n        features = self.backbone(x).flatten(start_dim=1)\n        output = self.adapter(features)\n        return self.ln(output)\n\n\nclass VisualEncoder(nn.Module):\n    def __init__(self, obs_shape: Tuple[int, int, int] = (3, 128, 128), hidden_dim: int = 256, dropout: float = 0.1):\n        super().__init__()\n\n        c, _, _ = obs_shape\n        self.conv = nn.Sequential(\n            nn.Conv2d(c, 32, 3, stride=2, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 256, 3, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True),\n            nn.AdaptiveAvgPool2d((1, 1))\n        )\n\n        self.head = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(256, hidden_dim),\n            nn.LayerNorm(hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout)\n        )\n\n        self._init_weights()\n\n    def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.conv(x)\n        return self.head(x)\n\n\nclass PromptEncoder(nn.Module):\n    \"\"\"Encodes natural-language task prompts via CLIP ViT-L/14 text tower.\"\"\"\n\n    def __init__(\n        self,\n        hidden_dim: int,\n        model_name: str = 'openai/clip-vit-large-patch14',\n        trainable: bool = False,\n        dropout: float = 0.1,\n        max_length: int = 77\n    ):\n        super().__init__()\n\n        self.tokenizer = CLIPTokenizer.from_pretrained(model_name)\n        self.text_model = CLIPTextModel.from_pretrained(model_name)\n        self.max_length = min(max_length, getattr(self.text_model.config, 'max_position_embeddings', max_length))\n\n        if not trainable:\n            self.text_model.eval()\n            for param in self.text_model.parameters():\n                param.requires_grad = False\n\n        text_hidden = self.text_model.config.hidden_size\n        self.adapter = nn.Sequential(\n            nn.LayerNorm(text_hidden),\n            nn.Linear(text_hidden, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout)\n        )\n\n        self._token_cache: Dict[str, Dict[str, torch.Tensor]] = {}\n\n    def _tokenize(self, prompt: str) -> Dict[str, torch.Tensor]:\n        if prompt not in self._token_cache:\n            tokens = self.tokenizer(\n                prompt,\n                return_tensors='pt',\n                padding='max_length',\n                truncation=True,\n                max_length=self.max_length\n            )\n            self._token_cache[prompt] = {k: v for k, v in tokens.items()}\n        cached = self._token_cache[prompt]\n        return {k: v.clone() for k, v in cached.items()}\n\n    def forward(self, prompts: List[str], device: torch.device) -> torch.Tensor:\n        if not prompts:\n            raise ValueError(\"PromptEncoder received an empty batch of prompts\")\n\n        token_batches = [self._tokenize(p) for p in prompts]\n        batch = {\n            key: torch.cat([tokens[key] for tokens in token_batches], dim=0).to(device)\n            for key in token_batches[0]\n        }\n\n        outputs = self.text_model(**batch)\n        pooled = outputs.pooler_output if outputs.pooler_output is not None else outputs.last_hidden_state[:, -1, :]\n        return self.adapter(pooled)\n\n\nclass RecursiveBlock(nn.Module):\n    \"\"\"\n    Blocco ricorsivo del TRM.\n    Implementa il ragionamento iterativo con self-attention e MLP.\n    \"\"\"\n    \n    def __init__(self, hidden_dim=256, num_heads=4, dropout=0.1):\n        super().__init__()\n        \n        # Self-attention per ragionamento\n        self.attention = nn.MultiheadAttention(\n            hidden_dim,\n            num_heads,\n            dropout=dropout,\n            batch_first=True\n        )\n        self.norm1 = nn.LayerNorm(hidden_dim)\n        self.dropout1 = nn.Dropout(dropout)\n        \n        # MLP per trasformazione\n        self.mlp = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim * 4),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim * 4, hidden_dim),\n            nn.Dropout(dropout)\n        )\n        self.norm2 = nn.LayerNorm(hidden_dim)\n    \n    def forward(self, h, x_cond):\n        \"\"\"\n        Args:\n            h: (B, D) hidden state corrente\n            x_cond: (B, D) conditioning dall'input\n        Returns:\n            (B, D) nuovo hidden state\n        \"\"\"\n        # Combina hidden state e conditioning\n        combined = h + x_cond\n        combined = combined.unsqueeze(1)  # (B, 1, D) per attention\n        \n        # Self-attention con residual\n        attn_out, _ = self.attention(combined, combined, combined)\n        combined = self.norm1(combined + self.dropout1(attn_out))\n        \n        # MLP con residual\n        mlp_out = self.mlp(combined)\n        h_new = self.norm2(combined + mlp_out)\n        \n        return h_new.squeeze(1)  # (B, D)\n\n\nclass TRMPolicy(nn.Module):\n    \"\"\"\n    Policy completa basata su TinyRecursiveModels per controllo robotico.\n    \n    Architettura:\n    1. Visual Encoder: CNN (custom o ResNet pre-trained) per estrarre features da immagini\n    2. Recursive Block: applicato N volte per ragionamento iterativo\n    3. Action Head: predice azioni da ultimo hidden state\n    \n    NOTA: Aspetta input in formato PyTorch standard (B, C, H, W)\n    \"\"\"\n    \n    def __init__(\n        self,\n        obs_shape=(3, 128, 128),  # Formato CHW (PyTorch standard)\n        action_dim=7,\n        hidden_dim=256,\n        num_heads=4,\n        num_recursions=8,\n        dropout=0.1,\n        adaptive_halt=False,\n        use_pretrained_encoder=True,\n        freeze_backbone=True,\n        encoder_dropout=0.1,\n        use_text_prompts=True,\n        text_encoder_name='openai/clip-vit-large-patch14',\n        train_text_encoder=False,\n        text_dropout=0.1\n    ):\n        super().__init__()\n        \n        self.hidden_dim = hidden_dim\n        self.num_recursions = num_recursions\n        self.adaptive_halt = adaptive_halt\n        self.obs_shape = obs_shape\n        self.use_pretrained_encoder = use_pretrained_encoder\n        self.use_text_prompts = use_text_prompts\n        \n        # Selezione encoder\n        if use_pretrained_encoder:\n            self.encoder = PretrainedVisualEncoder(\n                hidden_dim=hidden_dim,\n                freeze_backbone=freeze_backbone,\n                dropout=encoder_dropout\n            )\n        else:\n            self.encoder = VisualEncoder(\n                obs_shape=obs_shape,\n                hidden_dim=hidden_dim,\n                dropout=encoder_dropout\n            )\n        \n        if self.use_text_prompts:\n            self.prompt_encoder = PromptEncoder(\n                hidden_dim=hidden_dim,\n                model_name=text_encoder_name,\n                trainable=train_text_encoder,\n                dropout=text_dropout\n            )\n            fusion_in = hidden_dim * 2\n            self.fusion_adapter = nn.Sequential(\n                nn.LayerNorm(fusion_in),\n                nn.Linear(fusion_in, hidden_dim),\n                nn.GELU(),\n                nn.Dropout(dropout)\n            )\n        else:\n            self.prompt_encoder = None\n            self.fusion_adapter = None\n\n        # Recursive block\n        self.recursive_block = RecursiveBlock(hidden_dim, num_heads, dropout)\n        \n        # Action head\n        self.action_head = nn.Sequential(\n            nn.LayerNorm(hidden_dim),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, action_dim)\n        )\n        \n        # Adaptive halting (opzionale)\n        if adaptive_halt:\n            self.halt_predictor = nn.Sequential(\n                nn.Linear(hidden_dim, 64),\n                nn.ReLU(),\n                nn.Linear(64, 1),\n                nn.Sigmoid()\n            )\n    \n    def forward(self, obs, prompts: Optional[List[str]] = None, return_all_states=False):\n        \"\"\"\n        Args:\n            obs: (B, C, H, W) osservazioni in formato CHW (PyTorch standard)\n            prompts: lista di stringhe (B) con la descrizione del task\n            return_all_states: se True, restituisce tutti gli hidden states\n        Returns:\n            actions: (B, action_dim) azioni predette\n            (opzionale) states: lista di hidden states\n        \"\"\"\n        B = obs.shape[0]\n        \n        # Encoding iniziale\n        x_cond = self.encoder(obs)\n\n        if self.use_text_prompts:\n            if prompts is None:\n                raise ValueError(\"TRMPolicy richiede i prompt testuali quando use_text_prompts=True\")\n            text_features = self.prompt_encoder(prompts, device=obs.device)\n            x_cond = self.fusion_adapter(torch.cat([x_cond, text_features], dim=-1))\n\n        h = x_cond.clone()\n        \n        states = [h] if return_all_states else None\n        \n        # Applicazione ricorsiva del blocco\n        if self.adaptive_halt:\n            h, halt_info = self._forward_adaptive(h, x_cond, B)\n        else:\n            for t in range(self.num_recursions):\n                h = self.recursive_block(h, x_cond)\n                if return_all_states:\n                    states.append(h)\n        \n        # Predizione azione\n        actions = self.action_head(h)\n        \n        if return_all_states:\n            return actions, states\n        return actions\n    \n    def _forward_adaptive(self, h, x_cond, B):\n        \"\"\"Adaptive Computation Time (ACT)\"\"\"\n        halt_probs = []\n        remainders = torch.ones(B, device=h.device)\n        n_updates = torch.zeros(B, device=h.device)\n        accumulated_h = torch.zeros_like(h)\n        \n        for t in range(self.num_recursions):\n            h = self.recursive_block(h, x_cond)\n            \n            halt_p = self.halt_predictor(h).squeeze(-1)\n            halt_probs.append(halt_p)\n            \n            still_running = (remainders > 0.01).float()\n            accumulated_h += remainders.unsqueeze(-1) * h * still_running.unsqueeze(-1)\n            \n            remainders = remainders * (1 - halt_p) * still_running\n            n_updates += still_running\n            \n            if remainders.max() < 0.01:\n                break\n        \n        halt_info = {\n            'halt_probs': halt_probs,\n            'n_updates': n_updates\n        }\n        \n        return accumulated_h, halt_info\n\n\ndef build_policy_from_config(config: TrainingConfig, obs_shape: Tuple[int, int, int] = (3, 128, 128)) -> TRMPolicy:\n    \"\"\"Costruisce una TRMPolicy coerente con il TrainingConfig.\"\"\"\n\n    return TRMPolicy(\n        obs_shape=obs_shape,\n        action_dim=7,\n        hidden_dim=config.hidden_dim,\n        num_recursions=config.num_recursions,\n        dropout=config.dropout,\n        use_pretrained_encoder=config.use_pretrained_encoder,\n        freeze_backbone=config.freeze_backbone,\n        encoder_dropout=config.encoder_dropout,\n        use_text_prompts=config.use_text_prompts,\n        text_encoder_name=config.text_encoder_name,\n        train_text_encoder=config.train_text_encoder,\n        text_dropout=config.text_dropout\n    )\n\nprint(\"ok\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BehaviorCloningTrainer:\n    \"\"\"Trainer per Behavior Cloning con configurazione strutturata.\"\"\"\n\n    def __init__(\n        self,\n        model: nn.Module,\n        train_loader: DataLoader,\n        val_loader: DataLoader,\n        config: TrainingConfig,\n        device: torch.device,\n        use_wandb: bool = False\n    ):\n        self.model = model.to(device)\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.config = config\n        self.device = device\n        self.use_wandb = use_wandb\n        self.steps_per_epoch = max(len(train_loader), 1)\n\n        # Optimizer e scheduler\n        self.optimizer = torch.optim.AdamW(\n            model.parameters(),\n            lr=config.lr,\n            weight_decay=config.weight_decay\n        )\n\n        self.scheduler = None\n        if config.sched_T0:\n            # Cosine warm restarts operanti a livello di iterazioni\n            period_iters = max(1, config.sched_T0 * self.steps_per_epoch)\n            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n                self.optimizer,\n                T_0=period_iters,\n                T_mult=config.sched_T_mult,\n                eta_min=config.lr_min\n            )\n        else:\n            warmup_iters = max(1, config.warmup_epochs * self.steps_per_epoch)\n            total_iters = max(1, config.epochs * self.steps_per_epoch - warmup_iters)\n\n            for pg in self.optimizer.param_groups:\n                pg['lr'] = config.lr\n\n            warmup_scheduler = torch.optim.lr_scheduler.LinearLR(\n                self.optimizer,\n                start_factor=1e-8,\n                end_factor=1.0,\n                total_iters=warmup_iters\n            )\n\n            constant_scheduler = torch.optim.lr_scheduler.ConstantLR(\n                self.optimizer,\n                factor=1.0,\n                total_iters=total_iters\n            )\n\n            self.scheduler = torch.optim.lr_scheduler.SequentialLR(\n                self.optimizer,\n                schedulers=[warmup_scheduler, constant_scheduler],\n                milestones=[warmup_iters]\n            )\n\n        self.use_amp = (self.device.type == 'cuda')\n        self.scaler = torch.cuda.amp.GradScaler(enabled=self.use_amp)\n        self.grad_clip = config.grad_clip\n        self.early_stop_patience = config.early_stop_patience\n        self._epochs_no_improve = 0\n\n        self.best_val_loss = float('inf')\n        self.best_model_path = config.save_path\n    \n    def train(self):\n        \"\"\"Training loop completo\"\"\"\n        \n        for epoch in range(self.config.epochs):\n            # Training\n            train_metrics = self._train_epoch(epoch)\n            \n            # Validation\n            val_metrics = self._validate_epoch(epoch)\n            print(f\"Epoch {epoch}, training loss: {train_metrics['loss']}, validation loss: {val_metrics['loss']}\")\n            # Logging exaustive\n            self._log_metrics(epoch, train_metrics, val_metrics)\n\n            # print(f\"Epoch: {epoch}\\n\\t Training: {train_metrics}\\n\\t Validation: {val_metrics}\")\n            \n            # Save best model\n            if val_metrics['loss'] < self.best_val_loss:\n                self.best_val_loss = val_metrics['loss']\n                torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': self.model.state_dict(),\n                    'optimizer_state_dict': self.optimizer.state_dict(),\n                    'val_loss': val_metrics['loss'],\n                    'config': self.config.to_dict()\n                }, self.best_model_path)\n                print(f\"  ‚úì Saved best model (val_loss: {val_metrics['loss']:.4f})\")\n                self._epochs_no_improve = 0\n            else:\n                self._epochs_no_improve += 1\n\n            if self.early_stop_patience and self._epochs_no_improve >= self.early_stop_patience:\n                print(\"‚èπÔ∏è  Early stopping triggered\")\n                break\n        \n        print(f\"\\n‚úÖ Training completed! Best val loss: {self.best_val_loss:.4f}\")\n        return self.best_val_loss\n    \n    def _train_epoch(self, epoch):\n        \"\"\"Training per una epoch\"\"\"\n        self.model.train()\n        \n        total_loss = 0\n        action_mse = 0\n        action_l1 = 0\n        \n        for step, batch in enumerate(self.train_loader):\n            obs = batch['observations'].to(self.device, non_blocking=True)\n            target_actions = batch['actions'].to(self.device, non_blocking=True)\n            prompts = batch.get('prompt')\n\n            self.optimizer.zero_grad(set_to_none=True)\n\n            with torch.cuda.amp.autocast(enabled=self.use_amp):\n                pred_actions = self.model(obs, prompts=prompts)\n                mse = F.mse_loss(pred_actions, target_actions)\n                l1 = F.l1_loss(pred_actions, target_actions)\n                loss = 0.7 * mse + 0.3 * l1\n\n            if self.use_amp:\n                self.scaler.scale(loss).backward()\n                if self.grad_clip:\n                    self.scaler.unscale_(self.optimizer)\n                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)\n                self.scaler.step(self.optimizer)\n                self.scaler.update()\n            else:\n                loss.backward()\n                if self.grad_clip:\n                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)\n                self.optimizer.step()\n\n            total_loss += loss.item()\n            action_mse += mse.item()\n            action_l1 += l1.item()\n\n            if self.scheduler is not None:\n                self.scheduler.step()\n\n        n_batches = len(self.train_loader)\n        return {\n            'loss': total_loss / n_batches,\n            'action_mse': action_mse / n_batches,\n            'action_l1': action_l1 / n_batches\n        }\n    \n    def _validate_epoch(self, epoch):\n        \"\"\"Validation per una epoch\"\"\"\n        self.model.eval()\n        \n        total_loss = 0\n        action_mse = 0\n        action_l1 = 0\n        \n        with torch.no_grad():\n            \n            for batch in self.val_loader:\n                obs = batch['observations'].to(self.device)\n                target_actions = batch['actions'].to(self.device)\n                prompts = batch.get('prompt')\n                \n                # Forward pass\n                pred_actions = self.model(obs, prompts=prompts)\n                \n                # Loss\n                mse = F.mse_loss(pred_actions, target_actions)\n                l1 = F.l1_loss(pred_actions, target_actions)\n                loss = 0.7 * mse + 0.3 * l1\n                \n                total_loss += loss.item()\n                action_mse += mse.item()\n                action_l1 += l1.item()\n                \n        \n        n_batches = len(self.val_loader)\n        return {\n            'loss': total_loss / n_batches,\n            'action_mse': action_mse / n_batches,\n            'action_l1': action_l1 / n_batches\n        }\n    \n    def _log_metrics(self, epoch, train_metrics, val_metrics):\n        \"\"\"Log delle metriche\"\"\"\n        lr = self.optimizer.param_groups[0]['lr']\n        \n        # WandB logging\n        if self.use_wandb:\n            wandb.log({\n                'epoch': epoch,\n                'train/loss': train_metrics['loss'],\n                'train/action_mse': train_metrics['action_mse'],\n                'train/action_l1': train_metrics.get('action_l1'),\n                'val/loss': val_metrics['loss'],\n                'val/action_mse': val_metrics['action_mse'],\n                'val/action_l1': val_metrics.get('action_l1'),\n                'lr': lr\n            })\n\ndef build_dataloaders(\n    train_dataset: Dataset,\n    val_dataset: Dataset,\n    batch_size: int,\n    loader_kwargs: Dict[str, Any]\n) -> Tuple[DataLoader, DataLoader]:\n    \"\"\"Crea data loader consistenti a partire da un template di kwargs.\"\"\"\n\n    kwargs = loader_kwargs.copy()\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        drop_last=True,\n        **kwargs\n    )\n\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        drop_last=False,\n        **kwargs\n    )\n\n    return train_loader, val_loader\n\n\ndef _set_dataset_augmentation(dataset, flag: bool):\n    \"\"\"Imposta augmentation temporaneamente e restituisce funzione di restore.\"\"\"\n\n    if not hasattr(dataset, 'augmentation'):\n        return lambda: None\n\n    original = dataset.augmentation\n    dataset.augmentation = flag\n\n    def restore():\n        dataset.augmentation = original\n\n    return restore\n\n\ndef optuna_random_search(\n    train_dataset: Dataset,\n    val_dataset: Dataset,\n    loader_kwargs: Dict[str, Any],\n    device: torch.device,\n    quick_epochs: int = 10,\n    search_space: Optional[HyperparameterSearchSpace] = None,\n    n_trials: int = 50\n) -> Tuple[TrainingConfig, List[Dict[str, Any]]]:\n    \"\"\"\n    Randomized Optuna search (TPE-based) instead of grid-search.\n    Runs a fixed number of trials (n_trials).\n    \"\"\"\n\n    search_space = search_space or default_search_space()\n\n    # --- NEW: TPE Sampler (recommended over random sampling) ---\n    sampler = optuna.samplers.TPESampler(seed=42)\n\n    study = optuna.create_study(\n        direction='minimize',\n        sampler=sampler\n    )\n\n    print(f\"\\nüîç Starting Optuna random search\")\n    print(f\"  Number of trials: {n_trials}\")\n    print(f\"  Quick epochs: {quick_epochs}\")\n\n    def objective(trial: optuna.Trial) -> float:\n\n        # --- Suggest values from the search space ---\n        trial_config = TrainingConfig(\n            lr=trial.suggest_categorical('lr', search_space.lr),\n            hidden_dim=trial.suggest_categorical('hidden_dim', search_space.hidden_dim),\n            num_recursions=trial.suggest_categorical('num_recursions', search_space.num_recursions),\n            epochs=quick_epochs,\n            batch_size=trial.suggest_categorical('batch_size', search_space.batch_size),\n            weight_decay=trial.suggest_categorical('weight_decay', search_space.weight_decay),\n            use_pretrained_encoder=trial.suggest_categorical('pretrained_encoder', search_space.pretrained_encoder),\n            freeze_backbone=trial.suggest_categorical('freeze_backbone', search_space.freeze_backbone),\n            augmentation=trial.suggest_categorical('augmentation', search_space.augmentation),\n            dropout=trial.suggest_categorical('dropout', search_space.dropout),\n            encoder_dropout=trial.suggest_categorical('dropout', search_space.dropout),\n            grad_clip=1.0,\n            save_path=f\"optuna_trial_{trial.number}.pt\"\n        )\n\n        # Build dataloaders based on this trial's batch size\n        train_loader, val_loader = build_dataloaders(\n            train_dataset,\n            val_dataset,\n            trial_config.batch_size,\n            loader_kwargs\n        )\n\n        # Build model\n        model = build_policy_from_config(trial_config)\n        trainer = BehaviorCloningTrainer(\n            model,\n            train_loader,\n            val_loader,\n            trial_config,\n            device,\n            use_wandb=False\n        )\n\n        # Turn augmentation on/off safely\n        restore_aug = _set_dataset_augmentation(train_dataset, trial_config.augmentation)\n\n        try:\n            val_loss = trainer.train()\n        finally:\n            restore_aug()\n\n        # Store config inside trial for later retrieval\n        trial.set_user_attr('config', trial_config.to_dict())\n\n        return val_loss\n\n    # --- NEW: Run fixed-size random search ---\n    study.optimize(objective, n_trials=n_trials)\n\n    # Save trial history (full information)\n    history = []\n    for trial in study.trials:\n        history.append({\n            'trial_number': trial.number,\n            'state': str(trial.state),\n            'value': trial.value,\n            'params': trial.params,                # all parameters sampled\n            'user_attrs': trial.user_attrs,        # user metadata (our config)\n            'distributions': {\n                k: str(v) for k, v in trial.distributions.items()\n            },                                      # searchable distributions\n            'system_attrs': trial.system_attrs,    # optuna internal info\n            'datetime_start': str(trial.datetime_start),\n            'datetime_complete': str(trial.datetime_complete),\n        })\n\n\n    # Best configuration\n    best_config = TrainingConfig(**study.best_trial.user_attrs['config'])\n\n    return best_config, history","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_final_model(\n    base_config: TrainingConfig,\n    train_dataset: Dataset,\n    val_dataset: Dataset,\n    loader_kwargs: Dict[str, Any],\n    device,\n    final_epochs: Optional[int] = None,\n    use_wandb: bool = False\n) -> Tuple[nn.Module, float]:\n    \"\"\"Esegue il training finale a partire dalla configurazione scelta.\"\"\"\n\n    final_epochs = final_epochs or base_config.epochs\n    final_config = replace(\n        base_config,\n        epochs=final_epochs,\n        save_path='final_model.pt',\n        early_stop_patience=base_config.early_stop_patience or 10,\n        sched_T0=base_config.sched_T0 or max(1, final_epochs // 5)\n    )\n\n    print(f\"\\n{'='*60}\")\n    print(\"üéØ FINAL TRAINING\")\n    print(f\"Config: {final_config.label()}\")\n    print(f\"{'='*60}\")\n\n    if use_wandb:\n        wandb.init(\n            project=\"trm-robotics\",\n            name=f\"final_{final_config.label()}\",\n            config=final_config.to_dict()\n        )\n\n    train_loader, val_loader = build_dataloaders(\n        train_dataset,\n        val_dataset,\n        final_config.batch_size,\n        loader_kwargs\n    )\n\n    model = build_policy_from_config(final_config)\n    trainer = BehaviorCloningTrainer(\n        model,\n        train_loader,\n        val_loader,\n        final_config,\n        device,\n        use_wandb=use_wandb\n    )\n\n    restore_aug = _set_dataset_augmentation(train_dataset, final_config.augmentation)\n    try:\n        final_val_loss = trainer.train()\n    finally:\n        restore_aug()\n\n    if os.path.exists(final_config.save_path):\n        checkpoint = torch.load(final_config.save_path, map_location=device)\n        model.load_state_dict(checkpoint['model_state_dict'])\n\n    if use_wandb:\n        wandb.finish()\n\n    print(f\"\\n‚úÖ Final model trained! Val loss: {final_val_loss:.4f}\")\n\n    return model, final_val_loss\n\nclass PolicyEvaluator:\n    \"\"\"\n    Valutatore per policy robotiche in simulazione LIBERO\n    \"\"\"\n    \n    def __init__(\n        self,\n        policy: nn.Module,\n        device: torch.device,\n        action_stats: Dict = None\n    ):\n        self.policy = policy.to(device)\n        self.policy.eval()\n        self.device = device\n        self.action_stats = action_stats\n    \n    def evaluate_on_task(\n        self,\n        env,\n        init_states,\n        num_episodes=50,\n        max_steps=500,\n        record_video=False,\n        video_path=None,\n        task_prompt: Optional[str] = None\n    ):\n        \"\"\"\n        Valuta policy su un singolo task\n        \n        Args:\n            env: environment LIBERO\n            init_states: stati iniziali per il task\n            num_episodes: numero di episodi da valutare\n            max_steps: massimo numero di step per episodio\n            record_video: se True, registra video\n            video_path: path per salvare video\n        \n        Returns:\n            results: dizionario con metriche\n        \"\"\"\n        expect_prompt = getattr(self.policy, 'use_text_prompts', False)\n        if expect_prompt and not task_prompt:\n            raise ValueError(\"PolicyEvaluator richiede un task_prompt per policy testo-condizionate\")\n\n        successes = []\n        episode_lengths = []\n        frames_buffer = [] if record_video else None\n        \n        for ep in range(num_episodes):\n            # Reset environment\n            env.reset()\n            env.set_init_state(init_states[ep % len(init_states)])\n            \n            obs = env.get_observation()\n            done = False\n            step = 0\n            \n            episode_frames = [] if record_video and ep < 5 else None\n            \n            while not done and step < max_steps:\n                # Cattura frame\n                if episode_frames is not None:\n                    frame = env.render(mode='rgb_array')\n                    episode_frames.append(frame)\n                \n                # Preprocessing osservazione\n                obs_tensor = self._preprocess_obs(obs)\n                \n                # Predici azione\n                with torch.no_grad():\n                    prompt_batch = [task_prompt] if task_prompt else None\n                    action = self.policy(obs_tensor, prompts=prompt_batch)\n                    action = action.cpu().numpy().squeeze()\n                    \n                    # Denormalizza azione se necessario\n                    if self.action_stats is not None:\n                        action = action * self.action_stats['std'] + self.action_stats['mean']\n                \n                # Step environment\n                obs, reward, done, info = env.step(action)\n                step += 1\n            \n            # Registra risultati\n            success = info.get('success', False)\n            successes.append(success)\n            episode_lengths.append(step)\n            \n            if episode_frames is not None:\n                frames_buffer.append(episode_frames)\n        \n        # Salva video se richiesto\n        if record_video and frames_buffer and video_path:\n            self._save_videos(frames_buffer, video_path)\n        \n        # Calcola metriche\n        results = {\n            'success_rate': np.mean(successes),\n            'avg_episode_length': np.mean(episode_lengths),\n            'std_episode_length': np.std(episode_lengths),\n            'num_episodes': num_episodes\n        }\n        \n        return results\n    \n    def _preprocess_obs(self, obs):\n        \"\"\"Preprocessing osservazione per policy\"\"\"\n        # Estrai immagine RGB\n        img = obs['agentview_rgb']\n        \n        # Normalizza\n        img = img.astype(np.float32) / 255.0\n        \n        # Converti a tensor e aggiungi batch dimension\n        img = np.transpose(img, (2, 0, 1))  # HWC -> CHW\n        img_tensor = torch.from_numpy(img).unsqueeze(0).to(self.device)\n        \n        return img_tensor\n    \n    def _save_videos(self, frames_buffer, video_path):\n        \"\"\"Salva video degli episodi\"\"\"\n        os.makedirs(os.path.dirname(video_path), exist_ok=True)\n        \n        for i, frames in enumerate(frames_buffer):\n            path = video_path.replace('.mp4', f'_ep{i}.mp4')\n            \n            # Usa OpenCV per salvare video\n            height, width, _ = frames[0].shape\n            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n            out = cv2.VideoWriter(path, fourcc, 30.0, (width, height))\n            \n            for frame in frames:\n                frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n                out.write(frame_bgr)\n            \n            out.release()\n            print(f\"  Video salvato: {path}\")\n\n\ndef generate_evaluation_report(results_dict, output_path):\n    \"\"\"\n    Genera report HTML con risultati di valutazione\n    \n    Args:\n        results_dict: dizionario {task_name: results}\n        output_path: path per salvare report\n    \"\"\"\n    \n    html_content = f\"\"\"\n    <!DOCTYPE html>\n    <html>\n    <head>\n        <title>TRM Robotics - Evaluation Report</title>\n        <style>\n            body {{ font-family: Arial, sans-serif; margin: 40px; }}\n            h1 {{ color: #333; }}\n            table {{ border-collapse: collapse; width: 100%; margin-top: 20px; }}\n            th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}\n            th {{ background-color: #4CAF50; color: white; }}\n            tr:nth-child(even) {{ background-color: #f2f2f2; }}\n            .success {{ color: green; font-weight: bold; }}\n            .fail {{ color: red; }}\n        </style>\n    </head>\n    <body>\n        <h1>ü§ñ TRM Robotics Evaluation Report</h1>\n        <p><strong>Generated:</strong> {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}</p>\n        \n        <h2>Results Summary</h2>\n        <table>\n            <tr>\n                <th>Task Name</th>\n                <th>Success Rate</th>\n                <th>Avg Episode Length</th>\n                <th>Std Episode Length</th>\n                <th>Num Episodes</th>\n            </tr>\n    \"\"\"\n    \n    # Aggiungi righe per ogni task\n    overall_success = []\n    \n    for task_name, results in results_dict.items():\n        success_rate = results['success_rate']\n        overall_success.append(success_rate)\n        \n        success_class = 'success' if success_rate >= 0.5 else 'fail'\n        \n        html_content += f\"\"\"\n            <tr>\n                <td>{task_name}</td>\n                <td class=\"{success_class}\">{success_rate:.2%}</td>\n                <td>{results['avg_episode_length']:.1f}</td>\n                <td>{results['std_episode_length']:.1f}</td>\n                <td>{results['num_episodes']}</td>\n            </tr>\n        \"\"\"\n    \n    # Overall statistics\n    mean_success = np.mean(overall_success)\n    success_class = 'success' if mean_success >= 0.5 else 'fail'\n    \n    html_content += f\"\"\"\n            <tr style=\"font-weight: bold; background-color: #e0e0e0;\">\n                <td>OVERALL</td>\n                <td class=\"{success_class}\">{mean_success:.2%}</td>\n                <td colspan=\"3\">-</td>\n            </tr>\n        </table>\n    </body>\n    </html>\n    \"\"\"\n    \n    # Salva report\n    with open(output_path, 'w') as f:\n        f.write(html_content)\n    \n    print(f\"\\n‚úì Report salvato: {output_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main_pipeline(\n    data_path: str = '/kaggle/working/dataset/libero_spatial',\n    work_dir: str = '/kaggle/working/trm_robotics',\n    quick_search: bool = True,\n    train_final: bool = True,\n    use_custom_final_config: bool = False,\n    evaluate: bool = True,\n    use_wandb: bool = False\n):\n    \"\"\"\n    Pipeline completa per il progetto TRM Robotics\n    \n    Args:\n        data_path: path ai dati LIBERO\n        work_dir: directory di lavoro\n        quick_search: se True, esegue hyperparameter search\n        train_final: se True, esegue training finale\n        evaluate: se True, esegue valutazione in simulazione\n        use_wandb: se True, usa WandB per logging\n    \"\"\"\n    \n    print(f\"\"\"\n    {'='*80}\n    ü§ñ TinyRecursiveModels per Controllo Robotico\n    {'='*80}\n    \n    Obiettivi:\n    1. Adattare architettura TRM per robotica\n    2. Training con Behavior Cloning su LIBERO\n    3. Valutazione in simulazione con metriche quantitative e qualitative\n    \n    \"\"\")\n    \n    # Setup directories\n    os.makedirs(work_dir, exist_ok=True)\n    os.chdir(work_dir)\n    \n    # Device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"‚úì Using device: {device}\\n\")\n    \n    # ========== STEP 1: Caricamento Dataset ==========\n    print(f\"\\n{'='*80}\")\n    print(\"STEP 1: Caricamento Dataset\")\n    print(f\"{'='*80}\")\n    \n    # Trova file HDF5\n    data_path = Path(data_path)\n    hdf5_files = list(data_path.glob('**/*.hdf5'))\n    \n    if not hdf5_files:\n        print(f\"‚ùå Nessun file HDF5 trovato in {data_path}\")\n        print(\"Assicurati di aver scaricato il dataset LIBERO!\")\n        return\n    \n    print(f\"‚úì Trovati {len(hdf5_files)} file HDF5 (task)\")\n    \n    # Demo-level split: usa TUTTI i file per entrambi i dataset\n    # ma dividi le demo all'interno di ciascun file (80% train, 20% val)\n    demo_split_ratio = 0.8\n    print(f\"\\nüìä Demo-level split: {demo_split_ratio:.0%} train / {1-demo_split_ratio:.0%} val per ogni task\")\n    print(f\"   Tutti i {len(hdf5_files)} task presenti in entrambi train e val\")\n    \n    # Crea datasets con demo-level split\n    print(\"\\nCreating TRAIN dataset...\")\n    train_dataset = LIBERODataset(\n        hdf5_files,  # Usa TUTTI i file\n        sequence_length=1,\n        image_size=(128, 128),\n        augmentation=False,\n        max_demos_per_task=50,  # Limita per velocit√†\n        demo_split_ratio=demo_split_ratio,\n        is_train=True  # Prime 80% demo per task\n    )\n    \n    # Usa le stesse statistiche del training set per la validation\n    train_action_stats = train_dataset.action_stats\n    \n    print(\"\\nCreating VAL dataset...\")\n    val_dataset = LIBERODataset(\n        hdf5_files,  # Usa TUTTI i file\n        sequence_length=1,\n        image_size=(128, 128),\n        augmentation=False,\n        max_demos_per_task=50,  # Stesso limite per coerenza\n        demo_split_ratio=demo_split_ratio,\n        is_train=False,  # Ultime 20% demo per task\n        action_stats=train_action_stats  # Usa statistiche del training\n    )\n    \n    # Data loader template (verranno istanziati on-demand)\n    num_workers = min(4, os.cpu_count() or 1)\n    use_cuda = torch.cuda.is_available()\n\n    loader_common = {\n        'num_workers': num_workers,\n        'pin_memory': use_cuda,\n        'persistent_workers': num_workers > 0\n    }\n    if num_workers > 0:\n        loader_common['prefetch_factor'] = 2\n\n    print(f\"\\n‚úì Dataset creati con demo-level split\")\n    print(f\"  Train samples: {len(train_dataset)}\")\n    print(f\"  Val samples: {len(val_dataset)}\")\n    \n    # Salva action stats per denormalizzazione\n    action_stats = train_dataset.action_stats\n    with open('action_stats.json', 'w') as f:\n        json.dump({\n            'mean': action_stats['mean'].tolist(),\n            'std': action_stats['std'].tolist()\n        }, f)\n    \n    # ========== STEP 2: Hyperparameter Search ==========\n    best_config = None\n    \n    if quick_search:\n        print(f\"\\n{'='*80}\")\n        print(\"STEP 2: Hyperparameter Search\")\n        print(f\"{'='*80}\")\n        \n        best_config, all_results = optuna_random_search(\n            train_dataset,\n            val_dataset,\n            loader_common,\n            device,\n            quick_epochs=10\n        )\n        \n        # Salva risultati\n        with open('hyperparam_results.json', 'w') as f:\n            json.dump(all_results, f, indent=2)\n        with open('best_hyperparams.json', 'w') as f:\n            json.dump(best_config.to_dict(), f, indent=2)\n    \n    # ========== STEP 3: Training Finale ==========\n    trained_model = None\n    \n    if train_final:\n        print(f\"\\n{'='*80}\")\n        print(\"STEP 3: Training Finale\")\n        print(f\"{'='*80}\")\n        \n        if best_config is None or use_custom_final_config:\n            best_config = TrainingConfig(\n                lr=3e-4,\n                hidden_dim=256,\n                num_recursions=8,\n                batch_size=32,\n                epochs=100,\n                weight_decay=1e-4,\n                dropout=0.3,\n                encoder_dropout=0.1,\n                use_pretrained_encoder=True,\n                freeze_backbone=False,\n                augmentation=False\n            )\n            print(\"‚ö†Ô∏è  Usando configurazione custom di default\")\n\n        trained_model, final_val_loss = train_final_model(\n            best_config,\n            train_dataset,\n            val_dataset,\n            loader_common,\n            device,\n            final_epochs=100,\n            use_wandb=use_wandb\n        )\n    \n    # ========== STEP 4: Valutazione ==========\n    if evaluate:\n        print(f\"\\n{'='*80}\")\n        print(\"STEP 4: Valutazione in Simulazione\")\n        print(f\"{'='*80}\")\n        \n        # Carica modello se non gi√† trainato\n        if trained_model is None:\n            print(\"Caricando modello salvato...\")\n            checkpoint = torch.load('final_model.pt')\n            \n            if 'config' in checkpoint:\n                config_obj = TrainingConfig(**checkpoint['config'])\n                trained_model = build_policy_from_config(config_obj)\n            else:\n                trained_model = TRMPolicy(\n                    obs_shape=(3, 128, 128),\n                    action_dim=7,\n                    hidden_dim=256,\n                    num_recursions=8\n                )\n\n            trained_model.load_state_dict(checkpoint['model_state_dict'])\n            trained_model = trained_model.to(device)\n        \n        # Crea evaluator\n        evaluator = PolicyEvaluator(trained_model, device, action_stats)\n        \n        # Nota: La valutazione in simulazione richiede setup di LIBERO environment\n        # Questo √® un placeholder per la struttura\n        print(\"\\n‚ö†Ô∏è  La valutazione in simulazione richiede setup completo di LIBERO\")\n        print(\"Vedi sezione 'Valutazione in Simulazione' per implementazione completa\")\n        \n        # Pseudocodice per valutazione:\n        \"\"\"\n        from libero.libero import benchmark\n        \n        results_dict = {}\n        \n        for task_id in range(num_tasks):\n            env, init_states = create_eval_env(task_suite, task_id)\n            \n            results = evaluator.evaluate_on_task(\n                env,\n                init_states,\n                num_episodes=50,\n                record_video=True,\n                video_path=f'videos/task_{task_id}.mp4'\n            )\n            \n            results_dict[f'task_{task_id}'] = results\n            env.close()\n        \n        # Genera report\n        generate_evaluation_report(results_dict, 'evaluation_report.html')\n        \"\"\"\n    \n    print(f\"\\n{'='*80}\")\n    print(\"‚úÖ Pipeline completata!\")\n    print(f\"{'='*80}\")\n    print(f\"\\nFile generati in: {work_dir}\")\n    print(\"  - final_model.pt: modello allenato\")\n    print(\"  - action_stats.json: statistiche azioni\")\n    print(\"  - hyperparam_results.json: risultati hyperparameter search\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"Setup model with Optuna best params (trial 26), validate then train.\"\"\"\nimport torch\n\ndata_path =  '/kaggle/working/dataset/libero_spatial'\nwork_dir = '/kaggle/working/trm_robotics'\n\n# Optuna best parameters from your message\nbest_params = {\n    'lr': 1e-4,\n    'hidden_dim': 128,\n    'num_recursions': 12,\n    'batch_size': 256,\n    'weight_decay': 1.0,\n    'use_pretrained_encoder': True,\n    'freeze_backbone': False,\n    'augmentation': True,\n    'dropout': 0.3,\n}\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"‚úì Using device: {device}\\n\")\n\n# ========== STEP 1: Caricamento Dataset ==========\nprint(f\"\\n{'='*80}\")\nprint(\"STEP 1: Caricamento Dataset\")\nprint(f\"{'='*80}\")\n\n# Trova file HDF5\ndata_path = Path(data_path)\nhdf5_files = list(data_path.glob('**/*.hdf5'))\n\nif not hdf5_files:\n    print(f\"‚ùå Nessun file HDF5 trovato in {data_path}\")\n    print(\"Assicurati di aver scaricato il dataset LIBERO!\")\n\nprint(f\"‚úì Trovati {len(hdf5_files)} file HDF5 (task)\")\n\n# Demo-level split: usa TUTTI i file per entrambi i dataset\n# ma dividi le demo all'interno di ciascun file (80% train, 20% val)\ndemo_split_ratio = 0.8\nprint(f\"\\nüìä Demo-level split: {demo_split_ratio:.0%} train / {1-demo_split_ratio:.0%} val per ogni task\")\nprint(f\"   Tutti i {len(hdf5_files)} task presenti in entrambi train e val\")\n\n# Crea datasets con demo-level split\nprint(\"\\nCreating TRAIN dataset...\")\ntrain_dataset = LIBERODataset(\n    hdf5_files,  # Usa TUTTI i file\n    sequence_length=1,\n    image_size=(128, 128),\n    augmentation=False,\n    max_demos_per_task=50,  # Limita per velocit√†\n    demo_split_ratio=demo_split_ratio,\n    is_train=True  # Prime 80% demo per task\n)\n\n# Usa le stesse statistiche del training set per la validation\ntrain_action_stats = train_dataset.action_stats\n\nprint(\"\\nCreating VAL dataset...\")\nval_dataset = LIBERODataset(\n    hdf5_files,  # Usa TUTTI i file\n    sequence_length=1,\n    image_size=(128, 128),\n    augmentation=False,\n    max_demos_per_task=50,  # Stesso limite per coerenza\n    demo_split_ratio=demo_split_ratio,\n    is_train=False,  # Ultime 20% demo per task\n    action_stats=train_action_stats  # Usa statistiche del training\n)\n\n# Data loader template (verranno istanziati on-demand)\nnum_workers = min(4, os.cpu_count() or 1)\nuse_cuda = torch.cuda.is_available()\n\nloader_common = {\n    'num_workers': num_workers,\n    'pin_memory': use_cuda,\n    'persistent_workers': num_workers > 0\n}\nif num_workers > 0:\n    loader_common['prefetch_factor'] = 2\n\nprint(f\"\\n‚úì Dataset creati con demo-level split\")\nprint(f\"  Train samples: {len(train_dataset)}\")\nprint(f\"  Val samples: {len(val_dataset)}\")\n\n# Salva action stats per denormalizzazione\naction_stats = train_dataset.action_stats\nwith open('action_stats.json', 'w') as f:\n    json.dump({\n        'mean': action_stats['mean'].tolist(),\n        'std': action_stats['std'].tolist()\n    }, f)\n\n# Build TrainingConfig (adjust epochs as desired)\nconfig = TrainingConfig(\n    lr=best_params['lr'],\n    hidden_dim=best_params['hidden_dim'],\n    num_recursions=best_params['num_recursions'],\n    epochs=5,                       # change if you want longer training\n    batch_size=best_params['batch_size'],\n    weight_decay=best_params['weight_decay'],\n    use_pretrained_encoder=best_params['use_pretrained_encoder'],\n    freeze_backbone=best_params['freeze_backbone'],\n    augmentation=best_params['augmentation'],\n    dropout=best_params['dropout'],\n    encoder_dropout=best_params['dropout'],\n    save_path='best_trial_26.pt'\n)\n\n# Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Retrieve datasets and loader kwargs from notebook namespace (created earlier)\ntrain_dataset = globals().get('train_dataset')\nval_dataset = globals().get('val_dataset')\nloader_common = globals().get('loader_common', {'num_workers': 0, 'pin_memory': False, 'persistent_workers': False})\n\nif train_dataset is None or val_dataset is None:\n    raise RuntimeError(\"train_dataset or val_dataset not found. Run the dataset preparation cells before this cell.\")\n\n# Build dataloaders, model and trainer\ntrain_loader, val_loader = build_dataloaders(train_dataset, val_dataset, config.batch_size, loader_common)\nmodel = build_policy_from_config(config, obs_shape=(3, 128, 128)).to(device)\ntrainer = BehaviorCloningTrainer(model, train_loader, val_loader, config, device, use_wandb=False)\n\n# Initial validation (before training) and training\nprint('Running initial validation...')\nval_metrics = trainer._validate_epoch(0)\nprint(f\"Initial validation loss: {val_metrics['loss']:.4f}\")\n\nprint('Starting training...')\nbest_val = trainer.train()\nprint(f\"Training finished. Best validation loss saved: {best_val:.4f}\")\nprint(f\"Checkpoint saved to: {config.save_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}