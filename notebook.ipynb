{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Libero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from unittest.mock import MagicMock\n",
        "\n",
        "# The Patch\n",
        "mock_mpl = MagicMock()\n",
        "sys.modules[\"matplotlib\"] = mock_mpl\n",
        "sys.modules[\"matplotlib.pyplot\"] = mock_mpl\n",
        "sys.modules[\"matplotlib.cm\"] = mock_mpl\n",
        "sys.modules[\"matplotlib.colors\"] = mock_mpl\n",
        "sys.modules[\"matplotlib.transforms\"] = mock_mpl\n",
        "sys.modules[\"matplotlib.ticker\"] = mock_mpl\n",
        "sys.modules[\"matplotlib._path\"] = mock_mpl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Path to your demo file\n",
        "file_path = \"dataset/libero_spatial/pick_up_the_black_bowl_from_table_center_and_place_it_on_the_plate_demo.hdf5\"\n",
        "\n",
        "print(f\"Opening file: {file_path}\")\n",
        "\n",
        "try:\n",
        "    with h5py.File(file_path, \"r\") as f:\n",
        "        # Loop through all demos in the file\n",
        "        for demo_name in f[\"data\"]:\n",
        "            print(f\"\\n=== Demo: {demo_name} ===\")\n",
        "            \n",
        "            # Access the image data (AgentView RGB)\n",
        "            # Note: Ensure this path exists in your specific HDF5 structure\n",
        "            if \"obs/agentview_rgb\" in f[f\"data/{demo_name}\"]:\n",
        "                dataset = f[f\"data/{demo_name}/obs/agentview_rgb\"]\n",
        "                num_images = dataset.shape[0]\n",
        "                print(f\"Total frames: {num_images}\")\n",
        "                \n",
        "                # Pick indices: every 15th frame + the last one\n",
        "                indices = list(range(0, num_images, 15))\n",
        "                if num_images - 1 not in indices:\n",
        "                    indices.append(num_images - 1)\n",
        "                \n",
        "                # Display images horizontally using HTML/PIL (No Matplotlib)\n",
        "                images_html = []\n",
        "                for idx in indices:\n",
        "                    img_array = dataset[idx]\n",
        "                    \n",
        "                    # Convert numpy array to PIL Image\n",
        "                    # (Robosuite images are usually already correct, but sometimes flipped)\n",
        "                    img = Image.fromarray(img_array)\n",
        "                    \n",
        "                    # Resize for smaller display if needed\n",
        "                    img_small = img.resize((128, 128)) \n",
        "                    \n",
        "                    # Hack to display inline in loop\n",
        "                    print(f\"Frame {idx}:\")\n",
        "                    #display(img)\n",
        "            else:\n",
        "                print(f\"Skipping {demo_name}: 'obs/agentview_rgb' not found.\")\n",
        "                \n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import h5py\n",
        "import einops\n",
        "from torchvision import models\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "from torchvision import transforms\n",
        "\n",
        "# Verifica disponibilit√† GPU\n",
        "print(f\"PyTorch Version: {torch.__version__}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ GPU Disponibile: {torch.cuda.get_device_name(0)}\")\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è ATTENZIONE: GPU non rilevata. Vai su 'Settings' > 'Accelerator' e seleziona GPU P100 o T4.\")\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "# Verifica Einops (lo useremo molto nel modello ricorsivo)\n",
        "print(f\"Einops installato correttamente.\")\n",
        "\n",
        "# Configurazione base per riproducibilit√† (importante per la tesi)\n",
        "def set_seed(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "print(\"‚úÖ Ambiente pronto per il modello TinyRecursive.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import h5py\n",
        "# import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "from dataclasses import dataclass, asdict, replace, field\n",
        "import json\n",
        "import wandb\n",
        "import optuna\n",
        "from datetime import datetime\n",
        "from transformers import CLIPTokenizer, CLIPTextModel\n",
        "\n",
        "# Setup device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Seed per riproducibilit√†\n",
        "def set_seed(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# =========================\n",
        "# Configuration utilities\n",
        "# =========================\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    \"\"\"Container strutturato per gli iperparametri di training/model.\"\"\"\n",
        "\n",
        "    lr: float = 3e-4\n",
        "    hidden_dim: int = 256\n",
        "    num_recursions: int = 8\n",
        "    num_slots: int = 4\n",
        "    epochs: int = 20\n",
        "    batch_size: int = 64\n",
        "    weight_decay: float = 1e-4\n",
        "    grad_clip: Optional[float] = 1.0\n",
        "    sched_T0: Optional[int] = None\n",
        "    sched_T_mult: int = 1\n",
        "    lr_min: float = 1e-6\n",
        "    warmup_epochs: int = 3\n",
        "    early_stop_patience: Optional[int] = None\n",
        "    save_path: str = 'best_model.pt'\n",
        "    use_pretrained_encoder: bool = True\n",
        "    freeze_backbone: bool = True\n",
        "    augmentation: bool = False\n",
        "    dropout: float = 0.1\n",
        "    encoder_dropout: float = 0.1\n",
        "    use_text_prompts: bool = True\n",
        "    text_encoder_name: str = 'openai/clip-vit-large-patch14'\n",
        "    train_text_encoder: bool = False\n",
        "    text_dropout: float = 0.1\n",
        "\n",
        "    def to_dict(self) -> Dict[str, Any]:\n",
        "        return asdict(self)\n",
        "\n",
        "    def label(self) -> str:\n",
        "        return f\"lr{self.lr}_h{self.hidden_dim}_rec{self.num_recursions}_bs{self.batch_size}\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class HyperparameterSearchSpace:\n",
        "    lr: List[float]\n",
        "    hidden_dim: List[int]\n",
        "    num_recursions: List[int]\n",
        "    batch_size: List[int]\n",
        "    weight_decay: List[float]\n",
        "    pretrained_encoder: List[bool]\n",
        "    freeze_backbone: List[bool]\n",
        "    augmentation: List[bool]\n",
        "    dropout: List[float]\n",
        "    text_encoder_name: List[str]\n",
        "    train_text_encoder: List[bool]\n",
        "    text_dropout: List[float]\n",
        "    num_slots: List[int] = field(default_factory=lambda: [4])\n",
        "\n",
        "    def as_optuna_space(self) -> Dict[str, List[Any]]:\n",
        "        return {\n",
        "            'lr': self.lr,\n",
        "            'hidden_dim': self.hidden_dim,\n",
        "            'num_recursions': self.num_recursions,\n",
        "            'batch_size': self.batch_size,\n",
        "            'weight_decay': self.weight_decay,\n",
        "            'pretrained_encoder': self.pretrained_encoder,\n",
        "            'freeze_backbone': self.freeze_backbone,\n",
        "            'augmentation': self.augmentation,\n",
        "            'dropout': self.dropout,\n",
        "            'text_encoder_name': self.text_encoder_name,\n",
        "            'train_text_encoder': self.train_text_encoder,\n",
        "            'text_dropout': self.text_dropout,\n",
        "            'num_slots': self.num_slots,\n",
        "        }\n",
        "\n",
        "# Numero 1\n",
        "def default_search_space() -> HyperparameterSearchSpace:\n",
        "    \"\"\"Restituisce lo spazio di ricerca richiesto dall'utente.\"\"\"\n",
        "\n",
        "    return HyperparameterSearchSpace(\n",
        "        lr=[1e-5, 1e-6, 1e-4, 1e-3],\n",
        "        hidden_dim=[128, 256, 512],\n",
        "        num_recursions=[8, 12, 16],\n",
        "        batch_size=[32, 64, 128, 256, 512],\n",
        "        weight_decay=[0.1, 0.5, 1.0],\n",
        "        pretrained_encoder=[False],\n",
        "        freeze_backbone=[False],\n",
        "        augmentation=[True, False],\n",
        "        dropout=[0.1, 0.3, 0.5, 0.7],\n",
        "        text_encoder_name=[\n",
        "            'openai/clip-vit-base-patch32',\n",
        "            'openai/clip-vit-large-patch14'\n",
        "        ],\n",
        "        train_text_encoder=[False, True],\n",
        "        text_dropout=[0.05, 0.1, 0.2, 0.3],\n",
        "        num_slots=[4, 6, 8]\n",
        "    )\n",
        "#Numero 2\n",
        "'''\n",
        "def default_search_space() -> HyperparameterSearchSpace:\n",
        "    \"\"\"Restituisce lo spazio di ricerca richiesto dall'utente.\"\"\"\n",
        "\n",
        "    return HyperparameterSearchSpace(\n",
        "        lr=[1e-5, 1e-6, 1e-4, 1e-3],\n",
        "        hidden_dim=[128, 256, 512],\n",
        "        num_recursions=[8, 12, 16],\n",
        "        batch_size=[32, 64, 128, 256, 512],\n",
        "        weight_decay=[0.1, 0.5, 1.0],\n",
        "        pretrained_encoder=[False],\n",
        "        freeze_backbone=[True],\n",
        "        augmentation=[True, False],\n",
        "        dropout=[0.1, 0.3, 0.5, 0.7],\n",
        "    )\n",
        "'''\n",
        "\n",
        "# Numero 3\n",
        "'''\n",
        "def default_search_space() -> HyperparameterSearchSpace:\n",
        "    \"\"\"Restituisce lo spazio di ricerca richiesto dall'utente.\"\"\"\n",
        "\n",
        "    return HyperparameterSearchSpace(\n",
        "        lr=[1e-5, 1e-6, 1e-4, 1e-3],\n",
        "        hidden_dim=[128, 256, 512],\n",
        "        num_recursions=[8, 12, 16],\n",
        "        batch_size=[32, 64, 128, 256, 512],\n",
        "        weight_decay=[0.1, 0.5, 1.0],\n",
        "        pretrained_encoder=[True],\n",
        "        freeze_backbone=[False],\n",
        "        augmentation=[True, False],\n",
        "        dropout=[0.1, 0.3, 0.5, 0.7],\n",
        "    )\n",
        "'''\n",
        "\n",
        "# Numero 4\n",
        "'''\n",
        "def default_search_space() -> HyperparameterSearchSpace:\n",
        "    \"\"\"Restituisce lo spazio di ricerca richiesto dall'utente.\"\"\"\n",
        "\n",
        "    return HyperparameterSearchSpace(\n",
        "        lr=[1e-5, 1e-6, 1e-4, 1e-3],\n",
        "        hidden_dim=[128, 256, 512],\n",
        "        num_recursions=[8, 12, 16],\n",
        "        batch_size=[32, 64, 128, 256, 512],\n",
        "        weight_decay=[0.1, 0.5, 1.0],\n",
        "        pretrained_encoder=[True],\n",
        "        freeze_backbone=[True],\n",
        "        augmentation=[True, False],\n",
        "        dropout=[0.1, 0.3, 0.5, 0.7],\n",
        "    )\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "\n",
        "# --- HELPER FUNCTIONS (Kept largely the same) ---\n",
        "\n",
        "def load_images_robust(dataset):\n",
        "    \"\"\"\n",
        "    Carica immagini da dataset HDF5 usando metodo robusto.\n",
        "    \"\"\"\n",
        "    shape = dataset.shape\n",
        "    \n",
        "    # METODO 1: Lettura diretta uint8\n",
        "    try:\n",
        "        buffer = np.empty(shape, dtype=np.uint8)\n",
        "        dataset.read_direct(buffer)\n",
        "        return buffer\n",
        "    except Exception:\n",
        "        pass\n",
        "    \n",
        "    # METODO 2: Float32 -> Uint8\n",
        "    try:\n",
        "        buffer = np.empty(shape, dtype=np.float32)\n",
        "        dataset.read_direct(buffer)\n",
        "        if buffer.max() <= 1.0:\n",
        "            buffer = (buffer * 255).astype(np.uint8)\n",
        "        else:\n",
        "            buffer = np.clip(buffer, 0, 255).astype(np.uint8)\n",
        "        return buffer\n",
        "    except Exception:\n",
        "        pass\n",
        "    \n",
        "    # METODO 3: Float64 -> Uint8\n",
        "    try:\n",
        "        buffer = np.empty(shape, dtype=np.float64)\n",
        "        dataset.read_direct(buffer)\n",
        "        if buffer.max() <= 1.0:\n",
        "            buffer = (buffer * 255).astype(np.uint8)\n",
        "        else:\n",
        "            buffer = np.clip(buffer, 0, 255).astype(np.uint8)\n",
        "        return buffer\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # METODO 4: Fallback bytes\n",
        "    try:\n",
        "        buffer = np.empty(shape, dtype=np.uint8)\n",
        "        dataset.id.read(h5py.h5s.ALL, h5py.h5s.ALL, buffer)\n",
        "        return buffer\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Impossibile leggere il dataset: {e}\")\n",
        "\n",
        "def load_actions_robust(dataset):\n",
        "    \"\"\"\n",
        "    Carica azioni da dataset HDF5.\n",
        "    \"\"\"\n",
        "    shape = dataset.shape\n",
        "    try:\n",
        "        buffer = np.empty(shape, dtype=np.float32)\n",
        "        dataset.read_direct(buffer)\n",
        "        return buffer\n",
        "    except Exception:\n",
        "        pass\n",
        "    \n",
        "    try:\n",
        "        buffer = np.empty(shape, dtype=np.float64)\n",
        "        dataset.read_direct(buffer)\n",
        "        return buffer.astype(np.float32)\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Impossibile leggere le azioni: {e}\")\n",
        "\n",
        "# --- MODIFIED EXPLORATION FUNCTION (No Matplotlib) ---\n",
        "\n",
        "def explore_libero_dataset(data_path: Path):\n",
        "    \n",
        "    # Trova file\n",
        "    hdf5_files = list(data_path.glob('**/*.hdf5'))\n",
        "    \n",
        "    if not hdf5_files:\n",
        "        print(f\"‚ö†Ô∏è Nessun file HDF5 trovato in {data_path}\")\n",
        "        return []\n",
        "    \n",
        "    print(f\"‚úÖ Trovati {len(hdf5_files)} file HDF5\")\n",
        "    \n",
        "    # Analizza il primo file\n",
        "    demo_file = hdf5_files[0]\n",
        "    print(f\"\\nüìÑ Analizzando: {demo_file.name}\")\n",
        "    \n",
        "    try:\n",
        "        with h5py.File(demo_file, 'r') as f:\n",
        "            if 'data' not in f:\n",
        "                print(\"‚ö†Ô∏è Chiave 'data' non trovata\")\n",
        "                return hdf5_files\n",
        "            \n",
        "            data_group = f['data']\n",
        "            demo_keys = list(data_group.keys())\n",
        "            first_demo_key = demo_keys[0]\n",
        "            demo_0 = data_group[first_demo_key]\n",
        "            \n",
        "            imgs = None\n",
        "            \n",
        "            # 1. Caricamento Immagini\n",
        "            if 'obs' in demo_0:\n",
        "                obs_group = demo_0['obs']\n",
        "                \n",
        "                # Strategia di ricerca chiave immagine\n",
        "                image_keys = ['agentview_rgb', 'agentview_image', 'rgb', 'image', 'robot0_eye_in_hand_image']\n",
        "                img_key = next((k for k in image_keys if k in obs_group), None)\n",
        "                \n",
        "                # Fallback ricerca generica\n",
        "                if img_key is None:\n",
        "                    img_key = next((k for k in obs_group.keys() if 'rgb' in k.lower() or 'image' in k.lower()), None)\n",
        "                \n",
        "                if img_key:\n",
        "                    print(f\"\\nüñºÔ∏è Usando chiave immagini: '{img_key}'\")\n",
        "                    try:\n",
        "                        imgs = load_images_robust(obs_group[img_key])\n",
        "                        print(f\"  ‚úÖ Immagini caricate: {imgs.shape}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"  ‚ùå Errore immagini: {e}\")\n",
        "            \n",
        "            # 2. Caricamento Azioni\n",
        "            if 'actions' in demo_0:\n",
        "                try:\n",
        "                    actions = load_actions_robust(demo_0['actions'])\n",
        "                    print(f\"\\nüéÆ Azioni caricate: {actions.shape}\")\n",
        "                    print(f\"  Range: [{actions.min():.3f}, {actions.max():.3f}]\")\n",
        "                except Exception as e:\n",
        "                    print(f\"  ‚ùå Errore azioni: {e}\")\n",
        "\n",
        "            # 3. VISUALIZZAZIONE (Senza Matplotlib)\n",
        "            if imgs is not None and len(imgs) > 0:\n",
        "                print(\"\\nüé¨ Visualizzazione frame esempio (PIL/IPython):\")\n",
        "                \n",
        "                num_frames = min(4, len(imgs))\n",
        "                indices = np.linspace(0, len(imgs) - 1, num_frames, dtype=int)\n",
        "                \n",
        "                for idx in indices:\n",
        "                    img_array = imgs[idx]\n",
        "                    \n",
        "                    # Se l'immagine √® float [0,1], converti a uint8\n",
        "                    if img_array.dtype != np.uint8:\n",
        "                         img_array = (np.clip(img_array, 0, 1) * 255).astype(np.uint8)\n",
        "                    \n",
        "                    # Crea immagine PIL\n",
        "                    pil_img = Image.fromarray(img_array)\n",
        "                    \n",
        "                    # (Opzionale) Resize per non occupare troppo spazio\n",
        "                    # pil_img = pil_img.resize((128, 128))\n",
        "                    \n",
        "                    print(f\"--- Frame {idx} ---\")\n",
        "                    display(pil_img)\n",
        "            else:\n",
        "                print(\"\\n‚ö†Ô∏è Nessuna immagine valida da visualizzare\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Errore critico durante l'apertura del file: {e}\")\n",
        "    \n",
        "    return hdf5_files\n",
        "\n",
        "# Esegui\n",
        "hdf5_files = explore_libero_dataset(Path('dataset/libero_spatial'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LIBERODataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset per dimostrazioni LIBERO\n",
        "    \n",
        "    Carica osservazioni visive e azioni da file HDF5.\n",
        "    Supporta data augmentation e normalizzazione.\n",
        "    Gestisce automaticamente problemi di dtype non standard nei file HDF5.\n",
        "    \n",
        "    Supporta demo-level split: invece di dividere i file, divide le demo\n",
        "    all'interno di ciascun file (es. 80% train, 20% val per ogni file).\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        hdf5_files: List[Path],\n",
        "        sequence_length: int = 1,\n",
        "        image_size: Tuple[int, int] = (128, 128),\n",
        "        normalize_actions: bool = True,\n",
        "        augmentation: bool = False,\n",
        "        max_demos_per_task: Optional[int] = None,\n",
        "        demo_split_ratio: float = 0.8,\n",
        "        is_train: bool = True,\n",
        "        action_stats: Optional[Dict] = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            hdf5_files: lista di path ai file HDF5\n",
        "            sequence_length: lunghezza delle sequenze (1 = single-step prediction)\n",
        "            image_size: dimensioni delle immagini\n",
        "            normalize_actions: se True, normalizza le azioni con z-score\n",
        "            augmentation: se True, applica data augmentation\n",
        "            max_demos_per_task: limite massimo di demo per task (per debugging)\n",
        "            demo_split_ratio: percentuale di demo per training (default 0.8 = 80%)\n",
        "            is_train: se True, usa le prime demo_split_ratio% demo; se False, usa il resto\n",
        "            action_stats: statistiche azioni pre-calcolate (per validation set)\n",
        "        \"\"\"\n",
        "        self.hdf5_files = hdf5_files\n",
        "        self.sequence_length = sequence_length\n",
        "        self.image_size = (int(image_size[0]), int(image_size[1]))\n",
        "        self.augmentation = augmentation and is_train\n",
        "        self.normalize_actions = normalize_actions\n",
        "        self.demo_split_ratio = demo_split_ratio\n",
        "        self.is_train = is_train\n",
        "        \n",
        "        # Carica tutti i dati in memoria (assumendo dataset gestibile)\n",
        "        self.data = []\n",
        "        self.action_stats = action_stats if action_stats is not None else {'mean': None, 'std': None}\n",
        "        self.samples: List[Tuple[int, int]] = []  # (demo_idx, start_idx)\n",
        "        \n",
        "        split_name = \"TRAIN\" if is_train else \"VAL\"\n",
        "        print(f\"Loading {len(hdf5_files)} HDF5 files for {split_name} (demo split: {demo_split_ratio:.0%})...\")\n",
        "        all_actions = []\n",
        "        \n",
        "        for hdf5_file in hdf5_files:\n",
        "            try:\n",
        "                with h5py.File(hdf5_file, 'r') as f:\n",
        "                    if 'data' not in f:\n",
        "                        print(f\"‚ö†Ô∏è 'data' key not found in {hdf5_file.name}, skipping...\")\n",
        "                        continue\n",
        "                    \n",
        "                    demo_keys = list(f['data'].keys())\n",
        "                    \n",
        "                    # Limita numero di demo se richiesto\n",
        "                    if max_demos_per_task is not None:\n",
        "                        demo_keys = demo_keys[:max_demos_per_task]\n",
        "                    \n",
        "                    # Demo-level split: seleziona subset di demo in base a is_train\n",
        "                    n_demos = len(demo_keys)\n",
        "                    n_train_demos = int(n_demos * demo_split_ratio)\n",
        "                    \n",
        "                    if is_train:\n",
        "                        # Training: prime n_train_demos demo\n",
        "                        selected_demo_keys = demo_keys[:n_train_demos]\n",
        "                    else:\n",
        "                        # Validation: demo rimanenti\n",
        "                        selected_demo_keys = demo_keys[n_train_demos:]\n",
        "                    \n",
        "                    if len(selected_demo_keys) == 0:\n",
        "                        print(f\"‚ö†Ô∏è No demos selected from {hdf5_file.name} for {split_name}, skipping...\")\n",
        "                        continue\n",
        "                    \n",
        "                    task_prompt = self._prompt_from_filename(hdf5_file)\n",
        "\n",
        "                    for demo_key in selected_demo_keys:\n",
        "                        try:\n",
        "                            demo = f[f'data/{demo_key}']\n",
        "                            \n",
        "                            # Trova la chiave delle immagini\n",
        "                            obs_group = demo['obs']\n",
        "                            img_key = self._find_image_key(obs_group)\n",
        "                            \n",
        "                            if img_key is None:\n",
        "                                print(f\"‚ö†Ô∏è No image key found in {hdf5_file.name}/{demo_key}, skipping...\")\n",
        "                                continue\n",
        "                            \n",
        "                            # Carica osservazioni con metodo robusto\n",
        "                            obs = self._load_images_robust(obs_group[img_key])\n",
        "                            \n",
        "                            # Carica azioni con metodo robusto\n",
        "                            actions = self._load_actions_robust(demo['actions'])\n",
        "                            \n",
        "                            # Verifica che obs e actions abbiano lunghezze compatibili\n",
        "                            min_len = min(len(obs), len(actions))\n",
        "                            if min_len < self.sequence_length:\n",
        "                                print(f\"‚ö†Ô∏è Demo too short ({min_len} < {self.sequence_length}), skipping...\")\n",
        "                                continue\n",
        "                            \n",
        "                            obs = obs[:min_len]\n",
        "                            actions = actions[:min_len]\n",
        "                            \n",
        "                            # Aggiungi alla lista\n",
        "                            self.data.append({\n",
        "                                'observations': obs,\n",
        "                                'actions': actions,\n",
        "                                'prompt': task_prompt\n",
        "                            })\n",
        "                            \n",
        "                            all_actions.append(actions)\n",
        "                            \n",
        "                        except Exception as e:\n",
        "                            print(f\"‚ö†Ô∏è Error loading demo {demo_key} from {hdf5_file.name}: {e}\")\n",
        "                            continue\n",
        "                            \n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error opening file {hdf5_file}: {e}\")\n",
        "                continue\n",
        "        \n",
        "        print(f\"‚úÖ Loaded {len(self.data)} demonstrations for {split_name}\")\n",
        "        \n",
        "        if len(self.data) == 0:\n",
        "            raise ValueError(f\"No valid demonstrations loaded for {split_name}! Check your data files.\")\n",
        "        \n",
        "        # Calcola statistiche azioni per normalizzazione (solo per training set o se non fornite)\n",
        "        if self.normalize_actions and len(all_actions) > 0 and action_stats is None:\n",
        "            all_actions_concat = np.concatenate(all_actions, axis=0)\n",
        "        \n",
        "            mean = all_actions_concat.mean(axis=0).astype(np.float32)\n",
        "            std  = all_actions_concat.std(axis=0).astype(np.float32)\n",
        "        \n",
        "            # ‚ö†Ô∏è Floor di sicurezza: evita std troppo piccole che esplodono la normalizzazione\n",
        "            std_clipped = np.clip(std, 0.1, None)\n",
        "        \n",
        "            # Log dettagliato\n",
        "            print(f\"üìä Action statistics computed from {split_name} set:\")\n",
        "            print(f\"   Mean:        {np.round(mean, 3)}\")\n",
        "            print(f\"   Std (raw):   {np.round(std, 3)}\")\n",
        "            print(f\"   Std (clipped to >=0.1): {np.round(std_clipped, 3)}\")\n",
        "        \n",
        "            self.action_stats['mean'] = mean\n",
        "            self.action_stats['std']  = std_clipped\n",
        "        \n",
        "        elif action_stats is not None:\n",
        "            print(f\"üìä Using provided action statistics\")\n",
        "            self.action_stats = {\n",
        "                'mean': action_stats['mean'].astype(np.float32),\n",
        "                'std':  np.clip(action_stats['std'], 0.1, None).astype(np.float32)\n",
        "            }\n",
        "\n",
        "        # Costruisci indice delle transizioni per accesso O(1)\n",
        "        self.samples = self._build_sample_index()\n",
        "        print(f\"üì¶ Generated {len(self.samples)} transitions for {split_name}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def _prompt_from_filename(hdf5_file: Path) -> str:\n",
        "        \"\"\"Converte il nome del file HDF5 in un prompt naturale.\"\"\"\n",
        "        name = hdf5_file.stem\n",
        "        if name.endswith('_demo'):\n",
        "            name = name[:-5]\n",
        "        name = name.replace('_', ' ').replace('-', ' ')\n",
        "        return ' '.join(name.split()).strip()\n",
        "\n",
        "    \n",
        "    def _find_image_key(self, obs_group) -> Optional[str]:\n",
        "        \"\"\"Trova la chiave corretta per le immagini nel gruppo obs\"\"\"\n",
        "        # Lista di possibili chiavi per le immagini (in ordine di priorit√†)\n",
        "        possible_keys = [\n",
        "            'agentview_rgb',\n",
        "            'agentview_image', \n",
        "            'rgb',\n",
        "            'image',\n",
        "            'robot0_eye_in_hand_image',\n",
        "            'frontview_image',\n",
        "            'sideview_image'\n",
        "        ]\n",
        "        \n",
        "        obs_keys = list(obs_group.keys())\n",
        "        \n",
        "        # Prima cerca chiavi note\n",
        "        for key in possible_keys:\n",
        "            if key in obs_keys:\n",
        "                return key\n",
        "        \n",
        "        # Poi cerca qualsiasi chiave che contiene 'rgb' o 'image'\n",
        "        for key in obs_keys:\n",
        "            if 'rgb' in key.lower() or 'image' in key.lower():\n",
        "                return key\n",
        "        \n",
        "        return None\n",
        "    \n",
        "    def _load_images_robust(self, dataset) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Carica immagini da dataset HDF5 usando metodo robusto che bypassa problemi dtype.\n",
        "        \"\"\"\n",
        "        shape = dataset.shape\n",
        "        \n",
        "        # METODO 1: Prova lettura diretta come uint8\n",
        "        try:\n",
        "            buffer = np.empty(shape, dtype=np.uint8)\n",
        "            dataset.read_direct(buffer)\n",
        "            return buffer\n",
        "        except Exception:\n",
        "            pass\n",
        "        \n",
        "        # METODO 2: Prova come float32 e converti\n",
        "        try:\n",
        "            buffer = np.empty(shape, dtype=np.float32)\n",
        "            dataset.read_direct(buffer)\n",
        "            if buffer.max() <= 1.0:\n",
        "                buffer = (buffer * 255).astype(np.uint8)\n",
        "            else:\n",
        "                buffer = np.clip(buffer, 0, 255).astype(np.uint8)\n",
        "            return buffer\n",
        "        except Exception:\n",
        "            pass\n",
        "        \n",
        "        # METODO 3: Prova come float64\n",
        "        try:\n",
        "            buffer = np.empty(shape, dtype=np.float64)\n",
        "            dataset.read_direct(buffer)\n",
        "            if buffer.max() <= 1.0:\n",
        "                buffer = (buffer * 255).astype(np.uint8)\n",
        "            else:\n",
        "                buffer = np.clip(buffer, 0, 255).astype(np.uint8)\n",
        "            return buffer\n",
        "        except Exception:\n",
        "            pass\n",
        "        \n",
        "        # METODO 4: Lettura raw\n",
        "        try:\n",
        "            buffer = np.empty(shape, dtype=np.uint8)\n",
        "            dataset.id.read(h5py.h5s.ALL, h5py.h5s.ALL, buffer)\n",
        "            return buffer\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Cannot read image dataset: {e}\")\n",
        "    \n",
        "    def _load_actions_robust(self, dataset) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Carica azioni da dataset HDF5 usando metodo robusto.\n",
        "        \"\"\"\n",
        "        shape = dataset.shape\n",
        "        \n",
        "        # METODO 1: Prova lettura diretta come float32\n",
        "        try:\n",
        "            buffer = np.empty(shape, dtype=np.float32)\n",
        "            dataset.read_direct(buffer)\n",
        "            return buffer\n",
        "        except Exception:\n",
        "            pass\n",
        "        \n",
        "        # METODO 2: Prova come float64 e converti\n",
        "        try:\n",
        "            buffer = np.empty(shape, dtype=np.float64)\n",
        "            dataset.read_direct(buffer)\n",
        "            return buffer.astype(np.float32)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Cannot read actions dataset: {e}\")\n",
        "    \n",
        "    def _build_sample_index(self) -> List[Tuple[int, int]]:\n",
        "        \"\"\"Pre-calcola gli indici (demo_idx, start_idx) per ogni transizione\"\"\"\n",
        "        indices: List[Tuple[int, int]] = []\n",
        "        for demo_idx, demo in enumerate(self.data):\n",
        "            demo_transitions = len(demo['observations']) - self.sequence_length + 1\n",
        "            if demo_transitions <= 0:\n",
        "                continue\n",
        "            indices.extend((demo_idx, start) for start in range(demo_transitions))\n",
        "        if not indices:\n",
        "            raise ValueError(\"Dataset index is empty after preprocessing\")\n",
        "        return indices\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Numero totale di transizioni disponibili\"\"\"\n",
        "        return len(self.samples)\n",
        "    \n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Restituisce una transizione (osservazione, azione).\n",
        "        \n",
        "        Args:\n",
        "            idx: indice della transizione\n",
        "            \n",
        "        Returns:\n",
        "            Dict con 'observations' e 'actions' come tensori\n",
        "        \"\"\"\n",
        "        demo_idx, start_idx = self.samples[idx]\n",
        "        demo = self.data[demo_idx]\n",
        "        end_idx = start_idx + self.sequence_length\n",
        "\n",
        "        obs = demo['observations'][start_idx:end_idx].copy()\n",
        "        actions = demo['actions'][start_idx:end_idx].copy()\n",
        "\n",
        "        # Preprocessing\n",
        "        obs = self._preprocess_obs(obs)\n",
        "        actions = self._preprocess_actions(actions)\n",
        "\n",
        "        # Per single-step prediction, restituisci solo primo elemento\n",
        "        if self.sequence_length == 1:\n",
        "            obs = obs[0]\n",
        "            actions = actions[0]\n",
        "\n",
        "        # Converti HWC -> CHW per PyTorch\n",
        "        if obs.ndim == 3:  # Single image: (H, W, C) -> (C, H, W)\n",
        "            obs = np.transpose(obs, (2, 0, 1))\n",
        "        elif obs.ndim == 4:  # Sequence: (T, H, W, C) -> (T, C, H, W)\n",
        "            obs = np.transpose(obs, (0, 3, 1, 2))\n",
        "\n",
        "        return {\n",
        "            'observations': torch.from_numpy(obs).float(),\n",
        "            'actions': torch.from_numpy(actions).float(),\n",
        "            'prompt': demo.get('prompt', '')\n",
        "        }\n",
        "    \n",
        "    def _preprocess_obs(self, obs: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Preprocessing delle osservazioni\"\"\"\n",
        "        processed = []\n",
        "        target_h, target_w = self.image_size\n",
        "        for img in obs:\n",
        "            if img.shape[0] != target_h or img.shape[1] != target_w:\n",
        "                img = cv2.resize(img, (target_w, target_h), interpolation=cv2.INTER_AREA)\n",
        "            processed.append(img)\n",
        "        obs = np.stack(processed, axis=0)\n",
        "\n",
        "        # Normalizza [0, 255] -> [0, 1]\n",
        "        obs = obs.astype(np.float32) / 255.0\n",
        "\n",
        "        # Data augmentation (se abilitato)\n",
        "        if self.augmentation:\n",
        "            obs = self._augment_obs(obs)\n",
        "        \n",
        "        return obs\n",
        "    def _augment_obs(self, obs: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Data augmentation per osservazioni\"\"\"\n",
        "        # Color jitter (brightness)\n",
        "        if np.random.rand() < 0.5:\n",
        "            brightness = np.random.uniform(0.8, 1.2)\n",
        "            obs = np.clip(obs * brightness, 0, 1)\n",
        "        \n",
        "        # Contrast adjustment\n",
        "        if np.random.rand() < 0.3:\n",
        "            contrast = np.random.uniform(0.8, 1.2)\n",
        "            mean = obs.mean(axis=(1, 2), keepdims=True)\n",
        "            obs = np.clip((obs - mean) * contrast + mean, 0, 1)\n",
        "        \n",
        "        # Random crop (con padding)\n",
        "        if np.random.rand() < 0.3:\n",
        "            crop_ratio = np.random.uniform(0.85, 0.95)\n",
        "            crop_size_h = int(self.image_size[0] * crop_ratio)\n",
        "            crop_size_w = int(self.image_size[1] * crop_ratio)\n",
        "            \n",
        "            start_y = np.random.randint(0, self.image_size[0] - crop_size_h + 1)\n",
        "            start_x = np.random.randint(0, self.image_size[1] - crop_size_w + 1)\n",
        "            \n",
        "            cropped = []\n",
        "            for img in obs:\n",
        "                img_crop = img[start_y:start_y+crop_size_h, start_x:start_x+crop_size_w]\n",
        "                img_resized = cv2.resize(img_crop, (self.image_size[1], self.image_size[0]))\n",
        "                cropped.append(img_resized)\n",
        "            obs = np.stack(cropped)\n",
        "        \n",
        "        return obs\n",
        "    \n",
        "    def _preprocess_actions(self, actions: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Preprocessing delle azioni con normalizzazione z-score\"\"\"\n",
        "        actions = actions.astype(np.float32)\n",
        "        if self.action_stats['mean'] is not None:\n",
        "            actions = (actions - self.action_stats['mean']) / self.action_stats['std']\n",
        "        \n",
        "        return actions\n",
        "    \n",
        "    def get_action_stats(self) -> Dict[str, np.ndarray]:\n",
        "        \"\"\"Restituisce le statistiche delle azioni per denormalizzazione\"\"\"\n",
        "        return self.action_stats.copy()\n",
        "    \n",
        "    def denormalize_actions(self, actions: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Denormalizza le azioni per l'esecuzione nel simulatore\"\"\"\n",
        "        if self.action_stats['mean'] is not None:\n",
        "            return actions * self.action_stats['std'] + self.action_stats['mean']\n",
        "        return actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PretrainedVisualEncoder(nn.Module):\n",
        "    \"\"\"Visual encoder basato su ResNet18 con testa adattiva.\"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dim: int = 256, freeze_backbone: bool = True, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        resnet = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "        if freeze_backbone:\n",
        "            for param in resnet.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "\n",
        "        self.adapter = nn.Sequential(\n",
        "            nn.Linear(512, 512),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, hidden_dim)\n",
        "        )\n",
        "        self.ln = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
        "        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.adapter.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = x.float()\n",
        "\n",
        "        if x.shape[-1] != 224 or x.shape[-2] != 224:\n",
        "            x = nn.functional.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "\n",
        "        x = (x - self.mean) / self.std\n",
        "        features = self.backbone(x).flatten(start_dim=1)\n",
        "        output = self.adapter(features)\n",
        "        return self.ln(output)\n",
        "\n",
        "\n",
        "class VisualEncoder(nn.Module):\n",
        "    def __init__(self, obs_shape: Tuple[int, int, int] = (3, 128, 128), hidden_dim: int = 256, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        c, _, _ = obs_shape\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(c, 32, 3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 256, 3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.conv(x)\n",
        "        return self.head(x)\n",
        "\n",
        "\n",
        "class PromptEncoder(nn.Module):\n",
        "    \"\"\"Encodes natural-language task prompts via CLIP ViT-L/14 text tower.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_dim: int,\n",
        "        model_name: str = 'openai/clip-vit-large-patch14',\n",
        "        trainable: bool = False,\n",
        "        dropout: float = 0.1,\n",
        "        max_length: int = 77\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.tokenizer = CLIPTokenizer.from_pretrained(model_name)\n",
        "        self.text_model = CLIPTextModel.from_pretrained(model_name)\n",
        "        self.max_length = min(max_length, getattr(self.text_model.config, 'max_position_embeddings', max_length))\n",
        "\n",
        "        if not trainable:\n",
        "            self.text_model.eval()\n",
        "            for param in self.text_model.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        text_hidden = self.text_model.config.hidden_size\n",
        "        self.adapter = nn.Sequential(\n",
        "            nn.LayerNorm(text_hidden),\n",
        "            nn.Linear(text_hidden, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        self._token_cache: Dict[str, Dict[str, torch.Tensor]] = {}\n",
        "\n",
        "    def _tokenize(self, prompt: str) -> Dict[str, torch.Tensor]:\n",
        "        if prompt not in self._token_cache:\n",
        "            tokens = self.tokenizer(\n",
        "                prompt,\n",
        "                return_tensors='pt',\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                max_length=self.max_length\n",
        "            )\n",
        "            self._token_cache[prompt] = {k: v for k, v in tokens.items()}\n",
        "        cached = self._token_cache[prompt]\n",
        "        return {k: v.clone() for k, v in cached.items()}\n",
        "\n",
        "    def forward(self, prompts: List[str], device: torch.device) -> torch.Tensor:\n",
        "        if not prompts:\n",
        "            raise ValueError(\"PromptEncoder received an empty batch of prompts\")\n",
        "\n",
        "        token_batches = [self._tokenize(p) for p in prompts]\n",
        "        batch = {\n",
        "            key: torch.cat([tokens[key] for tokens in token_batches], dim=0).to(device)\n",
        "            for key in token_batches[0]\n",
        "        }\n",
        "\n",
        "        outputs = self.text_model(**batch)\n",
        "        pooled = outputs.pooler_output if outputs.pooler_output is not None else outputs.last_hidden_state[:, -1, :]\n",
        "        return self.adapter(pooled)\n",
        "\n",
        "\n",
        "class RecursiveBlock(nn.Module):\n",
        "    \"\"\"Slot-based TRM block with cross- then self-attention updates.\"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dim=256, num_heads=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cross_attn = nn.MultiheadAttention(\n",
        "            hidden_dim,\n",
        "            num_heads,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.self_attn = nn.MultiheadAttention(\n",
        "            hidden_dim,\n",
        "            num_heads,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.slot_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.cond_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.self_attn_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.mlp_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim * 4, hidden_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.gru = nn.GRUCell(hidden_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, slots: torch.Tensor, cond_tokens: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            slots: tensor (B, S, D) con gli slot correnti.\n",
        "            cond_tokens: tensor (B, T, D) con i token di condizionamento.\n",
        "        Returns:\n",
        "            slots aggiornati (B, S, D).\n",
        "        \"\"\"\n",
        "        bsz, num_slots, dim = slots.shape\n",
        "\n",
        "        # Cross-attention: gli slot interrogano i token (visivi/testuali).\n",
        "        slots_norm = self.slot_norm(slots)\n",
        "        cond_norm = self.cond_norm(cond_tokens)\n",
        "        cross_out, _ = self.cross_attn(slots_norm, cond_norm, cond_norm)\n",
        "        gru_input = cross_out.reshape(-1, dim)\n",
        "        prev_slots = slots.reshape(-1, dim)\n",
        "        slots = self.gru(gru_input, prev_slots).view(bsz, num_slots, dim)\n",
        "\n",
        "        # Self-attention tra slot per coordinare le ipotesi.\n",
        "        attn_in = self.self_attn_norm(slots)\n",
        "        self_out, _ = self.self_attn(attn_in, attn_in, attn_in)\n",
        "        slots = slots + self.dropout(self_out)\n",
        "\n",
        "        # MLP residua per raffinamento.\n",
        "        mlp_out = self.mlp(self.mlp_norm(slots))\n",
        "        slots = slots + self.dropout(mlp_out)\n",
        "        return slots\n",
        "\n",
        "\n",
        "class TRMPolicy(nn.Module):\n",
        "    \"\"\"Policy TRM con slot-attention e fusione vision-language compatibile con la letteratura.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        obs_shape=(3, 128, 128),\n",
        "        action_dim=7,\n",
        "        hidden_dim=256,\n",
        "        num_heads=4,\n",
        "        num_recursions=8,\n",
        "        num_slots=4,\n",
        "        dropout=0.1,\n",
        "        adaptive_halt=False,\n",
        "        use_pretrained_encoder=True,\n",
        "        freeze_backbone=True,\n",
        "        encoder_dropout=0.1,\n",
        "        use_text_prompts=True,\n",
        "        text_encoder_name='openai/clip-vit-large-patch14',\n",
        "        train_text_encoder=False,\n",
        "        text_dropout=0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_recursions = num_recursions\n",
        "        self.num_slots = num_slots\n",
        "        self.adaptive_halt = adaptive_halt\n",
        "        self.use_text_prompts = use_text_prompts\n",
        "\n",
        "        if use_pretrained_encoder:\n",
        "            self.encoder = PretrainedVisualEncoder(\n",
        "                hidden_dim=hidden_dim,\n",
        "                freeze_backbone=freeze_backbone,\n",
        "                dropout=encoder_dropout\n",
        "            )\n",
        "        else:\n",
        "            self.encoder = VisualEncoder(\n",
        "                obs_shape=obs_shape,\n",
        "                hidden_dim=hidden_dim,\n",
        "                dropout=encoder_dropout\n",
        "            )\n",
        "\n",
        "        if self.use_text_prompts:\n",
        "            self.prompt_encoder = PromptEncoder(\n",
        "                hidden_dim=hidden_dim,\n",
        "                model_name=text_encoder_name,\n",
        "                trainable=train_text_encoder,\n",
        "                dropout=text_dropout\n",
        "            )\n",
        "            self.text_token_adapter = nn.Sequential(\n",
        "                nn.LayerNorm(hidden_dim),\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout)\n",
        "            )\n",
        "        else:\n",
        "            self.prompt_encoder = None\n",
        "            self.text_token_adapter = None\n",
        "\n",
        "        self.vision_token_adapter = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        self.slot_init = nn.Parameter(torch.randn(num_slots, hidden_dim))\n",
        "        self.slot_conditioning = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.slot_readout = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        self.recursive_block = RecursiveBlock(hidden_dim, num_heads, dropout)\n",
        "\n",
        "        self.action_head = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, action_dim)\n",
        "        )\n",
        "\n",
        "        if adaptive_halt:\n",
        "            self.halt_predictor = nn.Sequential(\n",
        "                nn.Linear(hidden_dim, 64),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(64, 1),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "\n",
        "    def forward(self, obs, prompts: Optional[List[str]] = None, return_all_states: bool = False):\n",
        "        B = obs.shape[0]\n",
        "        device = obs.device\n",
        "\n",
        "        vision_token = self.vision_token_adapter(self.encoder(obs)).unsqueeze(1)\n",
        "        cond_tokens = [vision_token]\n",
        "\n",
        "        if self.use_text_prompts:\n",
        "            if prompts is None:\n",
        "                raise ValueError(\"TRMPolicy richiede i prompt testuali quando use_text_prompts=True\")\n",
        "            text_features = self.prompt_encoder(prompts, device=device)\n",
        "            text_token = self.text_token_adapter(text_features).unsqueeze(1)\n",
        "            cond_tokens.append(text_token)\n",
        "\n",
        "        cond_tokens = torch.cat(cond_tokens, dim=1)\n",
        "        cond_summary = cond_tokens.mean(dim=1)\n",
        "        slots = self._init_slots(B, device, cond_summary)\n",
        "\n",
        "        state_trace: Optional[List[torch.Tensor]] = [] if return_all_states else None\n",
        "        if return_all_states and state_trace is not None:\n",
        "            state_trace.append(self.slot_readout(slots.mean(dim=1)))\n",
        "\n",
        "        if self.adaptive_halt:\n",
        "            pooled, halt_info, adaptive_states = self._forward_adaptive(\n",
        "                slots,\n",
        "                cond_tokens,\n",
        "                B,\n",
        "                track_states=return_all_states\n",
        "            )\n",
        "            if return_all_states and adaptive_states and state_trace is not None:\n",
        "                state_trace.extend(self.slot_readout(state) for state in adaptive_states)\n",
        "        else:\n",
        "            for _ in range(self.num_recursions):\n",
        "                slots = self.recursive_block(slots, cond_tokens)\n",
        "                if return_all_states and state_trace is not None:\n",
        "                    state_trace.append(self.slot_readout(slots.mean(dim=1)))\n",
        "            pooled = slots.mean(dim=1)\n",
        "\n",
        "        pooled = self.slot_readout(pooled)\n",
        "        actions = self.action_head(pooled)\n",
        "\n",
        "        if return_all_states and state_trace is not None:\n",
        "            return actions, state_trace\n",
        "        return actions\n",
        "\n",
        "    def _forward_adaptive(self, slots, cond_tokens, batch_size, track_states: bool = False):\n",
        "        \"\"\"Adaptive Computation Time applicato agli slot.\"\"\"\n",
        "        halt_probs = []\n",
        "        remainders = torch.ones(batch_size, device=slots.device)\n",
        "        n_updates = torch.zeros(batch_size, device=slots.device)\n",
        "        accumulated = torch.zeros(batch_size, slots.size(-1), device=slots.device)\n",
        "        state_trace: Optional[List[torch.Tensor]] = [] if track_states else None\n",
        "\n",
        "        for _ in range(self.num_recursions):\n",
        "            slots = self.recursive_block(slots, cond_tokens)\n",
        "            pooled = slots.mean(dim=1)\n",
        "\n",
        "            if track_states and state_trace is not None:\n",
        "                state_trace.append(pooled.detach().clone())\n",
        "\n",
        "            halt_p = self.halt_predictor(pooled).squeeze(-1)\n",
        "            halt_probs.append(halt_p)\n",
        "\n",
        "            still_running = (remainders > 0.01).float()\n",
        "            accumulated += remainders.unsqueeze(-1) * pooled * still_running.unsqueeze(-1)\n",
        "            remainders = remainders * (1 - halt_p) * still_running\n",
        "            n_updates += still_running\n",
        "\n",
        "            if remainders.max() < 0.01:\n",
        "                break\n",
        "\n",
        "        halt_info = {\n",
        "            'halt_probs': halt_probs,\n",
        "            'n_updates': n_updates\n",
        "        }\n",
        "\n",
        "        if track_states:\n",
        "            return accumulated, halt_info, state_trace or []\n",
        "        return accumulated, halt_info, None\n",
        "\n",
        "    def _init_slots(self, batch_size: int, device: torch.device, cond_summary: torch.Tensor) -> torch.Tensor:\n",
        "        base = self.slot_init.unsqueeze(0).expand(batch_size, -1, -1).to(device)\n",
        "        cond_bias = self.slot_conditioning(cond_summary).unsqueeze(1)\n",
        "        return base + cond_bias\n",
        "\n",
        "\n",
        "def build_policy_from_config(config: TrainingConfig, obs_shape: Tuple[int, int, int] = (3, 128, 128)) -> TRMPolicy:\n",
        "    \"\"\"Costruisce una TRMPolicy coerente con il TrainingConfig.\"\"\"\n",
        "\n",
        "    return TRMPolicy(\n",
        "        obs_shape=obs_shape,\n",
        "        action_dim=7,\n",
        "        hidden_dim=config.hidden_dim,\n",
        "        num_recursions=config.num_recursions,\n",
        "        num_slots=config.num_slots,\n",
        "        dropout=config.dropout,\n",
        "        use_pretrained_encoder=config.use_pretrained_encoder,\n",
        "        freeze_backbone=config.freeze_backbone,\n",
        "        encoder_dropout=config.encoder_dropout,\n",
        "        use_text_prompts=config.use_text_prompts,\n",
        "        text_encoder_name=config.text_encoder_name,\n",
        "        train_text_encoder=config.train_text_encoder,\n",
        "        text_dropout=config.text_dropout\n",
        "    )\n",
        "\n",
        "print(\"ok\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BehaviorCloningTrainer:\n",
        "    \"\"\"Trainer per Behavior Cloning con configurazione strutturata.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        train_loader: DataLoader,\n",
        "        val_loader: DataLoader,\n",
        "        config: TrainingConfig,\n",
        "        device: torch.device,\n",
        "        use_wandb: bool = False\n",
        "    ):\n",
        "        self.model = model.to(device)\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "        self.use_wandb = use_wandb\n",
        "        self.steps_per_epoch = max(len(train_loader), 1)\n",
        "\n",
        "        # Optimizer e scheduler\n",
        "        self.optimizer = torch.optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=config.lr,\n",
        "            weight_decay=config.weight_decay\n",
        "        )\n",
        "\n",
        "        self.scheduler = None\n",
        "        if config.sched_T0:\n",
        "            # Cosine warm restarts operanti a livello di iterazioni\n",
        "            period_iters = max(1, config.sched_T0 * self.steps_per_epoch)\n",
        "            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "                self.optimizer,\n",
        "                T_0=period_iters,\n",
        "                T_mult=config.sched_T_mult,\n",
        "                eta_min=config.lr_min\n",
        "            )\n",
        "        else:\n",
        "            warmup_iters = max(1, config.warmup_epochs * self.steps_per_epoch)\n",
        "            total_iters = max(1, config.epochs * self.steps_per_epoch - warmup_iters)\n",
        "\n",
        "            for pg in self.optimizer.param_groups:\n",
        "                pg['lr'] = config.lr\n",
        "\n",
        "            warmup_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
        "                self.optimizer,\n",
        "                start_factor=1e-8,\n",
        "                end_factor=1.0,\n",
        "                total_iters=warmup_iters\n",
        "            )\n",
        "\n",
        "            constant_scheduler = torch.optim.lr_scheduler.ConstantLR(\n",
        "                self.optimizer,\n",
        "                factor=1.0,\n",
        "                total_iters=total_iters\n",
        "            )\n",
        "\n",
        "            self.scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
        "                self.optimizer,\n",
        "                schedulers=[warmup_scheduler, constant_scheduler],\n",
        "                milestones=[warmup_iters]\n",
        "            )\n",
        "\n",
        "        self.use_amp = (self.device.type == 'cuda')\n",
        "        self.scaler = torch.cuda.amp.GradScaler(enabled=self.use_amp)\n",
        "        self.grad_clip = config.grad_clip\n",
        "        self.early_stop_patience = config.early_stop_patience\n",
        "        self._epochs_no_improve = 0\n",
        "\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.best_model_path = config.save_path\n",
        "    \n",
        "    def train(self):\n",
        "        \"\"\"Training loop completo\"\"\"\n",
        "        \n",
        "        for epoch in range(self.config.epochs):\n",
        "            # Training\n",
        "            train_metrics = self._train_epoch(epoch)\n",
        "            \n",
        "            # Validation\n",
        "            val_metrics = self._validate_epoch(epoch)\n",
        "            print(f\"Epoch {epoch}, training loss: {train_metrics['loss']}, validation loss: {val_metrics['loss']}\")\n",
        "            # Logging exaustive\n",
        "            self._log_metrics(epoch, train_metrics, val_metrics)\n",
        "\n",
        "            # print(f\"Epoch: {epoch}\\n\\t Training: {train_metrics}\\n\\t Validation: {val_metrics}\")\n",
        "            \n",
        "            # Save best model\n",
        "            if val_metrics['loss'] < self.best_val_loss:\n",
        "                self.best_val_loss = val_metrics['loss']\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': self.model.state_dict(),\n",
        "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                    'val_loss': val_metrics['loss'],\n",
        "                    'config': self.config.to_dict()\n",
        "                }, self.best_model_path)\n",
        "                print(f\"  ‚úì Saved best model (val_loss: {val_metrics['loss']:.4f})\")\n",
        "                self._epochs_no_improve = 0\n",
        "            else:\n",
        "                self._epochs_no_improve += 1\n",
        "\n",
        "            if self.early_stop_patience and self._epochs_no_improve >= self.early_stop_patience:\n",
        "                print(\"‚èπÔ∏è  Early stopping triggered\")\n",
        "                break\n",
        "        \n",
        "        print(f\"\\n‚úÖ Training completed! Best val loss: {self.best_val_loss:.4f}\")\n",
        "        return self.best_val_loss\n",
        "    \n",
        "    def _train_epoch(self, epoch):\n",
        "        \"\"\"Training per una epoch\"\"\"\n",
        "        self.model.train()\n",
        "        \n",
        "        total_loss = 0\n",
        "        action_mse = 0\n",
        "        action_l1 = 0\n",
        "        \n",
        "        for step, batch in enumerate(self.train_loader):\n",
        "            obs = batch['observations'].to(self.device, non_blocking=True)\n",
        "            target_actions = batch['actions'].to(self.device, non_blocking=True)\n",
        "            prompts = batch.get('prompt')\n",
        "\n",
        "            self.optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
        "                pred_actions = self.model(obs, prompts=prompts)\n",
        "                mse = F.mse_loss(pred_actions, target_actions)\n",
        "                l1 = F.l1_loss(pred_actions, target_actions)\n",
        "                loss = 0.7 * mse + 0.3 * l1\n",
        "\n",
        "            if self.use_amp:\n",
        "                self.scaler.scale(loss).backward()\n",
        "                if self.grad_clip:\n",
        "                    self.scaler.unscale_(self.optimizer)\n",
        "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "            else:\n",
        "                loss.backward()\n",
        "                if self.grad_clip:\n",
        "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)\n",
        "                self.optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            action_mse += mse.item()\n",
        "            action_l1 += l1.item()\n",
        "\n",
        "            if self.scheduler is not None:\n",
        "                self.scheduler.step()\n",
        "\n",
        "        n_batches = len(self.train_loader)\n",
        "        return {\n",
        "            'loss': total_loss / n_batches,\n",
        "            'action_mse': action_mse / n_batches,\n",
        "            'action_l1': action_l1 / n_batches\n",
        "        }\n",
        "    \n",
        "    def _validate_epoch(self, epoch):\n",
        "        \"\"\"Validation per una epoch\"\"\"\n",
        "        self.model.eval()\n",
        "        \n",
        "        total_loss = 0\n",
        "        action_mse = 0\n",
        "        action_l1 = 0\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            \n",
        "            for batch in self.val_loader:\n",
        "                obs = batch['observations'].to(self.device)\n",
        "                target_actions = batch['actions'].to(self.device)\n",
        "                prompts = batch.get('prompt')\n",
        "                \n",
        "                # Forward pass\n",
        "                pred_actions = self.model(obs, prompts=prompts)\n",
        "                \n",
        "                # Loss\n",
        "                mse = F.mse_loss(pred_actions, target_actions)\n",
        "                l1 = F.l1_loss(pred_actions, target_actions)\n",
        "                loss = 0.7 * mse + 0.3 * l1\n",
        "                \n",
        "                total_loss += loss.item()\n",
        "                action_mse += mse.item()\n",
        "                action_l1 += l1.item()\n",
        "                \n",
        "        \n",
        "        n_batches = len(self.val_loader)\n",
        "        return {\n",
        "            'loss': total_loss / n_batches,\n",
        "            'action_mse': action_mse / n_batches,\n",
        "            'action_l1': action_l1 / n_batches\n",
        "        }\n",
        "    \n",
        "    def _log_metrics(self, epoch, train_metrics, val_metrics):\n",
        "        \"\"\"Log delle metriche\"\"\"\n",
        "        lr = self.optimizer.param_groups[0]['lr']\n",
        "        \n",
        "        # WandB logging\n",
        "        if self.use_wandb:\n",
        "            wandb.log({\n",
        "                'epoch': epoch,\n",
        "                'train/loss': train_metrics['loss'],\n",
        "                'train/action_mse': train_metrics['action_mse'],\n",
        "                'train/action_l1': train_metrics.get('action_l1'),\n",
        "                'val/loss': val_metrics['loss'],\n",
        "                'val/action_mse': val_metrics['action_mse'],\n",
        "                'val/action_l1': val_metrics.get('action_l1'),\n",
        "                'lr': lr\n",
        "            })\n",
        "\n",
        "def build_dataloaders(\n",
        "    train_dataset: Dataset,\n",
        "    val_dataset: Dataset,\n",
        "    batch_size: int,\n",
        "    loader_kwargs: Dict[str, Any]\n",
        ") -> Tuple[DataLoader, DataLoader]:\n",
        "    \"\"\"Crea data loader consistenti a partire da un template di kwargs.\"\"\"\n",
        "\n",
        "    kwargs = loader_kwargs.copy()\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "        **kwargs\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        drop_last=False,\n",
        "        **kwargs\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "def _set_dataset_augmentation(dataset, flag: bool):\n",
        "    \"\"\"Imposta augmentation temporaneamente e restituisce funzione di restore.\"\"\"\n",
        "\n",
        "    if not hasattr(dataset, 'augmentation'):\n",
        "        return lambda: None\n",
        "\n",
        "    original = dataset.augmentation\n",
        "    dataset.augmentation = flag\n",
        "\n",
        "    def restore():\n",
        "        dataset.augmentation = original\n",
        "\n",
        "    return restore\n",
        "\n",
        "\n",
        "def optuna_random_search(\n",
        "    train_dataset: Dataset,\n",
        "    val_dataset: Dataset,\n",
        "    loader_kwargs: Dict[str, Any],\n",
        "    device: torch.device,\n",
        "    quick_epochs: int = 10,\n",
        "    search_space: Optional[HyperparameterSearchSpace] = None,\n",
        "    n_trials: int = 50\n",
        ") -> Tuple[TrainingConfig, List[Dict[str, Any]]]:\n",
        "    \"\"\"\n",
        "    Randomized Optuna search (TPE-based) instead of grid-search.\n",
        "    Runs a fixed number of trials (n_trials).\n",
        "    \"\"\"\n",
        "\n",
        "    search_space = search_space or default_search_space()\n",
        "\n",
        "    # --- NEW: TPE Sampler (recommended over random sampling) ---\n",
        "    sampler = optuna.samplers.TPESampler(seed=42)\n",
        "\n",
        "    study = optuna.create_study(\n",
        "        direction='minimize',\n",
        "        sampler=sampler\n",
        "    )\n",
        "\n",
        "    print(f\"\\nüîç Starting Optuna random search\")\n",
        "    print(f\"  Number of trials: {n_trials}\")\n",
        "    print(f\"  Quick epochs: {quick_epochs}\")\n",
        "\n",
        "    def objective(trial: optuna.Trial) -> float:\n",
        "\n",
        "        # --- Suggest values from the search space ---\n",
        "        trial_config = TrainingConfig(\n",
        "            lr=trial.suggest_categorical('lr', search_space.lr),\n",
        "            hidden_dim=trial.suggest_categorical('hidden_dim', search_space.hidden_dim),\n",
        "            num_recursions=trial.suggest_categorical('num_recursions', search_space.num_recursions),\n",
        "            epochs=quick_epochs,\n",
        "            batch_size=trial.suggest_categorical('batch_size', search_space.batch_size),\n",
        "            weight_decay=trial.suggest_categorical('weight_decay', search_space.weight_decay),\n",
        "            use_pretrained_encoder=trial.suggest_categorical('pretrained_encoder', search_space.pretrained_encoder),\n",
        "            freeze_backbone=trial.suggest_categorical('freeze_backbone', search_space.freeze_backbone),\n",
        "            augmentation=trial.suggest_categorical('augmentation', search_space.augmentation),\n",
        "            dropout=trial.suggest_categorical('dropout', search_space.dropout),\n",
        "            encoder_dropout=trial.suggest_categorical('dropout', search_space.dropout),\n",
        "            text_encoder_name=trial.suggest_categorical('text_encoder_name', search_space.text_encoder_name),\n",
        "            train_text_encoder=trial.suggest_categorical('train_text_encoder', search_space.train_text_encoder),\n",
        "            text_dropout=trial.suggest_categorical('text_dropout', search_space.text_dropout),\n",
        "            num_slots=trial.suggest_categorical('num_slots', search_space.num_slots),\n",
        "            grad_clip=1.0,\n",
        "            save_path=f\"optuna_trial_{trial.number}.pt\"\n",
        "        )\n",
        "\n",
        "        print(\"-\" * 80)\n",
        "        print(f\"[Optuna] Trial {trial.number + 1}/{n_trials}\")\n",
        "        print(json.dumps(trial_config.to_dict(), indent=2))\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # Build dataloaders based on this trial's batch size\n",
        "        train_loader, val_loader = build_dataloaders(\n",
        "            train_dataset,\n",
        "            val_dataset,\n",
        "            trial_config.batch_size,\n",
        "            loader_kwargs\n",
        "        )\n",
        "\n",
        "        # Build model\n",
        "        model = build_policy_from_config(trial_config)\n",
        "        trainer = BehaviorCloningTrainer(\n",
        "            model,\n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            trial_config,\n",
        "            device,\n",
        "            use_wandb=False\n",
        "        )\n",
        "\n",
        "        # Turn augmentation on/off safely\n",
        "        restore_aug = _set_dataset_augmentation(train_dataset, trial_config.augmentation)\n",
        "\n",
        "        try:\n",
        "            val_loss = trainer.train()\n",
        "        finally:\n",
        "            restore_aug()\n",
        "\n",
        "        # Store config inside trial for later retrieval\n",
        "        trial.set_user_attr('config', trial_config.to_dict())\n",
        "\n",
        "        return val_loss\n",
        "\n",
        "    # --- NEW: Run fixed-size random search ---\n",
        "    study.optimize(objective, n_trials=n_trials)\n",
        "\n",
        "    # Save trial history (full information)\n",
        "    history = []\n",
        "    for trial in study.trials:\n",
        "        history.append({\n",
        "            'trial_number': trial.number,\n",
        "            'state': str(trial.state),\n",
        "            'value': trial.value,\n",
        "            'params': trial.params,                # all parameters sampled\n",
        "            'user_attrs': trial.user_attrs,        # user metadata (our config)\n",
        "            'distributions': {\n",
        "                k: str(v) for k, v in trial.distributions.items()\n",
        "            },                                      # searchable distributions\n",
        "            'system_attrs': trial.system_attrs,    # optuna internal info\n",
        "            'datetime_start': str(trial.datetime_start),\n",
        "            'datetime_complete': str(trial.datetime_complete),\n",
        "        })\n",
        "\n",
        "\n",
        "    # Best configuration\n",
        "    best_config = TrainingConfig(**study.best_trial.user_attrs['config'])\n",
        "\n",
        "    return best_config, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_final_model(\n",
        "    base_config: TrainingConfig,\n",
        "    train_dataset: Dataset,\n",
        "    val_dataset: Dataset,\n",
        "    loader_kwargs: Dict[str, Any],\n",
        "    device,\n",
        "    final_epochs: Optional[int] = None,\n",
        "    use_wandb: bool = False\n",
        ") -> Tuple[nn.Module, float]:\n",
        "    \"\"\"Esegue il training finale a partire dalla configurazione scelta.\"\"\"\n",
        "\n",
        "    final_epochs = final_epochs or base_config.epochs\n",
        "    final_config = replace(\n",
        "        base_config,\n",
        "        epochs=final_epochs,\n",
        "        save_path='final_model.pt',\n",
        "        early_stop_patience=base_config.early_stop_patience or 10,\n",
        "        sched_T0=base_config.sched_T0 or max(1, final_epochs // 5)\n",
        "    )\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"üéØ FINAL TRAINING\")\n",
        "    print(f\"Config: {final_config.label()}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    if use_wandb:\n",
        "        wandb.init(\n",
        "            project=\"trm-robotics\",\n",
        "            name=f\"final_{final_config.label()}\",\n",
        "            config=final_config.to_dict()\n",
        "        )\n",
        "\n",
        "    train_loader, val_loader = build_dataloaders(\n",
        "        train_dataset,\n",
        "        val_dataset,\n",
        "        final_config.batch_size,\n",
        "        loader_kwargs\n",
        "    )\n",
        "\n",
        "    model = build_policy_from_config(final_config)\n",
        "    trainer = BehaviorCloningTrainer(\n",
        "        model,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        final_config,\n",
        "        device,\n",
        "        use_wandb=use_wandb\n",
        "    )\n",
        "\n",
        "    restore_aug = _set_dataset_augmentation(train_dataset, final_config.augmentation)\n",
        "    try:\n",
        "        final_val_loss = trainer.train()\n",
        "    finally:\n",
        "        restore_aug()\n",
        "\n",
        "    if os.path.exists(final_config.save_path):\n",
        "        checkpoint = torch.load(final_config.save_path, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    if use_wandb:\n",
        "        wandb.finish()\n",
        "\n",
        "    print(f\"\\n‚úÖ Final model trained! Val loss: {final_val_loss:.4f}\")\n",
        "\n",
        "    return model, final_val_loss\n",
        "\n",
        "class PolicyEvaluator:\n",
        "    \"\"\"\n",
        "    Valutatore per policy robotiche in simulazione LIBERO\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        policy: nn.Module,\n",
        "        device: torch.device,\n",
        "        action_stats: Dict = None\n",
        "    ):\n",
        "        self.policy = policy.to(device)\n",
        "        self.policy.eval()\n",
        "        self.device = device\n",
        "        self.action_stats = action_stats\n",
        "    \n",
        "    def evaluate_on_task(\n",
        "        self,\n",
        "        env,\n",
        "        init_states,\n",
        "        num_episodes=50,\n",
        "        max_steps=500,\n",
        "        record_video=False,\n",
        "        video_path=None,\n",
        "        task_prompt: Optional[str] = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Valuta policy su un singolo task\n",
        "        \n",
        "        Args:\n",
        "            env: environment LIBERO\n",
        "            init_states: stati iniziali per il task\n",
        "            num_episodes: numero di episodi da valutare\n",
        "            max_steps: massimo numero di step per episodio\n",
        "            record_video: se True, registra video\n",
        "            video_path: path per salvare video\n",
        "        \n",
        "        Returns:\n",
        "            results: dizionario con metriche\n",
        "        \"\"\"\n",
        "        expect_prompt = getattr(self.policy, 'use_text_prompts', False)\n",
        "        if expect_prompt and not task_prompt:\n",
        "            raise ValueError(\"PolicyEvaluator richiede un task_prompt per policy testo-condizionate\")\n",
        "\n",
        "        successes = []\n",
        "        episode_lengths = []\n",
        "        frames_buffer = [] if record_video else None\n",
        "        \n",
        "        for ep in range(num_episodes):\n",
        "            # Reset environment\n",
        "            env.reset()\n",
        "            env.set_init_state(init_states[ep % len(init_states)])\n",
        "            \n",
        "            obs = env.get_observation()\n",
        "            done = False\n",
        "            step = 0\n",
        "            \n",
        "            episode_frames = [] if record_video and ep < 5 else None\n",
        "            \n",
        "            while not done and step < max_steps:\n",
        "                # Cattura frame\n",
        "                if episode_frames is not None:\n",
        "                    frame = env.render(mode='rgb_array')\n",
        "                    episode_frames.append(frame)\n",
        "                \n",
        "                # Preprocessing osservazione\n",
        "                obs_tensor = self._preprocess_obs(obs)\n",
        "                \n",
        "                # Predici azione\n",
        "                with torch.no_grad():\n",
        "                    prompt_batch = [task_prompt] if task_prompt else None\n",
        "                    action = self.policy(obs_tensor, prompts=prompt_batch)\n",
        "                    action = action.cpu().numpy().squeeze()\n",
        "                    \n",
        "                    # Denormalizza azione se necessario\n",
        "                    if self.action_stats is not None:\n",
        "                        action = action * self.action_stats['std'] + self.action_stats['mean']\n",
        "                \n",
        "                # Step environment\n",
        "                obs, reward, done, info = env.step(action)\n",
        "                step += 1\n",
        "            \n",
        "            # Registra risultati\n",
        "            success = info.get('success', False)\n",
        "            successes.append(success)\n",
        "            episode_lengths.append(step)\n",
        "            \n",
        "            if episode_frames is not None:\n",
        "                frames_buffer.append(episode_frames)\n",
        "        \n",
        "        # Salva video se richiesto\n",
        "        if record_video and frames_buffer and video_path:\n",
        "            self._save_videos(frames_buffer, video_path)\n",
        "        \n",
        "        # Calcola metriche\n",
        "        results = {\n",
        "            'success_rate': np.mean(successes),\n",
        "            'avg_episode_length': np.mean(episode_lengths),\n",
        "            'std_episode_length': np.std(episode_lengths),\n",
        "            'num_episodes': num_episodes\n",
        "        }\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def _preprocess_obs(self, obs):\n",
        "        \"\"\"Preprocessing osservazione per policy\"\"\"\n",
        "        # Estrai immagine RGB\n",
        "        img = obs['agentview_rgb']\n",
        "        \n",
        "        # Normalizza\n",
        "        img = img.astype(np.float32) / 255.0\n",
        "        \n",
        "        # Converti a tensor e aggiungi batch dimension\n",
        "        img = np.transpose(img, (2, 0, 1))  # HWC -> CHW\n",
        "        img_tensor = torch.from_numpy(img).unsqueeze(0).to(self.device)\n",
        "        \n",
        "        return img_tensor\n",
        "    \n",
        "    def _save_videos(self, frames_buffer, video_path):\n",
        "        \"\"\"Salva video degli episodi\"\"\"\n",
        "        os.makedirs(os.path.dirname(video_path), exist_ok=True)\n",
        "        \n",
        "        for i, frames in enumerate(frames_buffer):\n",
        "            path = video_path.replace('.mp4', f'_ep{i}.mp4')\n",
        "            \n",
        "            # Usa OpenCV per salvare video\n",
        "            height, width, _ = frames[0].shape\n",
        "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "            out = cv2.VideoWriter(path, fourcc, 30.0, (width, height))\n",
        "            \n",
        "            for frame in frames:\n",
        "                frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "                out.write(frame_bgr)\n",
        "            \n",
        "            out.release()\n",
        "            print(f\"  Video salvato: {path}\")\n",
        "\n",
        "\n",
        "def generate_evaluation_report(results_dict, output_path):\n",
        "    \"\"\"\n",
        "    Genera report HTML con risultati di valutazione\n",
        "    \n",
        "    Args:\n",
        "        results_dict: dizionario {task_name: results}\n",
        "        output_path: path per salvare report\n",
        "    \"\"\"\n",
        "    \n",
        "    html_content = f\"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html>\n",
        "    <head>\n",
        "        <title>TRM Robotics - Evaluation Report</title>\n",
        "        <style>\n",
        "            body {{ font-family: Arial, sans-serif; margin: 40px; }}\n",
        "            h1 {{ color: #333; }}\n",
        "            table {{ border-collapse: collapse; width: 100%; margin-top: 20px; }}\n",
        "            th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}\n",
        "            th {{ background-color: #4CAF50; color: white; }}\n",
        "            tr:nth-child(even) {{ background-color: #f2f2f2; }}\n",
        "            .success {{ color: green; font-weight: bold; }}\n",
        "            .fail {{ color: red; }}\n",
        "        </style>\n",
        "    </head>\n",
        "    <body>\n",
        "        <h1>ü§ñ TRM Robotics Evaluation Report</h1>\n",
        "        <p><strong>Generated:</strong> {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}</p>\n",
        "        \n",
        "        <h2>Results Summary</h2>\n",
        "        <table>\n",
        "            <tr>\n",
        "                <th>Task Name</th>\n",
        "                <th>Success Rate</th>\n",
        "                <th>Avg Episode Length</th>\n",
        "                <th>Std Episode Length</th>\n",
        "                <th>Num Episodes</th>\n",
        "            </tr>\n",
        "    \"\"\"\n",
        "    \n",
        "    # Aggiungi righe per ogni task\n",
        "    overall_success = []\n",
        "    \n",
        "    for task_name, results in results_dict.items():\n",
        "        success_rate = results['success_rate']\n",
        "        overall_success.append(success_rate)\n",
        "        \n",
        "        success_class = 'success' if success_rate >= 0.5 else 'fail'\n",
        "        \n",
        "        html_content += f\"\"\"\n",
        "            <tr>\n",
        "                <td>{task_name}</td>\n",
        "                <td class=\"{success_class}\">{success_rate:.2%}</td>\n",
        "                <td>{results['avg_episode_length']:.1f}</td>\n",
        "                <td>{results['std_episode_length']:.1f}</td>\n",
        "                <td>{results['num_episodes']}</td>\n",
        "            </tr>\n",
        "        \"\"\"\n",
        "    \n",
        "    # Overall statistics\n",
        "    mean_success = np.mean(overall_success)\n",
        "    success_class = 'success' if mean_success >= 0.5 else 'fail'\n",
        "    \n",
        "    html_content += f\"\"\"\n",
        "            <tr style=\"font-weight: bold; background-color: #e0e0e0;\">\n",
        "                <td>OVERALL</td>\n",
        "                <td class=\"{success_class}\">{mean_success:.2%}</td>\n",
        "                <td colspan=\"3\">-</td>\n",
        "            </tr>\n",
        "        </table>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "    \n",
        "    # Salva report\n",
        "    with open(output_path, 'w') as f:\n",
        "        f.write(html_content)\n",
        "    \n",
        "    print(f\"\\n‚úì Report salvato: {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main_pipeline(\n",
        "    data_path: str = 'dataset/libero_spatial',\n",
        "    work_dir: str = 'trm_robotics',\n",
        "    quick_search: bool = True,\n",
        "    train_final: bool = True,\n",
        "    use_custom_final_config: bool = False,\n",
        "    evaluate: bool = True,\n",
        "    use_wandb: bool = False\n",
        "):\n",
        "    \"\"\"\n",
        "    Pipeline completa per il progetto TRM Robotics\n",
        "    \n",
        "    Args:\n",
        "        data_path: path ai dati LIBERO\n",
        "        work_dir: directory di lavoro\n",
        "        quick_search: se True, esegue hyperparameter search\n",
        "        train_final: se True, esegue training finale\n",
        "        evaluate: se True, esegue valutazione in simulazione\n",
        "        use_wandb: se True, usa WandB per logging\n",
        "    \"\"\"\n",
        "    \n",
        "    print(f\"\"\"\n",
        "    {'='*80}\n",
        "    ü§ñ TinyRecursiveModels per Controllo Robotico\n",
        "    {'='*80}\n",
        "    \n",
        "    Obiettivi:\n",
        "    1. Adattare architettura TRM per robotica\n",
        "    2. Training con Behavior Cloning su LIBERO\n",
        "    3. Valutazione in simulazione con metriche quantitative e qualitative\n",
        "    \n",
        "    \"\"\")\n",
        "    \n",
        "    # Setup directories\n",
        "    os.makedirs(work_dir, exist_ok=True)\n",
        "    os.chdir(work_dir)\n",
        "    \n",
        "    # Device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"‚úì Using device: {device}\\n\")\n",
        "    \n",
        "    # ========== STEP 1: Caricamento Dataset ==========\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"STEP 1: Caricamento Dataset\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Trova file HDF5\n",
        "    data_path = Path(data_path)\n",
        "    hdf5_files = list(data_path.glob('**/*.hdf5'))\n",
        "    \n",
        "    if not hdf5_files:\n",
        "        print(f\"‚ùå Nessun file HDF5 trovato in {data_path}\")\n",
        "        print(\"Assicurati di aver scaricato il dataset LIBERO!\")\n",
        "        return\n",
        "    \n",
        "    print(f\"‚úì Trovati {len(hdf5_files)} file HDF5 (task)\")\n",
        "    \n",
        "    # Demo-level split: usa TUTTI i file per entrambi i dataset\n",
        "    # ma dividi le demo all'interno di ciascun file (80% train, 20% val)\n",
        "    demo_split_ratio = 0.8\n",
        "    print(f\"\\nüìä Demo-level split: {demo_split_ratio:.0%} train / {1-demo_split_ratio:.0%} val per ogni task\")\n",
        "    print(f\"   Tutti i {len(hdf5_files)} task presenti in entrambi train e val\")\n",
        "    \n",
        "    # Crea datasets con demo-level split\n",
        "    print(\"\\nCreating TRAIN dataset...\")\n",
        "    train_dataset = LIBERODataset(\n",
        "        hdf5_files,  # Usa TUTTI i file\n",
        "        sequence_length=1,\n",
        "        image_size=(128, 128),\n",
        "        augmentation=False,\n",
        "        max_demos_per_task=50,  # Limita per velocit√†\n",
        "        demo_split_ratio=demo_split_ratio,\n",
        "        is_train=True  # Prime 80% demo per task\n",
        "    )\n",
        "    \n",
        "    # Usa le stesse statistiche del training set per la validation\n",
        "    train_action_stats = train_dataset.action_stats\n",
        "    \n",
        "    print(\"\\nCreating VAL dataset...\")\n",
        "    val_dataset = LIBERODataset(\n",
        "        hdf5_files,  # Usa TUTTI i file\n",
        "        sequence_length=1,\n",
        "        image_size=(128, 128),\n",
        "        augmentation=False,\n",
        "        max_demos_per_task=50,  # Stesso limite per coerenza\n",
        "        demo_split_ratio=demo_split_ratio,\n",
        "        is_train=False,  # Ultime 20% demo per task\n",
        "        action_stats=train_action_stats  # Usa statistiche del training\n",
        "    )\n",
        "    \n",
        "    # Data loader template (verranno istanziati on-demand)\n",
        "    num_workers = min(4, os.cpu_count() or 1)\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "\n",
        "    loader_common = {\n",
        "        'num_workers': num_workers,\n",
        "        'pin_memory': use_cuda,\n",
        "        'persistent_workers': num_workers > 0\n",
        "    }\n",
        "    if num_workers > 0:\n",
        "        loader_common['prefetch_factor'] = 2\n",
        "\n",
        "    print(f\"\\n‚úì Dataset creati con demo-level split\")\n",
        "    print(f\"  Train samples: {len(train_dataset)}\")\n",
        "    print(f\"  Val samples: {len(val_dataset)}\")\n",
        "    \n",
        "    # Salva action stats per denormalizzazione\n",
        "    action_stats = train_dataset.action_stats\n",
        "    with open('action_stats.json', 'w') as f:\n",
        "        json.dump({\n",
        "            'mean': action_stats['mean'].tolist(),\n",
        "            'std': action_stats['std'].tolist()\n",
        "        }, f)\n",
        "    \n",
        "    # ========== STEP 2: Hyperparameter Search ==========\n",
        "    best_config = None\n",
        "    \n",
        "    quick_search = False\n",
        "    if quick_search:\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(\"STEP 2: Hyperparameter Search\")\n",
        "        print(f\"{'='*80}\")\n",
        "        \n",
        "        best_config, all_results = optuna_random_search(\n",
        "            train_dataset,\n",
        "            val_dataset,\n",
        "            loader_common,\n",
        "            device,\n",
        "            quick_epochs=10\n",
        "        )\n",
        "        \n",
        "        # Salva risultati\n",
        "        with open('hyperparam_results.json', 'w') as f:\n",
        "            json.dump(all_results, f, indent=2)\n",
        "        with open('best_hyperparams.json', 'w') as f:\n",
        "            json.dump(best_config.to_dict(), f, indent=2)\n",
        "    \n",
        "    # ========== STEP 3: Training Finale ==========\n",
        "    trained_model = None\n",
        "    \n",
        "    if train_final:\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(\"STEP 3: Training Finale\")\n",
        "        print(f\"{'='*80}\")\n",
        "        \n",
        "        if best_config is None or use_custom_final_config:\n",
        "            best_config = TrainingConfig(\n",
        "                lr=1e-4,\n",
        "                hidden_dim=128,\n",
        "                num_recursions=12,\n",
        "                batch_size=256,\n",
        "                epochs=20,\n",
        "                weight_decay=1.0,\n",
        "                dropout=0.3,\n",
        "                encoder_dropout=0.1,\n",
        "                use_pretrained_encoder=True,\n",
        "                freeze_backbone=False,\n",
        "                augmentation=True,\n",
        "                text_encoder_name='openai/clip-vit-large-patch14',\n",
        "                train_text_encoder = False,\n",
        "                text_dropout = 0.1,\n",
        "                use_text_prompts = True,\n",
        "                num_slots = 4\n",
        "            )\n",
        "            print(\"‚ö†Ô∏è  Usando configurazione custom di default\")\n",
        "\n",
        "        trained_model, final_val_loss = train_final_model(\n",
        "            best_config,\n",
        "            train_dataset,\n",
        "            val_dataset,\n",
        "            loader_common,\n",
        "            device,\n",
        "            final_epochs=100,\n",
        "            use_wandb=use_wandb\n",
        "        )\n",
        "    \n",
        "    # ========== STEP 4: Valutazione ==========\n",
        "    if evaluate:\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(\"STEP 4: Valutazione in Simulazione\")\n",
        "        print(f\"{'='*80}\")\n",
        "        \n",
        "        # Carica modello se non gi√† trainato\n",
        "        if trained_model is None:\n",
        "            print(\"Caricando modello salvato...\")\n",
        "            checkpoint = torch.load('final_model.pt')\n",
        "            \n",
        "            if 'config' in checkpoint:\n",
        "                config_obj = TrainingConfig(**checkpoint['config'])\n",
        "                trained_model = build_policy_from_config(config_obj)\n",
        "            else:\n",
        "                trained_model = TRMPolicy(\n",
        "                    obs_shape=(3, 128, 128),\n",
        "                    action_dim=7,\n",
        "                    hidden_dim=256,\n",
        "                    num_recursions=8,\n",
        "                    num_slots=4\n",
        "                )\n",
        "\n",
        "            trained_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            trained_model = trained_model.to(device)\n",
        "        \n",
        "        # Crea evaluator\n",
        "        evaluator = PolicyEvaluator(trained_model, device, action_stats)\n",
        "        \n",
        "        # Nota: La valutazione in simulazione richiede setup di LIBERO environment\n",
        "        # Questo √® un placeholder per la struttura\n",
        "        print(\"\\n‚ö†Ô∏è  La valutazione in simulazione richiede setup completo di LIBERO\")\n",
        "        print(\"Vedi sezione 'Valutazione in Simulazione' per implementazione completa\")\n",
        "        \n",
        "        # Pseudocodice per valutazione:\n",
        "        \"\"\"\n",
        "        from libero.libero import benchmark\n",
        "        \n",
        "        results_dict = {}\n",
        "        \n",
        "        for task_id in range(num_tasks):\n",
        "            env, init_states = create_eval_env(task_suite, task_id)\n",
        "            \n",
        "            results = evaluator.evaluate_on_task(\n",
        "                env,\n",
        "                init_states,\n",
        "                num_episodes=50,\n",
        "                record_video=True,\n",
        "                video_path=f'videos/task_{task_id}.mp4'\n",
        "            )\n",
        "            \n",
        "            results_dict[f'task_{task_id}'] = results\n",
        "            env.close()\n",
        "        \n",
        "        # Genera report\n",
        "        generate_evaluation_report(results_dict, 'evaluation_report.html')\n",
        "        \"\"\"\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"‚úÖ Pipeline completata!\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"\\nFile generati in: {work_dir}\")\n",
        "    print(\"  - final_model.pt: modello allenato\")\n",
        "    print(\"  - action_stats.json: statistiche azioni\")\n",
        "    print(\"  - hyperparam_results.json: risultati hyperparameter search\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Esegue Optuna HPO (incluso il text-encoder) e avvia il training finale.\"\"\"\n",
        "import torch\n",
        "\n",
        "data_path = 'dataset/libero_spatial'\n",
        "work_dir = 'trm_robotics'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"‚úì Using device: {device}\\n\")\n",
        "\n",
        "# ========== STEP 1: Caricamento Dataset ==========\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"STEP 1: Caricamento Dataset\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "data_path = Path(data_path)\n",
        "hdf5_files = list(data_path.glob('**/*.hdf5'))\n",
        "\n",
        "if not hdf5_files:\n",
        "    raise RuntimeError(f\"‚ùå Nessun file HDF5 trovato in {data_path}. Scarica prima il dataset LIBERO.\")\n",
        "\n",
        "print(f\"‚úì Trovati {len(hdf5_files)} file HDF5 (task)\")\n",
        "\n",
        "demo_split_ratio = 0.8\n",
        "print(f\"\\nüìä Demo-level split: {demo_split_ratio:.0%} train / {1-demo_split_ratio:.0%} val per ogni task\")\n",
        "print(f\"   Tutti i {len(hdf5_files)} task presenti in entrambi train e val\")\n",
        "\n",
        "print(\"\\nCreating TRAIN dataset...\")\n",
        "train_dataset = LIBERODataset(\n",
        "    hdf5_files,\n",
        "    sequence_length=1,\n",
        "    image_size=(128, 128),\n",
        "    augmentation=False,\n",
        "    max_demos_per_task=50,\n",
        "    demo_split_ratio=demo_split_ratio,\n",
        "    is_train=True\n",
        ")\n",
        "\n",
        "train_action_stats = train_dataset.action_stats\n",
        "\n",
        "print(\"\\nCreating VAL dataset...\")\n",
        "val_dataset = LIBERODataset(\n",
        "    hdf5_files,\n",
        "    sequence_length=1,\n",
        "    image_size=(128, 128),\n",
        "    augmentation=False,\n",
        "    max_demos_per_task=50,\n",
        "    demo_split_ratio=demo_split_ratio,\n",
        "    is_train=False,\n",
        "    action_stats=train_action_stats\n",
        ")\n",
        "\n",
        "num_workers = min(4, os.cpu_count() or 1)\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "loader_common = {\n",
        "    'num_workers': num_workers,\n",
        "    'pin_memory': use_cuda,\n",
        "    'persistent_workers': num_workers > 0\n",
        "}\n",
        "if num_workers > 0:\n",
        "    loader_common['prefetch_factor'] = 2\n",
        "\n",
        "print(f\"\\n‚úì Dataset creati con demo-level split\")\n",
        "print(f\"  Train samples: {len(train_dataset)}\")\n",
        "print(f\"  Val samples: {len(val_dataset)}\")\n",
        "\n",
        "action_stats = train_dataset.action_stats\n",
        "with open('action_stats.json', 'w') as f:\n",
        "    json.dump({\n",
        "        'mean': action_stats['mean'].tolist(),\n",
        "        'std': action_stats['std'].tolist()\n",
        "    }, f)\n",
        "\n",
        "# ========== STEP 2: Hyperparameter Search con Optuna ==========\n",
        "search_trials = 20\n",
        "quick_epochs = 5\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"STEP 2: Optuna search ({search_trials} trials, {quick_epochs} quick epochs)\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "best_config, search_history = optuna_random_search(\n",
        "    train_dataset,\n",
        "    val_dataset,\n",
        "    loader_common,\n",
        "    device,\n",
        "    quick_epochs=quick_epochs,\n",
        "    search_space=default_search_space(),\n",
        "    n_trials=search_trials\n",
        ")\n",
        "\n",
        "with open('hyperparam_results.json', 'w') as f:\n",
        "    json.dump(search_history, f, indent=2)\n",
        "\n",
        "with open('best_hyperparams.json', 'w') as f:\n",
        "    json.dump(best_config.to_dict(), f, indent=2)\n",
        "\n",
        "print(\"\\n‚úÖ Optuna search completata. Miglior configurazione:\")\n",
        "print(best_config.to_dict())\n",
        "\n",
        "# ========== STEP 3: Training con la miglior configurazione ==========\n",
        "final_config = replace(\n",
        "    best_config,\n",
        "    epochs=10,\n",
        "    save_path='optuna_best.pt'\n",
        ")\n",
        "\n",
        "train_loader, val_loader = build_dataloaders(train_dataset, val_dataset, final_config.batch_size, loader_common)\n",
        "model = build_policy_from_config(final_config, obs_shape=(3, 128, 128)).to(device)\n",
        "trainer = BehaviorCloningTrainer(model, train_loader, val_loader, final_config, device, use_wandb=False)\n",
        "\n",
        "print('\\nRunning validation before training...')\n",
        "val_metrics = trainer._validate_epoch(0)\n",
        "print(f\"Initial validation loss: {val_metrics['loss']:.4f}\")\n",
        "\n",
        "print('Starting final training with Optuna config...')\n",
        "best_val = trainer.train()\n",
        "print(f\"Training finished. Best validation loss saved: {best_val:.4f}\")\n",
        "print(f\"Checkpoint saved to: {final_config.save_path}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
