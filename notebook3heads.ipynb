{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T17:37:23.533615Z",
     "iopub.status.busy": "2025-12-04T17:37:23.533003Z",
     "iopub.status.idle": "2025-12-04T17:37:24.355766Z",
     "shell.execute_reply": "2025-12-04T17:37:24.353485Z",
     "shell.execute_reply.started": "2025-12-04T17:37:23.533581Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!rm -r /kaggle/working/LIBERO/libero/datasets/libero_spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from unittest.mock import MagicMock\n",
    "\n",
    "# 1. Fix PyTorch Bugs & Crashers\n",
    "os.environ[\"TORCH_FSDP_NO_WARN_CIRCULAR_DEPENDENCY\"] = \"1\"\n",
    "os.environ[\"TORCH_ALLOW_TF32\"] = \"1\"\n",
    "\n",
    "# 2. Mock Matplotlib (Critical to prevent the 0x10 crash later)\n",
    "mock_mpl = MagicMock()\n",
    "for module in [\"matplotlib\", \"matplotlib.pyplot\", \"matplotlib.cm\", \"matplotlib.colors\", \"matplotlib.transforms\", \"matplotlib.ticker\", \"matplotlib.path\", \"matplotlib._path\"]:\n",
    "    sys.modules[module] = mock_mpl\n",
    "\n",
    "# 3. Clone LIBERO\n",
    "if not os.path.exists(\"LIBERO\"):\n",
    "    print(\"â¬‡ï¸ Cloning LIBERO...\")\n",
    "    !git clone https://github.com/Lifelong-Robot-Learning/LIBERO.git\n",
    "else:\n",
    "    print(\"âœ“ LIBERO already cloned\")\n",
    "\n",
    "# 4. Smart Install (Skips heavy/conflicting libs)\n",
    "print(\"\\nğŸ“¦ Installing LIBERO requirements (Lightweight Mode)...\")\n",
    "# We filter out numpy, torch, etc. to use Kaggle's pre-installed fast versions\n",
    "!grep -v -E \"numpy|matplotlib|tokenizers|wandb|transformers|opencv-python|torch\" /kaggle/working/LIBERO/requirements.txt > /kaggle/working/LIBERO/filtered_requirements.txt\n",
    "!pip install -r /kaggle/working/LIBERO/filtered_requirements.txt --no-deps\n",
    "\n",
    "# 5. Download Dataset (Visible Progress!)\n",
    "# Removed \"> /dev/null\" so you can see it working.\n",
    "print(\"\\nğŸŒ Downloading LIBERO-Spatial (This make take 1-2 mins)...\")\n",
    "!echo n | python /kaggle/working/LIBERO/benchmark_scripts/download_libero_datasets.py --datasets libero_spatial --use-huggingface\n",
    "\n",
    "print(\"\\nâœ… BASE SETUP COMPLETE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T16:03:57.804373Z",
     "iopub.status.busy": "2025-12-04T16:03:57.803791Z",
     "iopub.status.idle": "2025-12-04T16:05:20.679150Z",
     "shell.execute_reply": "2025-12-04T16:05:20.678261Z",
     "shell.execute_reply.started": "2025-12-04T16:03:57.804349Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Installing extra project tools...\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.4.1)\n",
      "Requirement already satisfied: robomimic in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from robomimic) (1.26.4)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from robomimic) (3.14.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from robomimic) (7.1.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from robomimic) (4.67.1)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from robomimic) (3.1.0)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from robomimic) (2.18.0)\n",
      "Collecting tensorboardX (from robomimic)\n",
      "  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (from robomimic) (2.37.0)\n",
      "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.11/dist-packages (from robomimic) (0.6.0)\n",
      "Collecting egl-probe>=1.0.1 (from robomimic)\n",
      "  Downloading egl_probe-1.0.2.tar.gz (217 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m217.5/217.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from robomimic) (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from robomimic) (0.21.0+cu124)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.13.3->robomimic) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.13.3->robomimic) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.13.3->robomimic) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.13.3->robomimic) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.13.3->robomimic) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.13.3->robomimic) (2.4.1)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio->robomimic) (11.3.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->robomimic) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->robomimic) (1.74.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->robomimic) (3.8.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard->robomimic) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard->robomimic) (6.33.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->robomimic) (75.2.0)\n",
      "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard->robomimic) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->robomimic) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard->robomimic) (3.1.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->robomimic) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->robomimic) (4.15.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->robomimic) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->robomimic) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->robomimic) (2025.10.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->robomimic)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->robomimic)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->robomimic)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->robomimic)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->robomimic)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->robomimic)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->robomimic)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->robomimic)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->robomimic)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->robomimic) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->robomimic) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->robomimic) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->robomimic)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->robomimic) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->robomimic) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->robomimic) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard->robomimic) (3.0.3)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.13.3->robomimic) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.13.3->robomimic) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.13.3->robomimic) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.13.3->robomimic) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.13.3->robomimic) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.13.3->robomimic) (2024.2.0)\n",
      "Downloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: egl-probe\n",
      "  Building wheel for egl-probe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for egl-probe: filename=egl_probe-1.0.2-cp311-cp311-linux_x86_64.whl size=478023 sha256=054527b424c4d7d2dd101b813f897756a05f49d2cd0d5efb5b4fa41dcf89b484\n",
      "  Stored in directory: /root/.cache/pip/wheels/fa/fb/31/094ecd1938a964427d319a7960441dce49359675bc09af4615\n",
      "Successfully built egl-probe\n",
      "Installing collected packages: egl-probe, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, tensorboardX\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed egl-probe-1.0.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 tensorboardX-2.6.4\n",
      "Collecting tokenizers==0.15.2\n",
      "  Downloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers==0.15.2) (0.36.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers==0.15.2) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers==0.15.2) (2025.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers==0.15.2) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers==0.15.2) (6.0.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers==0.15.2) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers==0.15.2) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers==0.15.2) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers==0.15.2) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers==0.15.2) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers==0.15.2) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers==0.15.2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers==0.15.2) (2025.10.5)\n",
      "Downloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.2\n",
      "    Uninstalling tokenizers-0.21.2:\n",
      "      Successfully uninstalled tokenizers-0.21.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformers 4.53.3 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.15.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed tokenizers-0.15.2\n",
      "Collecting mujoco==2.3.7\n",
      "  Downloading mujoco-2.3.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mujoco==2.3.7) (1.4.0)\n",
      "Collecting glfw (from mujoco==2.3.7)\n",
      "  Downloading glfw-2.10.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from mujoco==2.3.7) (1.26.4)\n",
      "Requirement already satisfied: pyopengl in /usr/local/lib/python3.11/dist-packages (from mujoco==2.3.7) (3.1.9)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->mujoco==2.3.7) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->mujoco==2.3.7) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->mujoco==2.3.7) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->mujoco==2.3.7) (2025.3.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->mujoco==2.3.7) (2022.3.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->mujoco==2.3.7) (2.4.1)\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->mujoco==2.3.7) (2025.3.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->mujoco==2.3.7) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->mujoco==2.3.7) (2022.3.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->mujoco==2.3.7) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->mujoco==2.3.7) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->mujoco==2.3.7) (2024.2.0)\n",
      "Downloading mujoco-2.3.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading glfw-2.10.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl (243 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m243.5/243.5 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: glfw, mujoco\n",
      "Successfully installed glfw-2.10.0 mujoco-2.3.7\n",
      "âœ… EXTRA TOOLS INSTALLED.\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“¦ Installing extra project tools...\")\n",
    "\n",
    "# 1. Install missing tools\n",
    "!pip install einops robomimic\n",
    "\n",
    "# 2. Fix Tokenizers (Specific version required by some models)\n",
    "!pip install tokenizers==0.15.2\n",
    "\n",
    "# 3. MuJoCo (Only if needed, usually safe)\n",
    "!pip install mujoco==2.3.7\n",
    "\n",
    "# NOTE: We SKIP 'wandb', 'transformers', and 'numba' downgrade because \n",
    "# Kaggle already has better versions installed.\n",
    "\n",
    "print(\"âœ… EXTRA TOOLS INSTALLED.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T16:05:25.779841Z",
     "iopub.status.busy": "2025-12-04T16:05:25.779030Z",
     "iopub.status.idle": "2025-12-04T16:05:31.674300Z",
     "shell.execute_reply": "2025-12-04T16:05:31.673546Z",
     "shell.execute_reply.started": "2025-12-04T16:05:25.779807Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from trm-on-robotic.ipynb\n",
      "PyTorch Version: 2.6.0+cu124\n",
      "âœ… GPU Disponibile: Tesla P100-PCIE-16GB\n",
      "Einops installato correttamente.\n",
      "âœ… Ambiente pronto per il modello TinyRecursive.\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello from trm-on-robotic.ipynb\")\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import h5py\n",
    "import einops\n",
    "from torchvision import models\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torchvision import transforms\n",
    "\n",
    "# Verifica disponibilitÃ  GPU\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ… GPU Disponibile: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"âš ï¸ ATTENZIONE: GPU non rilevata. Vai su 'Settings' > 'Accelerator' e seleziona GPU P100 o T4.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Verifica Einops (lo useremo molto nel modello ricorsivo)\n",
    "print(f\"Einops installato correttamente.\")\n",
    "\n",
    "# Configurazione base per riproducibilitÃ  (importante per la tesi)\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "print(\"âœ… Ambiente pronto per il modello TinyRecursive.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T16:05:31.675671Z",
     "iopub.status.busy": "2025-12-04T16:05:31.675353Z",
     "iopub.status.idle": "2025-12-04T16:05:34.464846Z",
     "shell.execute_reply": "2025-12-04T16:05:34.464051Z",
     "shell.execute_reply.started": "2025-12-04T16:05:31.675655Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import h5py\n",
    "# import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from dataclasses import dataclass, asdict, replace\n",
    "import json\n",
    "import wandb\n",
    "import optuna\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Seed per riproducibilitÃ \n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# =========================\n",
    "# Configuration utilities\n",
    "# =========================\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Container strutturato per gli iperparametri di training/model.\"\"\"\n",
    "\n",
    "    lr: float = 3e-4\n",
    "    hidden_dim: int = 256\n",
    "    num_recursions: int = 8\n",
    "    epochs: int = 20\n",
    "    batch_size: int = 64\n",
    "    weight_decay: float = 1e-4\n",
    "    grad_clip: Optional[float] = 1.0\n",
    "    sched_T0: Optional[int] = None\n",
    "    sched_T_mult: int = 1\n",
    "    lr_min: float = 1e-6\n",
    "    warmup_epochs: int = 3\n",
    "    early_stop_patience: Optional[int] = None\n",
    "    save_path: str = 'best_model.pt'\n",
    "    use_pretrained_encoder: bool = True\n",
    "    freeze_backbone: bool = True\n",
    "    augmentation: bool = False\n",
    "    dropout: float = 0.1\n",
    "    encoder_dropout: float = 0.1\n",
    "\n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        return asdict(self)\n",
    "\n",
    "    def label(self) -> str:\n",
    "        return f\"lr{self.lr}_h{self.hidden_dim}_rec{self.num_recursions}_bs{self.batch_size}\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class HyperparameterSearchSpace:\n",
    "    lr: List[float]\n",
    "    hidden_dim: List[int]\n",
    "    num_recursions: List[int]\n",
    "    batch_size: List[int]\n",
    "    weight_decay: List[float]\n",
    "    pretrained_encoder: List[bool]\n",
    "    freeze_backbone: List[bool]\n",
    "    augmentation: List[bool]\n",
    "    dropout: List[float]\n",
    "\n",
    "    def as_optuna_space(self) -> Dict[str, List[Any]]:\n",
    "        return {\n",
    "            'lr': self.lr,\n",
    "            'hidden_dim': self.hidden_dim,\n",
    "            'num_recursions': self.num_recursions,\n",
    "            'batch_size': self.batch_size,\n",
    "            'weight_decay': self.weight_decay,\n",
    "            'pretrained_encoder': self.pretrained_encoder,\n",
    "            'freeze_backbone': self.freeze_backbone,\n",
    "            'augmentation': self.augmentation,\n",
    "            'dropout': self.dropout,\n",
    "        }\n",
    "\n",
    "# Numero 1\n",
    "def default_search_space() -> HyperparameterSearchSpace:\n",
    "    \"\"\"Restituisce lo spazio di ricerca richiesto dall'utente.\"\"\"\n",
    "\n",
    "    return HyperparameterSearchSpace(\n",
    "        lr=[1e-5, 1e-6, 1e-4, 1e-3],\n",
    "        hidden_dim=[128, 256, 512],\n",
    "        num_recursions=[8, 12, 16],\n",
    "        batch_size=[32, 64, 128, 256, 512],\n",
    "        weight_decay=[0.1, 0.5, 1.0],\n",
    "        pretrained_encoder=[False],\n",
    "        freeze_backbone=[False],\n",
    "        augmentation=[True, False],\n",
    "        dropout=[0.1, 0.3, 0.5, 0.7],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "# --- HELPER FUNCTIONS (Kept largely the same) ---\n",
    "\n",
    "def load_images_robust(dataset):\n",
    "    \"\"\"\n",
    "    Carica immagini da dataset HDF5 usando metodo robusto.\n",
    "    \"\"\"\n",
    "    shape = dataset.shape\n",
    "    \n",
    "    # METODO 1: Lettura diretta uint8\n",
    "    try:\n",
    "        buffer = np.empty(shape, dtype=np.uint8)\n",
    "        dataset.read_direct(buffer)\n",
    "        return buffer\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # METODO 2: Float32 -> Uint8\n",
    "    try:\n",
    "        buffer = np.empty(shape, dtype=np.float32)\n",
    "        dataset.read_direct(buffer)\n",
    "        if buffer.max() <= 1.0:\n",
    "            buffer = (buffer * 255).astype(np.uint8)\n",
    "        else:\n",
    "            buffer = np.clip(buffer, 0, 255).astype(np.uint8)\n",
    "        return buffer\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # METODO 3: Float64 -> Uint8\n",
    "    try:\n",
    "        buffer = np.empty(shape, dtype=np.float64)\n",
    "        dataset.read_direct(buffer)\n",
    "        if buffer.max() <= 1.0:\n",
    "            buffer = (buffer * 255).astype(np.uint8)\n",
    "        else:\n",
    "            buffer = np.clip(buffer, 0, 255).astype(np.uint8)\n",
    "        return buffer\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # METODO 4: Fallback bytes\n",
    "    try:\n",
    "        buffer = np.empty(shape, dtype=np.uint8)\n",
    "        dataset.id.read(h5py.h5s.ALL, h5py.h5s.ALL, buffer)\n",
    "        return buffer\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Impossibile leggere il dataset: {e}\")\n",
    "\n",
    "def load_actions_robust(dataset):\n",
    "    \"\"\"\n",
    "    Carica azioni da dataset HDF5.\n",
    "    \"\"\"\n",
    "    shape = dataset.shape\n",
    "    try:\n",
    "        buffer = np.empty(shape, dtype=np.float32)\n",
    "        dataset.read_direct(buffer)\n",
    "        return buffer\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        buffer = np.empty(shape, dtype=np.float64)\n",
    "        dataset.read_direct(buffer)\n",
    "        return buffer.astype(np.float32)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Impossibile leggere le azioni: {e}\")\n",
    "\n",
    "# --- MODIFIED EXPLORATION FUNCTION (No Matplotlib) ---\n",
    "\n",
    "def explore_libero_dataset(data_path: Path):\n",
    "    \n",
    "    # Trova file\n",
    "    hdf5_files = list(data_path.glob('**/*.hdf5'))\n",
    "    \n",
    "    if not hdf5_files:\n",
    "        print(f\"âš ï¸ Nessun file HDF5 trovato in {data_path}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"âœ… Trovati {len(hdf5_files)} file HDF5\")\n",
    "    \n",
    "    # Analizza il primo file\n",
    "    demo_file = hdf5_files[0]\n",
    "    print(f\"\\nğŸ“„ Analizzando: {demo_file.name}\")\n",
    "    \n",
    "    try:\n",
    "        with h5py.File(demo_file, 'r') as f:\n",
    "            if 'data' not in f:\n",
    "                print(\"âš ï¸ Chiave 'data' non trovata\")\n",
    "                return hdf5_files\n",
    "            \n",
    "            data_group = f['data']\n",
    "            demo_keys = list(data_group.keys())\n",
    "            first_demo_key = demo_keys[0]\n",
    "            demo_0 = data_group[first_demo_key]\n",
    "            \n",
    "            imgs = None\n",
    "            \n",
    "            # 1. Caricamento Immagini\n",
    "            if 'obs' in demo_0:\n",
    "                obs_group = demo_0['obs']\n",
    "                \n",
    "                # Strategia di ricerca chiave immagine\n",
    "                image_keys = ['agentview_rgb', 'agentview_image', 'rgb', 'image', 'robot0_eye_in_hand_image']\n",
    "                img_key = next((k for k in image_keys if k in obs_group), None)\n",
    "                \n",
    "                # Fallback ricerca generica\n",
    "                if img_key is None:\n",
    "                    img_key = next((k for k in obs_group.keys() if 'rgb' in k.lower() or 'image' in k.lower()), None)\n",
    "                \n",
    "                if img_key:\n",
    "                    print(f\"\\nğŸ–¼ï¸ Usando chiave immagini: '{img_key}'\")\n",
    "                    try:\n",
    "                        imgs = load_images_robust(obs_group[img_key])\n",
    "                        print(f\"  âœ… Immagini caricate: {imgs.shape}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"  âŒ Errore immagini: {e}\")\n",
    "            \n",
    "            # 2. Caricamento Azioni\n",
    "            if 'actions' in demo_0:\n",
    "                try:\n",
    "                    actions = load_actions_robust(demo_0['actions'])\n",
    "                    print(f\"\\nğŸ® Azioni caricate: {actions.shape}\")\n",
    "                    print(f\"  Range: [{actions.min():.3f}, {actions.max():.3f}]\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  âŒ Errore azioni: {e}\")\n",
    "\n",
    "            # 3. VISUALIZZAZIONE (Senza Matplotlib)\n",
    "            if imgs is not None and len(imgs) > 0:\n",
    "                print(\"\\nğŸ¬ Visualizzazione frame esempio (PIL/IPython):\")\n",
    "                \n",
    "                num_frames = min(4, len(imgs))\n",
    "                indices = np.linspace(0, len(imgs) - 1, num_frames, dtype=int)\n",
    "                \n",
    "                for idx in indices:\n",
    "                    img_array = imgs[idx]\n",
    "                    \n",
    "                    # Se l'immagine Ã¨ float [0,1], converti a uint8\n",
    "                    if img_array.dtype != np.uint8:\n",
    "                         img_array = (np.clip(img_array, 0, 1) * 255).astype(np.uint8)\n",
    "                    \n",
    "                    # Crea immagine PIL\n",
    "                    pil_img = Image.fromarray(img_array)\n",
    "                    \n",
    "                    # (Opzionale) Resize per non occupare troppo spazio\n",
    "                    # pil_img = pil_img.resize((128, 128))\n",
    "                    \n",
    "                    print(f\"--- Frame {idx} ---\")\n",
    "                    display(pil_img)\n",
    "            else:\n",
    "                print(\"\\nâš ï¸ Nessuna immagine valida da visualizzare\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Errore critico durante l'apertura del file: {e}\")\n",
    "    \n",
    "    return hdf5_files\n",
    "\n",
    "# Esegui\n",
    "hdf5_files = explore_libero_dataset(Path('/kaggle/working/LIBERO/libero/datasets/libero_spatial'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T16:07:14.870406Z",
     "iopub.status.busy": "2025-12-04T16:07:14.869888Z",
     "iopub.status.idle": "2025-12-04T16:07:14.900112Z",
     "shell.execute_reply": "2025-12-04T16:07:14.899334Z",
     "shell.execute_reply.started": "2025-12-04T16:07:14.870379Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LIBERODataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset per dimostrazioni LIBERO\n",
    "    \n",
    "    Carica osservazioni visive e azioni da file HDF5.\n",
    "    Supporta data augmentation e normalizzazione.\n",
    "    Gestisce automaticamente problemi di dtype non standard nei file HDF5.\n",
    "    \n",
    "    Supporta demo-level split: invece di dividere i file, divide le demo\n",
    "    all'interno di ciascun file (es. 80% train, 20% val per ogni file).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        hdf5_files: List[Path],\n",
    "        sequence_length: int = 1,\n",
    "        image_size: Tuple[int, int] = (128, 128),\n",
    "        normalize_actions: bool = True,\n",
    "        augmentation: bool = False,\n",
    "        max_demos_per_task: Optional[int] = None,\n",
    "        demo_split_ratio: float = 0.8,\n",
    "        is_train: bool = True,\n",
    "        action_stats: Optional[Dict] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hdf5_files: lista di path ai file HDF5\n",
    "            sequence_length: lunghezza delle sequenze (1 = single-step prediction)\n",
    "            image_size: dimensioni delle immagini\n",
    "            normalize_actions: se True, normalizza le azioni con z-score\n",
    "            augmentation: se True, applica data augmentation\n",
    "            max_demos_per_task: limite massimo di demo per task (per debugging)\n",
    "            demo_split_ratio: percentuale di demo per training (default 0.8 = 80%)\n",
    "            is_train: se True, usa le prime demo_split_ratio% demo; se False, usa il resto\n",
    "            action_stats: statistiche azioni pre-calcolate (per validation set)\n",
    "        \"\"\"\n",
    "        self.hdf5_files = hdf5_files\n",
    "        self.sequence_length = sequence_length\n",
    "        self.image_size = (int(image_size[0]), int(image_size[1]))\n",
    "        self.augmentation = augmentation and is_train\n",
    "        self.normalize_actions = normalize_actions\n",
    "        self.demo_split_ratio = demo_split_ratio\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        # Carica tutti i dati in memoria (assumendo dataset gestibile)\n",
    "        self.data = []\n",
    "        self.action_stats = action_stats if action_stats is not None else {'mean': None, 'std': None}\n",
    "        self.samples: List[Tuple[int, int]] = []  # (demo_idx, start_idx)\n",
    "        \n",
    "        split_name = \"TRAIN\" if is_train else \"VAL\"\n",
    "        print(f\"Loading {len(hdf5_files)} HDF5 files for {split_name} (demo split: {demo_split_ratio:.0%})...\")\n",
    "        all_actions = []\n",
    "        \n",
    "        for hdf5_file in hdf5_files:\n",
    "            try:\n",
    "                with h5py.File(hdf5_file, 'r') as f:\n",
    "                    if 'data' not in f:\n",
    "                        print(f\"âš ï¸ 'data' key not found in {hdf5_file.name}, skipping...\")\n",
    "                        continue\n",
    "                    \n",
    "                    demo_keys = list(f['data'].keys())\n",
    "                    \n",
    "                    # Limita numero di demo se richiesto\n",
    "                    if max_demos_per_task is not None:\n",
    "                        demo_keys = demo_keys[:max_demos_per_task]\n",
    "                    \n",
    "                    # Demo-level split: seleziona subset di demo in base a is_train\n",
    "                    n_demos = len(demo_keys)\n",
    "                    n_train_demos = int(n_demos * demo_split_ratio)\n",
    "                    \n",
    "                    if is_train:\n",
    "                        # Training: prime n_train_demos demo\n",
    "                        selected_demo_keys = demo_keys[:n_train_demos]\n",
    "                    else:\n",
    "                        # Validation: demo rimanenti\n",
    "                        selected_demo_keys = demo_keys[n_train_demos:]\n",
    "                    \n",
    "                    if len(selected_demo_keys) == 0:\n",
    "                        print(f\"âš ï¸ No demos selected from {hdf5_file.name} for {split_name}, skipping...\")\n",
    "                        continue\n",
    "                    \n",
    "                    for demo_key in selected_demo_keys:\n",
    "                        try:\n",
    "                            demo = f[f'data/{demo_key}']\n",
    "                            \n",
    "                            # Trova la chiave delle immagini\n",
    "                            obs_group = demo['obs']\n",
    "                            img_key = self._find_image_key(obs_group)\n",
    "                            \n",
    "                            if img_key is None:\n",
    "                                print(f\"âš ï¸ No image key found in {hdf5_file.name}/{demo_key}, skipping...\")\n",
    "                                continue\n",
    "                            \n",
    "                            # Carica osservazioni con metodo robusto\n",
    "                            obs = self._load_images_robust(obs_group[img_key])\n",
    "                            \n",
    "                            # Carica azioni con metodo robusto\n",
    "                            actions = self._load_actions_robust(demo['actions'])\n",
    "                            \n",
    "                            # Verifica che obs e actions abbiano lunghezze compatibili\n",
    "                            min_len = min(len(obs), len(actions))\n",
    "                            if min_len < self.sequence_length:\n",
    "                                print(f\"âš ï¸ Demo too short ({min_len} < {self.sequence_length}), skipping...\")\n",
    "                                continue\n",
    "                            \n",
    "                            obs = obs[:min_len]\n",
    "                            actions = actions[:min_len]\n",
    "                            \n",
    "                            # Aggiungi alla lista\n",
    "                            self.data.append({\n",
    "                                'observations': obs,\n",
    "                                'actions': actions\n",
    "                            })\n",
    "                            \n",
    "                            all_actions.append(actions)\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"âš ï¸ Error loading demo {demo_key} from {hdf5_file.name}: {e}\")\n",
    "                            continue\n",
    "                            \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Error opening file {hdf5_file}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"âœ… Loaded {len(self.data)} demonstrations for {split_name}\")\n",
    "        \n",
    "        if len(self.data) == 0:\n",
    "            raise ValueError(f\"No valid demonstrations loaded for {split_name}! Check your data files.\")\n",
    "        \n",
    "        # Calcola statistiche azioni per normalizzazione (solo per training set o se non fornite)\n",
    "        if self.normalize_actions and len(all_actions) > 0 and action_stats is None:\n",
    "            all_actions_concat = np.concatenate(all_actions, axis=0)\n",
    "        \n",
    "            mean = all_actions_concat.mean(axis=0).astype(np.float32)\n",
    "            std  = all_actions_concat.std(axis=0).astype(np.float32)\n",
    "        \n",
    "            # âš ï¸ Floor di sicurezza: evita std troppo piccole che esplodono la normalizzazione\n",
    "            std_clipped = np.clip(std, 0.1, None)\n",
    "        \n",
    "            # Log dettagliato\n",
    "            print(f\"ğŸ“Š Action statistics computed from {split_name} set:\")\n",
    "            print(f\"   Mean:        {np.round(mean, 3)}\")\n",
    "            print(f\"   Std (raw):   {np.round(std, 3)}\")\n",
    "            print(f\"   Std (clipped to >=0.1): {np.round(std_clipped, 3)}\")\n",
    "        \n",
    "            self.action_stats['mean'] = mean\n",
    "            self.action_stats['std']  = std_clipped\n",
    "        \n",
    "        elif action_stats is not None:\n",
    "            print(f\"ğŸ“Š Using provided action statistics\")\n",
    "            self.action_stats = {\n",
    "                'mean': action_stats['mean'].astype(np.float32),\n",
    "                'std':  np.clip(action_stats['std'], 0.1, None).astype(np.float32)\n",
    "            }\n",
    "\n",
    "        # Costruisci indice delle transizioni per accesso O(1)\n",
    "        self.samples = self._build_sample_index()\n",
    "        print(f\"ğŸ“¦ Generated {len(self.samples)} transitions for {split_name}\")\n",
    "\n",
    "    def _find_image_key(self, obs_group) -> Optional[str]:\n",
    "        \"\"\"Trova la chiave corretta per le immagini nel gruppo obs\"\"\"\n",
    "        # Lista di possibili chiavi per le immagini (in ordine di prioritÃ )\n",
    "        possible_keys = [\n",
    "            'agentview_rgb',\n",
    "            'agentview_image', \n",
    "            'rgb',\n",
    "            'image',\n",
    "            'robot0_eye_in_hand_image',\n",
    "            'frontview_image',\n",
    "            'sideview_image'\n",
    "        ]\n",
    "        \n",
    "        obs_keys = list(obs_group.keys())\n",
    "        \n",
    "        # Prima cerca chiavi note\n",
    "        for key in possible_keys:\n",
    "            if key in obs_keys:\n",
    "                return key\n",
    "        \n",
    "        # Poi cerca qualsiasi chiave che contiene 'rgb' o 'image'\n",
    "        for key in obs_keys:\n",
    "            if 'rgb' in key.lower() or 'image' in key.lower():\n",
    "                return key\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _load_images_robust(self, dataset) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Carica immagini da dataset HDF5 usando metodo robusto che bypassa problemi dtype.\n",
    "        \"\"\"\n",
    "        shape = dataset.shape\n",
    "        \n",
    "        # METODO 1: Prova lettura diretta come uint8\n",
    "        try:\n",
    "            buffer = np.empty(shape, dtype=np.uint8)\n",
    "            dataset.read_direct(buffer)\n",
    "            return buffer\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        # METODO 2: Prova come float32 e converti\n",
    "        try:\n",
    "            buffer = np.empty(shape, dtype=np.float32)\n",
    "            dataset.read_direct(buffer)\n",
    "            if buffer.max() <= 1.0:\n",
    "                buffer = (buffer * 255).astype(np.uint8)\n",
    "            else:\n",
    "                buffer = np.clip(buffer, 0, 255).astype(np.uint8)\n",
    "            return buffer\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        # METODO 3: Prova come float64\n",
    "        try:\n",
    "            buffer = np.empty(shape, dtype=np.float64)\n",
    "            dataset.read_direct(buffer)\n",
    "            if buffer.max() <= 1.0:\n",
    "                buffer = (buffer * 255).astype(np.uint8)\n",
    "            else:\n",
    "                buffer = np.clip(buffer, 0, 255).astype(np.uint8)\n",
    "            return buffer\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        # METODO 4: Lettura raw\n",
    "        try:\n",
    "            buffer = np.empty(shape, dtype=np.uint8)\n",
    "            dataset.id.read(h5py.h5s.ALL, h5py.h5s.ALL, buffer)\n",
    "            return buffer\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Cannot read image dataset: {e}\")\n",
    "    \n",
    "    def _load_actions_robust(self, dataset) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Carica azioni da dataset HDF5 usando metodo robusto.\n",
    "        \"\"\"\n",
    "        shape = dataset.shape\n",
    "        \n",
    "        # METODO 1: Prova lettura diretta come float32\n",
    "        try:\n",
    "            buffer = np.empty(shape, dtype=np.float32)\n",
    "            dataset.read_direct(buffer)\n",
    "            return buffer\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        # METODO 2: Prova come float64 e converti\n",
    "        try:\n",
    "            buffer = np.empty(shape, dtype=np.float64)\n",
    "            dataset.read_direct(buffer)\n",
    "            return buffer.astype(np.float32)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Cannot read actions dataset: {e}\")\n",
    "    \n",
    "    def _build_sample_index(self) -> List[Tuple[int, int]]:\n",
    "        \"\"\"Pre-calcola gli indici (demo_idx, start_idx) per ogni transizione\"\"\"\n",
    "        indices: List[Tuple[int, int]] = []\n",
    "        for demo_idx, demo in enumerate(self.data):\n",
    "            demo_transitions = len(demo['observations']) - self.sequence_length + 1\n",
    "            if demo_transitions <= 0:\n",
    "                continue\n",
    "            indices.extend((demo_idx, start) for start in range(demo_transitions))\n",
    "        if not indices:\n",
    "            raise ValueError(\"Dataset index is empty after preprocessing\")\n",
    "        return indices\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Numero totale di transizioni disponibili\"\"\"\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Restituisce una transizione (osservazione, azione).\n",
    "        \n",
    "        Args:\n",
    "            idx: indice della transizione\n",
    "            \n",
    "        Returns:\n",
    "            Dict con 'observations' e 'actions' come tensori\n",
    "        \"\"\"\n",
    "        demo_idx, start_idx = self.samples[idx]\n",
    "        demo = self.data[demo_idx]\n",
    "        end_idx = start_idx + self.sequence_length\n",
    "\n",
    "        obs = demo['observations'][start_idx:end_idx].copy()\n",
    "        actions = demo['actions'][start_idx:end_idx].copy()\n",
    "\n",
    "        # Preprocessing\n",
    "        obs = self._preprocess_obs(obs)\n",
    "        actions = self._preprocess_actions(actions)\n",
    "\n",
    "        # Per single-step prediction, restituisci solo primo elemento\n",
    "        if self.sequence_length == 1:\n",
    "            obs = obs[0]\n",
    "            actions = actions[0]\n",
    "\n",
    "        # Converti HWC -> CHW per PyTorch\n",
    "        if obs.ndim == 3:  # Single image: (H, W, C) -> (C, H, W)\n",
    "            obs = np.transpose(obs, (2, 0, 1))\n",
    "        elif obs.ndim == 4:  # Sequence: (T, H, W, C) -> (T, C, H, W)\n",
    "            obs = np.transpose(obs, (0, 3, 1, 2))\n",
    "\n",
    "        return {\n",
    "            'observations': torch.from_numpy(obs).float(),\n",
    "            'actions': torch.from_numpy(actions).float()\n",
    "        }\n",
    "    \n",
    "    def _preprocess_obs(self, obs: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Preprocessing delle osservazioni\"\"\"\n",
    "        processed = []\n",
    "        target_h, target_w = self.image_size\n",
    "        for img in obs:\n",
    "            if img.shape[0] != target_h or img.shape[1] != target_w:\n",
    "                img = cv2.resize(img, (target_w, target_h), interpolation=cv2.INTER_AREA)\n",
    "            processed.append(img)\n",
    "        obs = np.stack(processed, axis=0)\n",
    "\n",
    "        # Normalizza [0, 255] -> [0, 1]\n",
    "        obs = obs.astype(np.float32) / 255.0\n",
    "\n",
    "        # Data augmentation (se abilitato)\n",
    "        if self.augmentation:\n",
    "            obs = self._augment_obs(obs)\n",
    "        \n",
    "        return obs\n",
    "    def _augment_obs(self, obs: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Data augmentation per osservazioni\"\"\"\n",
    "        # Color jitter (brightness)\n",
    "        if np.random.rand() < 0.5:\n",
    "            brightness = np.random.uniform(0.8, 1.2)\n",
    "            obs = np.clip(obs * brightness, 0, 1)\n",
    "        \n",
    "        # Contrast adjustment\n",
    "        if np.random.rand() < 0.3:\n",
    "            contrast = np.random.uniform(0.8, 1.2)\n",
    "            mean = obs.mean(axis=(1, 2), keepdims=True)\n",
    "            obs = np.clip((obs - mean) * contrast + mean, 0, 1)\n",
    "        \n",
    "        # Random crop (con padding)\n",
    "        if np.random.rand() < 0.3:\n",
    "            crop_ratio = np.random.uniform(0.85, 0.95)\n",
    "            crop_size_h = int(self.image_size[0] * crop_ratio)\n",
    "            crop_size_w = int(self.image_size[1] * crop_ratio)\n",
    "            \n",
    "            start_y = np.random.randint(0, self.image_size[0] - crop_size_h + 1)\n",
    "            start_x = np.random.randint(0, self.image_size[1] - crop_size_w + 1)\n",
    "            \n",
    "            cropped = []\n",
    "            for img in obs:\n",
    "                img_crop = img[start_y:start_y+crop_size_h, start_x:start_x+crop_size_w]\n",
    "                img_resized = cv2.resize(img_crop, (self.image_size[1], self.image_size[0]))\n",
    "                cropped.append(img_resized)\n",
    "            obs = np.stack(cropped)\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def _preprocess_actions(self, actions: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Preprocessing delle azioni con normalizzazione z-score\"\"\"\n",
    "        actions = actions.astype(np.float32)\n",
    "        if self.action_stats['mean'] is not None:\n",
    "            actions = (actions - self.action_stats['mean']) / self.action_stats['std']\n",
    "        \n",
    "        return actions\n",
    "    \n",
    "    def get_action_stats(self) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Restituisce le statistiche delle azioni per denormalizzazione\"\"\"\n",
    "        return self.action_stats.copy()\n",
    "    \n",
    "    def denormalize_actions(self, actions: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Denormalizza le azioni per l'esecuzione nel simulatore\"\"\"\n",
    "        if self.action_stats['mean'] is not None:\n",
    "            return actions * self.action_stats['std'] + self.action_stats['mean']\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T16:07:11.562076Z",
     "iopub.status.busy": "2025-12-04T16:07:11.561565Z",
     "iopub.status.idle": "2025-12-04T16:07:11.582248Z",
     "shell.execute_reply": "2025-12-04T16:07:11.581489Z",
     "shell.execute_reply.started": "2025-12-04T16:07:11.562050Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PretrainedVisualEncoder(nn.Module):\n",
    "    \"\"\"Visual encoder basato su ResNet18 con testa adattiva.\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim: int = 256, freeze_backbone: bool = True, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        resnet = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "        if freeze_backbone:\n",
    "            for param in resnet.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "\n",
    "        self.adapter = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, hidden_dim)\n",
    "        )\n",
    "        self.ln = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
    "        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.adapter.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.float()\n",
    "\n",
    "        if x.shape[-1] != 224 or x.shape[-2] != 224:\n",
    "            x = nn.functional.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "\n",
    "        x = (x - self.mean) / self.std\n",
    "        features = self.backbone(x).flatten(start_dim=1)\n",
    "        output = self.adapter(features)\n",
    "        return self.ln(output)\n",
    "\n",
    "\n",
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self, obs_shape: Tuple[int, int, int] = (3, 128, 128), hidden_dim: int = 256, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        c, _, _ = obs_shape\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(c, 32, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 256, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv(x)\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "class RecursiveBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Blocco ricorsivo del TRM.\n",
    "    Implementa il ragionamento iterativo con self-attention e MLP.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim=256, num_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Self-attention per ragionamento\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            hidden_dim,\n",
    "            num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        # MLP per trasformazione\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim * 4, hidden_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "    \n",
    "    def forward(self, h, x_cond):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h: (B, D) hidden state corrente\n",
    "            x_cond: (B, D) conditioning dall'input\n",
    "        Returns:\n",
    "            (B, D) nuovo hidden state\n",
    "        \"\"\"\n",
    "        # Combina hidden state e conditioning\n",
    "        combined = h + x_cond\n",
    "        combined = combined.unsqueeze(1)  # (B, 1, D) per attention\n",
    "        \n",
    "        # Self-attention con residual\n",
    "        attn_out, _ = self.attention(combined, combined, combined)\n",
    "        combined = self.norm1(combined + self.dropout1(attn_out))\n",
    "        \n",
    "        # MLP con residual\n",
    "        mlp_out = self.mlp(combined)\n",
    "        h_new = self.norm2(combined + mlp_out)\n",
    "        \n",
    "        return h_new.squeeze(1)  # (B, D)\n",
    "\n",
    "\n",
    "class TRMPolicy(nn.Module):\n",
    "    \"\"\"Policy con heads separati e component attention\"\"\"\n",
    "    def __init__(self, obs_shape=(3, 128, 128), action_dim=7, hidden_dim=256, num_heads=4, num_recursions=8, dropout=0.1, use_pretrained_encoder=True, freeze_backbone=True, encoder_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_recursions = num_recursions\n",
    "        self.obs_shape = obs_shape\n",
    "        self.use_pretrained_encoder = use_pretrained_encoder\n",
    "        # Encoder\n",
    "        if use_pretrained_encoder:\n",
    "            self.encoder = PretrainedVisualEncoder(hidden_dim=hidden_dim, freeze_backbone=freeze_backbone, dropout=encoder_dropout)\n",
    "        else:\n",
    "            self.encoder = VisualEncoder(obs_shape=obs_shape, hidden_dim=hidden_dim, dropout=encoder_dropout)\n",
    "        # Recursive block\n",
    "        self.recursive_block = RecursiveBlock(hidden_dim, num_heads, dropout)\n",
    "        # Component attention\n",
    "        self.component_attention = nn.MultiheadAttention(hidden_dim, num_heads, batch_first=True)\n",
    "        # Heads separati\n",
    "        self.head_pos = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, 32),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(32, 3)\n",
    "        )\n",
    "        self.head_rot = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, 32),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(32, 3)\n",
    "        )\n",
    "        self.head_grip = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, 16),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "        # Xavier uniform init\n",
    "        for m in [self.head_pos, self.head_rot, self.head_grip]:\n",
    "            for layer in m:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(layer.weight)\n",
    "                    nn.init.zeros_(layer.bias)\n",
    "    def forward(self, obs):\n",
    "        x_cond = self.encoder(obs)\n",
    "        h = x_cond.clone()\n",
    "        for _ in range(self.num_recursions):\n",
    "            h = self.recursive_block(h, x_cond)\n",
    "        # Component attention: stack and attend\n",
    "        h_stack = torch.stack([h, h, h], dim=1)  # (B, 3, D)\n",
    "        h_attn, _ = self.component_attention(h_stack, h_stack, h_stack)\n",
    "        # Heads\n",
    "        pos = self.head_pos(h_attn[:,0])\n",
    "        rot = self.head_rot(h_attn[:,1])\n",
    "        grip = self.head_grip(h_attn[:,2])\n",
    "        return torch.cat([pos, rot, grip], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T17:11:46.305196Z",
     "iopub.status.busy": "2025-12-04T17:11:46.304921Z",
     "iopub.status.idle": "2025-12-04T17:11:46.338336Z",
     "shell.execute_reply": "2025-12-04T17:11:46.337570Z",
     "shell.execute_reply.started": "2025-12-04T17:11:46.305176Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_policy_from_config(config: TrainingConfig, obs_shape: Tuple[int, int, int] = (3, 128, 128)) -> TRMPolicy:\n",
    "    \"\"\"Costruisce una TRMPolicy coerente con il TrainingConfig.\"\"\"\n",
    "\n",
    "    return TRMPolicy(\n",
    "        obs_shape=obs_shape,\n",
    "        action_dim=7,\n",
    "        hidden_dim=config.hidden_dim,\n",
    "        num_recursions=config.num_recursions,\n",
    "        dropout=config.dropout,\n",
    "        use_pretrained_encoder=config.use_pretrained_encoder,\n",
    "        freeze_backbone=config.freeze_backbone,\n",
    "        encoder_dropout=config.encoder_dropout\n",
    "    )\n",
    "\n",
    "class BehaviorCloningTrainer:\n",
    "    \"\"\"Trainer per Behavior Cloning con configurazione strutturata.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        config: TrainingConfig,\n",
    "        device: torch.device,\n",
    "        use_wandb: bool = False\n",
    "    ):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        self.use_wandb = use_wandb\n",
    "        self.steps_per_epoch = max(len(train_loader), 1)\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=config.lr,\n",
    "            weight_decay=config.weight_decay,\n",
    "            amsgrad=True\n",
    "        )\n",
    "        self.scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            self.optimizer,\n",
    "            max_lr=config.lr,\n",
    "            steps_per_epoch=self.steps_per_epoch,\n",
    "            epochs=config.epochs\n",
    "        )\n",
    "        self.use_amp = (self.device.type == 'cuda')\n",
    "        self.scaler = torch.cuda.amp.GradScaler(enabled=self.use_amp)\n",
    "        self.grad_clip = config.grad_clip\n",
    "        self.early_stop_patience = config.early_stop_patience\n",
    "        self._epochs_no_improve = 0\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_model_path = config.save_path\n",
    "    def train(self):\n",
    "        \"\"\"Training loop completo\"\"\"\n",
    "        \n",
    "        for epoch in range(self.config.epochs):\n",
    "            # Training\n",
    "            train_metrics = self._train_epoch(epoch)\n",
    "            \n",
    "            # Validation\n",
    "            val_metrics = self._validate_epoch(epoch)\n",
    "            \n",
    "            # Logging exaustive\n",
    "            self._log_metrics(epoch, train_metrics, val_metrics)\n",
    "\n",
    "            # print(f\"Epoch: {epoch}\\n\\t Training: {train_metrics}\\n\\t Validation: {val_metrics}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_metrics['loss'] < self.best_val_loss:\n",
    "                self.best_val_loss = val_metrics['loss']\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'val_loss': val_metrics['loss'],\n",
    "                    'config': self.config.to_dict()\n",
    "                }, self.best_model_path)\n",
    "                print(f\"  âœ“ Saved best model (val_loss: {val_metrics['loss']:.4f}) epoch: {epoch}\")\n",
    "                self._epochs_no_improve = 0\n",
    "            else:\n",
    "                self._epochs_no_improve += 1\n",
    "\n",
    "            if self.early_stop_patience and self._epochs_no_improve >= self.early_stop_patience:\n",
    "                print(\"â¹ï¸  Early stopping triggered\")\n",
    "                break\n",
    "        \n",
    "        print(f\"\\nâœ… Training completed! Best val loss: {self.best_val_loss:.4f}\")\n",
    "        return self.best_val_loss\n",
    "    def _train_epoch(self, epoch):\n",
    "        \"\"\"Training per una epoch\"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        total_loss = 0\n",
    "        pos_loss = 0\n",
    "        rot_loss = 0\n",
    "        grip_loss = 0\n",
    "        smooth_loss = 0\n",
    "        for step, batch in enumerate(self.train_loader):\n",
    "            obs = batch['observations'].to(self.device, non_blocking=True)\n",
    "            target_actions = batch['actions'].to(self.device, non_blocking=True)\n",
    "            self.optimizer.zero_grad(set_to_none=True)\n",
    "            with torch.cuda.amp.autocast(enabled=self.use_amp):\n",
    "                pred_actions = self.model(obs)\n",
    "                # Split components\n",
    "                pos_pred, rot_pred, grip_pred = pred_actions[:,:3], pred_actions[:,3:6], pred_actions[:,6:7]\n",
    "                pos_true, rot_true, grip_true = target_actions[:,:3], target_actions[:,3:6], target_actions[:,6:7]\n",
    "                # Huber loss\n",
    "                pos_l = F.huber_loss(pos_pred, pos_true, delta=1.0)\n",
    "                rot_l = F.huber_loss(rot_pred, rot_true, delta=1.0)\n",
    "                grip_l = F.huber_loss(grip_pred, grip_true, delta=1.0)\n",
    "                # Smoothness regularization\n",
    "                if step > 0:\n",
    "                    smooth_l = F.mse_loss(pred_actions, prev_pred)\n",
    "                else:\n",
    "                    smooth_l = torch.tensor(0.0, device=self.device)\n",
    "                prev_pred = pred_actions.detach()\n",
    "                # Weighted sum\n",
    "                loss = 2.0*pos_l + 1.5*rot_l + 1.0*grip_l + 0.1*smooth_l\n",
    "            if self.use_amp:\n",
    "                self.scaler.scale(loss).backward()\n",
    "                if self.grad_clip:\n",
    "                    self.scaler.unscale_(self.optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                if self.grad_clip:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)\n",
    "                self.optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            pos_loss += pos_l.item()\n",
    "            rot_loss += rot_l.item()\n",
    "            grip_loss += grip_l.item()\n",
    "            smooth_loss += smooth_l.item()\n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step()\n",
    "        n_batches = len(self.train_loader)\n",
    "        return {\n",
    "            'loss': total_loss / n_batches,\n",
    "            'pos_loss': pos_loss / n_batches,\n",
    "            'rot_loss': rot_loss / n_batches,\n",
    "            'grip_loss': grip_loss / n_batches,\n",
    "            'smooth_loss': smooth_loss / n_batches\n",
    "        }\n",
    "    def _validate_epoch(self, epoch):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        pos_loss = 0\n",
    "        rot_loss = 0\n",
    "        grip_loss = 0\n",
    "        smooth_loss = 0\n",
    "        prev_pred = None\n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                obs = batch['observations'].to(self.device)\n",
    "                target_actions = batch['actions'].to(self.device)\n",
    "                pred_actions = self.model(obs)\n",
    "                pos_pred, rot_pred, grip_pred = pred_actions[:,:3], pred_actions[:,3:6], pred_actions[:,6:7]\n",
    "                pos_true, rot_true, grip_true = target_actions[:,:3], target_actions[:,3:6], target_actions[:,6:7]\n",
    "                pos_l = F.huber_loss(pos_pred, pos_true, delta=1.0)\n",
    "                rot_l = F.huber_loss(rot_pred, rot_true, delta=1.0)\n",
    "                grip_l = F.huber_loss(grip_pred, grip_true, delta=1.0)\n",
    "                # --- FIX: Only compute smooth_l if shapes match ---\n",
    "                if prev_pred is not None and prev_pred.shape == pred_actions.shape:\n",
    "                    smooth_l = F.mse_loss(pred_actions, prev_pred)\n",
    "                else:\n",
    "                    smooth_l = torch.tensor(0.0, device=self.device)\n",
    "                prev_pred = pred_actions\n",
    "                loss = 2.0*pos_l + 1.0*rot_l + 1.0*grip_l\n",
    "                total_loss += loss.item()\n",
    "                pos_loss += pos_l.item()\n",
    "                rot_loss += rot_l.item()\n",
    "                grip_loss += grip_l.item()\n",
    "                smooth_loss += smooth_l.item()\n",
    "        n_batches = len(self.val_loader)\n",
    "        return {\n",
    "            'loss': total_loss / n_batches,\n",
    "            'pos_loss': pos_loss / n_batches,\n",
    "            'rot_loss': rot_loss / n_batches,\n",
    "            'grip_loss': grip_loss / n_batches,\n",
    "            'smooth_loss': smooth_loss / n_batches\n",
    "        }\n",
    "    def _log_metrics(self, epoch, train_metrics, val_metrics):\n",
    "        lr = self.optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch}, train loss {train_metrics['loss']}, val loss {val_metrics['loss']}\")\n",
    "        print(f\"  [pos] train: {train_metrics['pos_loss']:.4f} val: {val_metrics['pos_loss']:.4f}\")\n",
    "        print(f\"  [rot] train: {train_metrics['rot_loss']:.4f} val: {val_metrics['rot_loss']:.4f}\")\n",
    "        print(f\"  [grip] train: {train_metrics['grip_loss']:.4f} val: {val_metrics['grip_loss']:.4f}\")\n",
    "        print(f\"  [smooth] train: {train_metrics['smooth_loss']:.4f} val: {val_metrics['smooth_loss']:.4f}\")\n",
    "        if self.use_wandb:\n",
    "            wandb.log({\n",
    "                'epoch': epoch,\n",
    "                'train/loss': train_metrics['loss'],\n",
    "                'train/pos_loss': train_metrics['pos_loss'],\n",
    "                'train/rot_loss': train_metrics['rot_loss'],\n",
    "                'train/grip_loss': train_metrics['grip_loss'],\n",
    "                'train/smooth_loss': train_metrics['smooth_loss'],\n",
    "                'val/loss': val_metrics['loss'],\n",
    "                'val/pos_loss': val_metrics['pos_loss'],\n",
    "                'val/rot_loss': val_metrics['rot_loss'],\n",
    "                'val/grip_loss': val_metrics['grip_loss'],\n",
    "                'val/smooth_loss': val_metrics['smooth_loss'],\n",
    "                'lr': lr\n",
    "            })\n",
    "\n",
    "def build_dataloaders(\n",
    "    train_dataset: Dataset,\n",
    "    val_dataset: Dataset,\n",
    "    batch_size: int,\n",
    "    loader_kwargs: Dict[str, Any]\n",
    ") -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"Crea data loader consistenti a partire da un template di kwargs.\"\"\"\n",
    "\n",
    "    kwargs = loader_kwargs.copy()\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def _set_dataset_augmentation(dataset, flag: bool):\n",
    "    \"\"\"Imposta augmentation temporaneamente e restituisce funzione di restore.\"\"\"\n",
    "\n",
    "    if not hasattr(dataset, 'augmentation'):\n",
    "        return lambda: None\n",
    "\n",
    "    original = dataset.augmentation\n",
    "    dataset.augmentation = flag\n",
    "\n",
    "    def restore():\n",
    "        dataset.augmentation = original\n",
    "\n",
    "    return restore\n",
    "\n",
    "\n",
    "def optuna_random_search(\n",
    "    train_dataset: Dataset,\n",
    "    val_dataset: Dataset,\n",
    "    loader_kwargs: Dict[str, Any],\n",
    "    device: torch.device,\n",
    "    quick_epochs: int = 10,\n",
    "    search_space: Optional[HyperparameterSearchSpace] = None,\n",
    "    n_trials: int = 50\n",
    ") -> Tuple[TrainingConfig, List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Randomized Optuna search (TPE-based) instead of grid-search.\n",
    "    Runs a fixed number of trials (n_trials).\n",
    "    \"\"\"\n",
    "\n",
    "    search_space = search_space or default_search_space()\n",
    "\n",
    "    # --- NEW: TPE Sampler (recommended over random sampling) ---\n",
    "    sampler = optuna.samplers.TPESampler(seed=42)\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction='minimize',\n",
    "        sampler=sampler\n",
    "    )\n",
    "\n",
    "    print(f\"\\nğŸ” Starting Optuna random search\")\n",
    "    print(f\"  Number of trials: {n_trials}\")\n",
    "    print(f\"  Quick epochs: {quick_epochs}\")\n",
    "\n",
    "    def objective(trial: optuna.Trial) -> float:\n",
    "\n",
    "        # --- Suggest values from the search space ---\n",
    "        trial_config = TrainingConfig(\n",
    "            lr=trial.suggest_categorical('lr', search_space.lr),\n",
    "            hidden_dim=trial.suggest_categorical('hidden_dim', search_space.hidden_dim),\n",
    "            num_recursions=trial.suggest_categorical('num_recursions', search_space.num_recursions),\n",
    "            epochs=quick_epochs,\n",
    "            batch_size=trial.suggest_categorical('batch_size', search_space.batch_size),\n",
    "            weight_decay=trial.suggest_categorical('weight_decay', search_space.weight_decay),\n",
    "            use_pretrained_encoder=trial.suggest_categorical('pretrained_encoder', search_space.pretrained_encoder),\n",
    "            freeze_backbone=trial.suggest_categorical('freeze_backbone', search_space.freeze_backbone),\n",
    "            augmentation=trial.suggest_categorical('augmentation', search_space.augmentation),\n",
    "            dropout=trial.suggest_categorical('dropout', search_space.dropout),\n",
    "            encoder_dropout=trial.suggest_categorical('dropout', search_space.dropout),\n",
    "            grad_clip=1.0,\n",
    "            save_path=f\"optuna_trial_{trial.number}.pt\"\n",
    "        )\n",
    "\n",
    "        # Build dataloaders based on this trial's batch size\n",
    "        train_loader, val_loader = build_dataloaders(\n",
    "            train_dataset,\n",
    "            val_dataset,\n",
    "            trial_config.batch_size,\n",
    "            loader_kwargs\n",
    "        )\n",
    "\n",
    "        # Build model\n",
    "        model = build_policy_from_config(trial_config)\n",
    "        trainer = BehaviorCloningTrainer(\n",
    "            model,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            trial_config,\n",
    "            device,\n",
    "            use_wandb=False\n",
    "        )\n",
    "\n",
    "        # Turn augmentation on/off safely\n",
    "        restore_aug = _set_dataset_augmentation(train_dataset, trial_config.augmentation)\n",
    "\n",
    "        try:\n",
    "            val_loss = trainer.train()\n",
    "        finally:\n",
    "            restore_aug()\n",
    "\n",
    "        # Store config inside trial for later retrieval\n",
    "        trial.set_user_attr('config', trial_config.to_dict())\n",
    "\n",
    "        return val_loss\n",
    "\n",
    "    # --- NEW: Run fixed-size random search ---\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    # Save trial history (full information)\n",
    "    history = []\n",
    "    for trial in study.trials:\n",
    "        history.append({\n",
    "            'trial_number': trial.number,\n",
    "            'state': str(trial.state),\n",
    "            'value': trial.value,\n",
    "            'params': trial.params,                # all parameters sampled\n",
    "            'user_attrs': trial.user_attrs,        # user metadata (our config)\n",
    "            'distributions': {\n",
    "                k: str(v) for k, v in trial.distributions.items()\n",
    "            },                                      # searchable distributions\n",
    "            'system_attrs': trial.system_attrs,    # optuna internal info\n",
    "            'datetime_start': str(trial.datetime_start),\n",
    "            'datetime_complete': str(trial.datetime_complete),\n",
    "        })\n",
    "\n",
    "\n",
    "    # Best configuration\n",
    "    best_config = TrainingConfig(**study.best_trial.user_attrs['config'])\n",
    "\n",
    "    return best_config, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T16:07:26.303303Z",
     "iopub.status.busy": "2025-12-04T16:07:26.302620Z",
     "iopub.status.idle": "2025-12-04T16:07:26.321026Z",
     "shell.execute_reply": "2025-12-04T16:07:26.320441Z",
     "shell.execute_reply.started": "2025-12-04T16:07:26.303275Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_final_model(\n",
    "    base_config: TrainingConfig,\n",
    "    train_dataset: Dataset,\n",
    "    val_dataset: Dataset,\n",
    "    loader_kwargs: Dict[str, Any],\n",
    "    device,\n",
    "    final_epochs: Optional[int] = None,\n",
    "    use_wandb: bool = False\n",
    ") -> Tuple[nn.Module, float]:\n",
    "    \"\"\"Esegue il training finale a partire dalla configurazione scelta.\"\"\"\n",
    "\n",
    "    final_epochs = final_epochs or base_config.epochs\n",
    "    final_config = replace(\n",
    "        base_config,\n",
    "        epochs=final_epochs,\n",
    "        save_path='final_model.pt',\n",
    "        early_stop_patience=base_config.early_stop_patience or 10,\n",
    "        sched_T0=base_config.sched_T0 or max(1, final_epochs // 5)\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"ğŸ¯ FINAL TRAINING\")\n",
    "    print(f\"Config: {final_config.label()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    if use_wandb:\n",
    "        wandb.init(\n",
    "            project=\"trm-robotics\",\n",
    "            name=f\"final_{final_config.label()}\",\n",
    "            config=final_config.to_dict()\n",
    "        )\n",
    "\n",
    "    train_loader, val_loader = build_dataloaders(\n",
    "        train_dataset,\n",
    "        val_dataset,\n",
    "        final_config.batch_size,\n",
    "        loader_kwargs\n",
    "    )\n",
    "\n",
    "    model = build_policy_from_config(final_config)\n",
    "    trainer = BehaviorCloningTrainer(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        final_config,\n",
    "        device,\n",
    "        use_wandb=use_wandb\n",
    "    )\n",
    "\n",
    "    restore_aug = _set_dataset_augmentation(train_dataset, final_config.augmentation)\n",
    "    try:\n",
    "        final_val_loss = trainer.train()\n",
    "    finally:\n",
    "        restore_aug()\n",
    "\n",
    "    if os.path.exists(final_config.save_path):\n",
    "        checkpoint = torch.load(final_config.save_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    if use_wandb:\n",
    "        wandb.finish()\n",
    "\n",
    "    print(f\"\\nâœ… Final model trained! Val loss: {final_val_loss:.4f}\")\n",
    "\n",
    "    return model, final_val_loss\n",
    "\n",
    "class PolicyEvaluator:\n",
    "    \"\"\"\n",
    "    Valutatore per policy robotiche in simulazione LIBERO\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        policy: nn.Module,\n",
    "        device: torch.device,\n",
    "        action_stats: Dict = None\n",
    "    ):\n",
    "        self.policy = policy.to(device)\n",
    "        self.policy.eval()\n",
    "        self.device = device\n",
    "        self.action_stats = action_stats\n",
    "    \n",
    "    def evaluate_on_task(\n",
    "        self,\n",
    "        env,\n",
    "        init_states,\n",
    "        num_episodes=50,\n",
    "        max_steps=500,\n",
    "        record_video=False,\n",
    "        video_path=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Valuta policy su un singolo task\n",
    "        \n",
    "        Args:\n",
    "            env: environment LIBERO\n",
    "            init_states: stati iniziali per il task\n",
    "            num_episodes: numero di episodi da valutare\n",
    "            max_steps: massimo numero di step per episodio\n",
    "            record_video: se True, registra video\n",
    "            video_path: path per salvare video\n",
    "        \n",
    "        Returns:\n",
    "            results: dizionario con metriche\n",
    "        \"\"\"\n",
    "        successes = []\n",
    "        episode_lengths = []\n",
    "        frames_buffer = [] if record_video else None\n",
    "        \n",
    "        for ep in range(num_episodes):\n",
    "            # Reset environment\n",
    "            env.reset()\n",
    "            env.set_init_state(init_states[ep % len(init_states)])\n",
    "            \n",
    "            obs = env.get_observation()\n",
    "            done = False\n",
    "            step = 0\n",
    "            \n",
    "            episode_frames = [] if record_video and ep < 5 else None\n",
    "            \n",
    "            while not done and step < max_steps:\n",
    "                # Cattura frame\n",
    "                if episode_frames is not None:\n",
    "                    frame = env.render(mode='rgb_array')\n",
    "                    episode_frames.append(frame)\n",
    "                \n",
    "                # Preprocessing osservazione\n",
    "                obs_tensor = self._preprocess_obs(obs)\n",
    "                \n",
    "                # Predici azione\n",
    "                with torch.no_grad():\n",
    "                    action = self.policy(obs_tensor)\n",
    "                    action = action.cpu().numpy().squeeze()\n",
    "                    \n",
    "                    # Denormalizza azione se necessario\n",
    "                    if self.action_stats is not None:\n",
    "                        action = action * self.action_stats['std'] + self.action_stats['mean']\n",
    "                \n",
    "                # Step environment\n",
    "                obs, reward, done, info = env.step(action)\n",
    "                step += 1\n",
    "            \n",
    "            # Registra risultati\n",
    "            success = info.get('success', False)\n",
    "            successes.append(success)\n",
    "            episode_lengths.append(step)\n",
    "            \n",
    "            if episode_frames is not None:\n",
    "                frames_buffer.append(episode_frames)\n",
    "        \n",
    "        # Salva video se richiesto\n",
    "        if record_video and frames_buffer and video_path:\n",
    "            self._save_videos(frames_buffer, video_path)\n",
    "        \n",
    "        # Calcola metriche\n",
    "        results = {\n",
    "            'success_rate': np.mean(successes),\n",
    "            'avg_episode_length': np.mean(episode_lengths),\n",
    "            'std_episode_length': np.std(episode_lengths),\n",
    "            'num_episodes': num_episodes\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _preprocess_obs(self, obs):\n",
    "        \"\"\"Preprocessing osservazione per policy\"\"\"\n",
    "        # Estrai immagine RGB\n",
    "        img = obs['agentview_rgb']\n",
    "        \n",
    "        # Normalizza\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Converti a tensor e aggiungi batch dimension\n",
    "        img = np.transpose(img, (2, 0, 1))  # HWC -> CHW\n",
    "        img_tensor = torch.from_numpy(img).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        return img_tensor\n",
    "    \n",
    "    def _save_videos(self, frames_buffer, video_path):\n",
    "        \"\"\"Salva video degli episodi\"\"\"\n",
    "        os.makedirs(os.path.dirname(video_path), exist_ok=True)\n",
    "        \n",
    "        for i, frames in enumerate(frames_buffer):\n",
    "            path = video_path.replace('.mp4', f'_ep{i}.mp4')\n",
    "            \n",
    "            # Usa OpenCV per salvare video\n",
    "            height, width, _ = frames[0].shape\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            out = cv2.VideoWriter(path, fourcc, 30.0, (width, height))\n",
    "            \n",
    "            for frame in frames:\n",
    "                frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                out.write(frame_bgr)\n",
    "            \n",
    "            out.release()\n",
    "            print(f\"  Video salvato: {path}\")\n",
    "\n",
    "\n",
    "def generate_evaluation_report(results_dict, output_path):\n",
    "    \"\"\"\n",
    "    Genera report HTML con risultati di valutazione\n",
    "    \n",
    "    Args:\n",
    "        results_dict: dizionario {task_name: results}\n",
    "        output_path: path per salvare report\n",
    "    \"\"\"\n",
    "    \n",
    "    html_content = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>TRM Robotics - Evaluation Report</title>\n",
    "        <style>\n",
    "            body {{ font-family: Arial, sans-serif; margin: 40px; }}\n",
    "            h1 {{ color: #333; }}\n",
    "            table {{ border-collapse: collapse; width: 100%; margin-top: 20px; }}\n",
    "            th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}\n",
    "            th {{ background-color: #4CAF50; color: white; }}\n",
    "            tr:nth-child(even) {{ background-color: #f2f2f2; }}\n",
    "            .success {{ color: green; font-weight: bold; }}\n",
    "            .fail {{ color: red; }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>ğŸ¤– TRM Robotics Evaluation Report</h1>\n",
    "        <p><strong>Generated:</strong> {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}</p>\n",
    "        \n",
    "        <h2>Results Summary</h2>\n",
    "        <table>\n",
    "            <tr>\n",
    "                <th>Task Name</th>\n",
    "                <th>Success Rate</th>\n",
    "                <th>Avg Episode Length</th>\n",
    "                <th>Std Episode Length</th>\n",
    "                <th>Num Episodes</th>\n",
    "            </tr>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Aggiungi righe per ogni task\n",
    "    overall_success = []\n",
    "    \n",
    "    for task_name, results in results_dict.items():\n",
    "        success_rate = results['success_rate']\n",
    "        overall_success.append(success_rate)\n",
    "        \n",
    "        success_class = 'success' if success_rate >= 0.5 else 'fail'\n",
    "        \n",
    "        html_content += f\"\"\"\n",
    "            <tr>\n",
    "                <td>{task_name}</td>\n",
    "                <td class=\"{success_class}\">{success_rate:.2%}</td>\n",
    "                <td>{results['avg_episode_length']:.1f}</td>\n",
    "                <td>{results['std_episode_length']:.1f}</td>\n",
    "                <td>{results['num_episodes']}</td>\n",
    "            </tr>\n",
    "        \"\"\"\n",
    "    \n",
    "    # Overall statistics\n",
    "    mean_success = np.mean(overall_success)\n",
    "    success_class = 'success' if mean_success >= 0.5 else 'fail'\n",
    "    \n",
    "    html_content += f\"\"\"\n",
    "            <tr style=\"font-weight: bold; background-color: #e0e0e0;\">\n",
    "                <td>OVERALL</td>\n",
    "                <td class=\"{success_class}\">{mean_success:.2%}</td>\n",
    "                <td colspan=\"3\">-</td>\n",
    "            </tr>\n",
    "        </table>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Salva report\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(html_content)\n",
    "    \n",
    "    print(f\"\\nâœ“ Report salvato: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T16:11:35.826578Z",
     "iopub.status.busy": "2025-12-04T16:11:35.825552Z",
     "iopub.status.idle": "2025-12-04T16:11:35.843034Z",
     "shell.execute_reply": "2025-12-04T16:11:35.842261Z",
     "shell.execute_reply.started": "2025-12-04T16:11:35.826531Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def main_pipeline(\n",
    "    data_path: str = '/kaggle/working/dataset/libero_spatial',\n",
    "    work_dir: str = '/kaggle/working/trm_robotics',\n",
    "    quick_search: bool = True,\n",
    "    train_final: bool = True,\n",
    "    use_custom_final_config: bool = False,\n",
    "    evaluate: bool = True,\n",
    "    use_wandb: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Pipeline completa per il progetto TRM Robotics\n",
    "    \n",
    "    Args:\n",
    "        data_path: path ai dati LIBERO\n",
    "        work_dir: directory di lavoro\n",
    "        quick_search: se True, esegue hyperparameter search\n",
    "        train_final: se True, esegue training finale\n",
    "        evaluate: se True, esegue valutazione in simulazione\n",
    "        use_wandb: se True, usa WandB per logging\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\"\"\n",
    "    {'='*80}\n",
    "    ğŸ¤– TinyRecursiveModels per Controllo Robotico\n",
    "    {'='*80}\n",
    "    \n",
    "    Obiettivi:\n",
    "    1. Adattare architettura TRM per robotica\n",
    "    2. Training con Behavior Cloning su LIBERO\n",
    "    3. Valutazione in simulazione con metriche quantitative e qualitative\n",
    "    \n",
    "    \"\"\")\n",
    "    \n",
    "    # Setup directories\n",
    "    os.makedirs(work_dir, exist_ok=True)\n",
    "    os.chdir(work_dir)\n",
    "    \n",
    "    # Device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"âœ“ Using device: {device}\\n\")\n",
    "    \n",
    "    # ========== STEP 1: Caricamento Dataset ==========\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"STEP 1: Caricamento Dataset\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Trova file HDF5\n",
    "    data_path = Path(data_path)\n",
    "    hdf5_files = list(data_path.glob('**/*.hdf5'))\n",
    "    \n",
    "    if not hdf5_files:\n",
    "        print(f\"âŒ Nessun file HDF5 trovato in {data_path}\")\n",
    "        print(\"Assicurati di aver scaricato il dataset LIBERO!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"âœ“ Trovati {len(hdf5_files)} file HDF5 (task)\")\n",
    "    \n",
    "    # Demo-level split: usa TUTTI i file per entrambi i dataset\n",
    "    # ma dividi le demo all'interno di ciascun file (80% train, 20% val)\n",
    "    demo_split_ratio = 0.8\n",
    "    print(f\"\\nğŸ“Š Demo-level split: {demo_split_ratio:.0%} train / {1-demo_split_ratio:.0%} val per ogni task\")\n",
    "    print(f\"   Tutti i {len(hdf5_files)} task presenti in entrambi train e val\")\n",
    "    \n",
    "    # Crea datasets con demo-level split\n",
    "    print(\"\\nCreating TRAIN dataset...\")\n",
    "    train_dataset = LIBERODataset(\n",
    "        hdf5_files,  # Usa TUTTI i file\n",
    "        sequence_length=1,\n",
    "        image_size=(128, 128),\n",
    "        augmentation=False,\n",
    "        max_demos_per_task=50,  # Limita per velocitÃ \n",
    "        demo_split_ratio=demo_split_ratio,\n",
    "        is_train=True  # Prime 80% demo per task\n",
    "    )\n",
    "    \n",
    "    # Usa le stesse statistiche del training set per la validation\n",
    "    train_action_stats = train_dataset.action_stats\n",
    "    \n",
    "    print(\"\\nCreating VAL dataset...\")\n",
    "    val_dataset = LIBERODataset(\n",
    "        hdf5_files,  # Usa TUTTI i file\n",
    "        sequence_length=1,\n",
    "        image_size=(128, 128),\n",
    "        augmentation=False,\n",
    "        max_demos_per_task=50,  # Stesso limite per coerenza\n",
    "        demo_split_ratio=demo_split_ratio,\n",
    "        is_train=False,  # Ultime 20% demo per task\n",
    "        action_stats=train_action_stats  # Usa statistiche del training\n",
    "    )\n",
    "    \n",
    "    # Data loader template (verranno istanziati on-demand)\n",
    "    num_workers = min(4, os.cpu_count() or 1)\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "\n",
    "    loader_common = {\n",
    "        'num_workers': num_workers,\n",
    "        'pin_memory': use_cuda,\n",
    "        'persistent_workers': num_workers > 0\n",
    "    }\n",
    "    if num_workers > 0:\n",
    "        loader_common['prefetch_factor'] = 2\n",
    "\n",
    "    print(f\"\\nâœ“ Dataset creati con demo-level split\")\n",
    "    print(f\"  Train samples: {len(train_dataset)}\")\n",
    "    print(f\"  Val samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Salva action stats per denormalizzazione\n",
    "    action_stats = train_dataset.action_stats\n",
    "    with open('action_stats.json', 'w') as f:\n",
    "        json.dump({\n",
    "            'mean': action_stats['mean'].tolist(),\n",
    "            'std': action_stats['std'].tolist()\n",
    "        }, f)\n",
    "    \n",
    "    # ========== STEP 2: Hyperparameter Search ==========\n",
    "    best_config = None\n",
    "    \n",
    "    if quick_search:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"STEP 2: Hyperparameter Search\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        best_config, all_results = optuna_random_search(\n",
    "            train_dataset,\n",
    "            val_dataset,\n",
    "            loader_common,\n",
    "            device,\n",
    "            quick_epochs=10\n",
    "        )\n",
    "        \n",
    "        # Salva risultati\n",
    "        with open('hyperparam_results.json', 'w') as f:\n",
    "            json.dump(all_results, f, indent=2)\n",
    "        with open('best_hyperparams.json', 'w') as f:\n",
    "            json.dump(best_config.to_dict(), f, indent=2)\n",
    "    \n",
    "    # ========== STEP 3: Training Finale ==========\n",
    "    trained_model = None\n",
    "    \n",
    "    if train_final:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"STEP 3: Training Finale\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        if best_config is None or use_custom_final_config:\n",
    "            best_config = TrainingConfig(\n",
    "                lr=3e-4,\n",
    "                hidden_dim=256,\n",
    "                num_recursions=8,\n",
    "                batch_size=32,\n",
    "                epochs=100,\n",
    "                weight_decay=1e-4,\n",
    "                dropout=0.3,\n",
    "                encoder_dropout=0.1,\n",
    "                use_pretrained_encoder=True,\n",
    "                freeze_backbone=False,\n",
    "                augmentation=False\n",
    "            )\n",
    "            print(\"âš ï¸  Usando configurazione custom di default\")\n",
    "\n",
    "        trained_model, final_val_loss = train_final_model(\n",
    "            best_config,\n",
    "            train_dataset,\n",
    "            val_dataset,\n",
    "            loader_common,\n",
    "            device,\n",
    "            final_epochs=100,\n",
    "            use_wandb=use_wandb\n",
    "        )\n",
    "    \n",
    "    # ========== STEP 4: Valutazione ==========\n",
    "    if evaluate:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"STEP 4: Valutazione in Simulazione\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Carica modello se non giÃ  trainato\n",
    "        if trained_model is None:\n",
    "            print(\"Caricando modello salvato...\")\n",
    "            checkpoint = torch.load('final_model.pt')\n",
    "            config_dict = checkpoint['config'] if 'config' in checkpoint else {}\n",
    "            # Usa build_policy_from_config per garantire coerenza architetturale\n",
    "            trained_model = build_policy_from_config(\n",
    "                TrainingConfig(**config_dict),\n",
    "                obs_shape=(3, 128, 128)\n",
    "            )\n",
    "            trained_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            trained_model = trained_model.to(device)\n",
    "        \n",
    "        # Crea evaluator\n",
    "        evaluator = PolicyEvaluator(trained_model, device, action_stats)\n",
    "        \n",
    "        # Nota: La valutazione in simulazione richiede setup di LIBERO environment\n",
    "        # Questo Ã¨ un placeholder per la struttura\n",
    "        print(\"\\nâš ï¸  La valutazione in simulazione richiede setup completo di LIBERO\")\n",
    "        print(\"Vedi sezione 'Valutazione in Simulazione' per implementazione completa\")\n",
    "        \n",
    "        # Pseudocodice per valutazione:\n",
    "        \"\"\"\n",
    "        from libero.libero import benchmark\n",
    "        \n",
    "        results_dict = {}\n",
    "        \n",
    "        for task_id in range(num_tasks):\n",
    "            env, init_states = create_eval_env(task_suite, task_id)\n",
    "            \n",
    "            results = evaluator.evaluate_on_task(\n",
    "                env,\n",
    "                init_states,\n",
    "                num_episodes=50,\n",
    "                record_video=True,\n",
    "                video_path=f'videos/task_{task_id}.mp4'\n",
    "            )\n",
    "            \n",
    "            results_dict[f'task_{task_id}'] = results\n",
    "            env.close()\n",
    "        \n",
    "        # Genera report\n",
    "        generate_evaluation_report(results_dict, 'evaluation_report.html')\n",
    "        \"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"âœ… Pipeline completata!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nFile generati in: {work_dir}\")\n",
    "    print(\"  - final_model.pt: modello allenato\")\n",
    "    print(\"  - action_stats.json: statistiche azioni\")\n",
    "    print(\"  - hyperparam_results.json: risultati hyperparameter search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-12-04T15:57:06.002Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     # Configura parametri\n",
    "#     CONFIG = {\n",
    "#         'data_path': '/kaggle/working/dataset/libero_spatial',\n",
    "#         'work_dir': '/kaggle/working/trm_robotics',\n",
    "#         'quick_search': True,\n",
    "#         'train_final': True,\n",
    "#         'use_custom_final_config': True,\n",
    "#         'evaluate': True,  # Richiede setup LIBERO completo\n",
    "#         'use_wandb': False  # Imposta True per logging WandB\n",
    "#     }\n",
    "    \n",
    "#     # Esegui pipeline\n",
    "#     main_pipeline(**CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T18:10:08.355183Z",
     "iopub.status.busy": "2025-12-04T18:10:08.354196Z",
     "iopub.status.idle": "2025-12-04T18:10:45.820721Z",
     "shell.execute_reply": "2025-12-04T18:10:45.817128Z",
     "shell.execute_reply.started": "2025-12-04T18:10:08.355149Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Using device: cuda\n",
      "\n",
      "\n",
      "================================================================================\n",
      "STEP 1: Caricamento Dataset\n",
      "================================================================================\n",
      "âœ“ Trovati 4 file HDF5 (task)\n",
      "Using only the first task: pick_up_the_black_bowl_on_the_wooden_cabinet_and_place_it_on_the_plate_demo.hdf5\n",
      "\n",
      "ğŸ“Š Demo-level split: 80% train / 20% val per ogni task\n",
      "   Tutti i 4 task presenti in entrambi train e val\n",
      "\n",
      "Creating TRAIN dataset...\n",
      "Loading 4 HDF5 files for TRAIN (demo split: 80%)...\n",
      "âœ… Loaded 160 demonstrations for TRAIN\n",
      "ğŸ“Š Action statistics computed from TRAIN set:\n",
      "   Mean:        [ 0.153  0.131 -0.145 -0.005 -0.012 -0.023  0.086]\n",
      "   Std (raw):   [0.449 0.349 0.533 0.041 0.075 0.057 0.996]\n",
      "   Std (clipped to >=0.1): [0.449 0.349 0.533 0.1   0.1   0.1   0.996]\n",
      "ğŸ“¦ Generated 19278 transitions for TRAIN\n",
      "\n",
      "Creating VAL dataset...\n",
      "Loading 4 HDF5 files for VAL (demo split: 80%)...\n",
      "âœ… Loaded 40 demonstrations for VAL\n",
      "ğŸ“Š Using provided action statistics\n",
      "ğŸ“¦ Generated 4778 transitions for VAL\n",
      "\n",
      "âœ“ Dataset creati con demo-level split\n",
      "  Train samples: 19278\n",
      "  Val samples: 4778\n",
      "Running initial validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47/1161071716.py:47: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(enabled=self.use_amp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial validation loss: 2.6588\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47/1161071716.py:102: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=self.use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss 3.455023628870646, val loss 2.762466920049567\n",
      "  [pos] train: 0.8006 val: 0.7363\n",
      "  [rot] train: 0.5275 val: 0.4652\n",
      "  [grip] train: 0.9257 val: 0.8247\n",
      "  [smooth] train: 1.3692 val: 0.5287\n",
      "  âœ“ Saved best model (val_loss: 2.7625) epoch: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_47/3109205968.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Starting training...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m \u001b[0mbest_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training finished. Best validation loss saved: {best_val:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Checkpoint saved to: {config.save_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_47/1161071716.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;31m# Validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_47/1161071716.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpos_l\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrot_l\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mgrip_l\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msmooth_l\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_amp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_clip\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"Setup model with Optuna best params (trial 26), optionally rerun Optuna, then validate and train.\"\"\"\n",
    "import torch\n",
    "from dataclasses import replace\n",
    "\n",
    "data_path =  '/kaggle/working/LIBERO/libero/datasets/libero_spatial'\n",
    "work_dir = '/kaggle/working/trm_robotics'\n",
    "\n",
    "# Optuna best parameters from your message\n",
    "best_params = {\n",
    "    'lr': 1e-6,\n",
    "    'hidden_dim': 128,\n",
    "    'num_recursions': 12,\n",
    "    'batch_size': 256,\n",
    "    'weight_decay': 1.0,\n",
    "    'use_pretrained_encoder': True,\n",
    "    'freeze_backbone': False,\n",
    "    'augmentation': True,\n",
    "    'dropout': 0.3,\n",
    "}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"âœ“ Using device: {device}\\n\")\n",
    "\n",
    "# ========== STEP 1: Caricamento Dataset ==========\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"STEP 1: Caricamento Dataset\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Trova file HDF5\n",
    "data_path = Path(data_path)\n",
    "hdf5_files = list(data_path.glob('**/*.hdf5'))\n",
    "\n",
    "if not hdf5_files:\n",
    "    print(f\"âŒ Nessun file HDF5 trovato in {data_path}\")\n",
    "    print(\"Assicurati di aver scaricato il dataset LIBERO!\")\n",
    "\n",
    "print(f\"âœ“ Trovati {len(hdf5_files)} file HDF5 (task)\")\n",
    "\n",
    "print(f\"Using only the first task: {hdf5_files[0].name}\")\n",
    "\n",
    "# Demo-level split: usa TUTTI i file per entrambi i dataset\n",
    "# ma dividi le demo all'interno di ciascun file (80% train, 20% val)\n",
    "demo_split_ratio = 0.8\n",
    "print(f\"\\nğŸ“Š Demo-level split: {demo_split_ratio:.0%} train / {1-demo_split_ratio:.0%} val per ogni task\")\n",
    "print(f\"   Tutti i {len(hdf5_files)} task presenti in entrambi train e val\")\n",
    "\n",
    "# Crea datasets con demo-level split\n",
    "print(\"\\nCreating TRAIN dataset...\")\n",
    "train_dataset = LIBERODataset(\n",
    "    hdf5_files,  # Usa TUTTI i file\n",
    "    sequence_length=1,\n",
    "    image_size=(128, 128),\n",
    "    augmentation=False,\n",
    "    max_demos_per_task=50,  # Limita per velocitÃ \n",
    "    demo_split_ratio=demo_split_ratio,\n",
    "    is_train=True  # Prime 80% demo per task\n",
    ")\n",
    "\n",
    "# Usa le stesse statistiche del training set per la validation\n",
    "train_action_stats = train_dataset.action_stats\n",
    "\n",
    "print(\"\\nCreating VAL dataset...\")\n",
    "val_dataset = LIBERODataset(\n",
    "    hdf5_files,  # Usa TUTTI i file\n",
    "    sequence_length=1,\n",
    "    image_size=(128, 128),\n",
    "    augmentation=False,\n",
    "    max_demos_per_task=50,  # Stesso limite per coerenza\n",
    "    demo_split_ratio=demo_split_ratio,\n",
    "    is_train=False,  # Ultime 20% demo per task\n",
    "    action_stats=train_action_stats  # Usa statistiche del training\n",
    ")\n",
    "\n",
    "# Data loader template (verranno istanziati on-demand)\n",
    "num_workers = min(4, os.cpu_count() or 1)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "loader_common = {\n",
    "    'num_workers': num_workers,\n",
    "    'pin_memory': use_cuda,\n",
    "    'persistent_workers': num_workers > 0\n",
    "}\n",
    "if num_workers > 0:\n",
    "    loader_common['prefetch_factor'] = 2\n",
    "\n",
    "print(f\"\\nâœ“ Dataset creati con demo-level split\")\n",
    "print(f\"  Train samples: {len(train_dataset)}\")\n",
    "print(f\"  Val samples: {len(val_dataset)}\")\n",
    "\n",
    "# Salva action stats per denormalizzazione\n",
    "action_stats = train_dataset.action_stats\n",
    "with open('action_stats.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'mean': action_stats['mean'].tolist(),\n",
    "        'std': action_stats['std'].tolist()\n",
    "    }, f)\n",
    "\n",
    "# Build TrainingConfig or use fresh Optuna search\n",
    "run_optuna_search = True\n",
    "optuna_trials = 5\n",
    "optuna_epochs = 10\n",
    "\n",
    "if run_optuna_search:\n",
    "    print(\"\\n=== OPTUNA SEARCH (best_params-style) ===\")\n",
    "    best_config, optuna_history = optuna_random_search(\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        loader_kwargs=loader_common,\n",
    "        device=device,\n",
    "        quick_epochs=optuna_epochs,\n",
    "        n_trials=optuna_trials\n",
    "    )\n",
    "    with open('hyperparam_results.json', 'w') as f:\n",
    "        json.dump(optuna_history, f, indent=2)\n",
    "    with open('best_hyperparams.json', 'w') as f:\n",
    "        json.dump(best_config.to_dict(), f, indent=2)\n",
    "    config = replace(\n",
    "        best_config,\n",
    "        epochs=50,\n",
    "        save_path='best_trial_26.pt'\n",
    "    )\n",
    "    print(f\"Using Optuna best config: {config.label()}\")\n",
    "else:\n",
    "    config = TrainingConfig(\n",
    "        lr=best_params['lr'],\n",
    "        hidden_dim=best_params['hidden_dim'],\n",
    "        num_recursions=best_params['num_recursions'],\n",
    "        epochs=50,\n",
    "        batch_size=best_params['batch_size'],\n",
    "        weight_decay=best_params['weight_decay'],\n",
    "        use_pretrained_encoder=best_params['use_pretrained_encoder'],\n",
    "        freeze_backbone=best_params['freeze_backbone'],\n",
    "        augmentation=best_params['augmentation'],\n",
    "        dropout=best_params['dropout'],\n",
    "        encoder_dropout=best_params['dropout'],\n",
    "        save_path='best_trial_26.pt'\n",
    "    )\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Retrieve datasets and loader kwargs from notebook namespace (created earlier)\n",
    "train_dataset = globals().get('train_dataset')\n",
    "val_dataset = globals().get('val_dataset')\n",
    "loader_common = globals().get('loader_common', {'num_workers': 0, 'pin_memory': False, 'persistent_workers': False})\n",
    "\n",
    "if train_dataset is None or val_dataset is None:\n",
    "    raise RuntimeError(\"train_dataset or val_dataset not found. Run the dataset preparation cells before this cell.\")\n",
    "\n",
    "# Build dataloaders, model and trainer\n",
    "train_loader, val_loader = build_dataloaders(train_dataset, val_dataset, config.batch_size, loader_common)\n",
    "model = build_policy_from_config(config, obs_shape=(3, 128, 128)).to(device)\n",
    "trainer = BehaviorCloningTrainer(model, train_loader, val_loader, config, device, use_wandb=False)\n",
    "\n",
    "# Initial validation (before training) and training\n",
    "print('Running initial validation...')\n",
    "val_metrics = trainer._validate_epoch(0)\n",
    "print(f\"Initial validation loss: {val_metrics['loss']:.4f}\")\n",
    "\n",
    "print('Starting training...')\n",
    "best_val = trainer.train()\n",
    "print(f\"Training finished. Best validation loss saved: {best_val:.4f}\")\n",
    "print(f\"Checkpoint saved to: {config.save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weird trick \n",
    "Why this works\n",
    "\n",
    "The sys.modules[\"matplotlib\"] = MagicMock() lines tell Python: \"If anyone asks for matplotlib, just give them this fake empty object.\" When libero tries to import matplotlib.cm, it gets the fake object instead of trying to load the actual file from disk. Since libero likely only uses matplotlib for color maps (which aren't critical for simple RGB evaluation), the code will proceed without crashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-04T18:10:45.822211Z",
     "iopub.status.idle": "2025-12-04T18:10:45.825673Z",
     "shell.execute_reply": "2025-12-04T18:10:45.825472Z",
     "shell.execute_reply.started": "2025-12-04T18:10:45.825451Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from unittest.mock import MagicMock\n",
    "\n",
    "# --- 1. MONKEY PATCH (CRITICAL FIX) ---\n",
    "# We force Python to use a fake 'matplotlib' so it doesn't try to load the broken real one.\n",
    "# This must be done BEFORE importing robosuite or libero.\n",
    "mock_mpl = MagicMock()\n",
    "sys.modules[\"matplotlib\"] = mock_mpl\n",
    "sys.modules[\"matplotlib.pyplot\"] = mock_mpl\n",
    "sys.modules[\"matplotlib.cm\"] = mock_mpl\n",
    "sys.modules[\"matplotlib.colors\"] = mock_mpl\n",
    "sys.modules[\"matplotlib.transforms\"] = mock_mpl\n",
    "sys.modules[\"matplotlib.ticker\"] = mock_mpl\n",
    "sys.modules[\"matplotlib._path\"] = mock_mpl\n",
    "# --------------------------------------\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Callable\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# --- 2. SETUP PATHS ---\n",
    "LIBERO_REPO_ROOT = Path('/kaggle/working/LIBERO')\n",
    "if LIBERO_REPO_ROOT.exists() and str(LIBERO_REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(LIBERO_REPO_ROOT))\n",
    "\n",
    "# Fix for Numba\n",
    "try:\n",
    "    from robosuite.utils.numba import jit_decorator\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Now these imports will work because matplotlib is mocked\n",
    "from libero.libero import get_libero_path\n",
    "from libero.libero.benchmark import get_benchmark\n",
    "from libero.libero.envs import OffScreenRenderEnv\n",
    "from libero.libero.utils.time_utils import Timer\n",
    "from libero.libero.utils.video_utils import VideoWriter\n",
    "\n",
    "# --- 3. SEQUENTIAL VECTOR ENV (Crash-Proof Class) ---\n",
    "class SequentialVectorEnv:\n",
    "    def __init__(self, env_fns: List[Callable]):\n",
    "        self.envs = [fn() for fn in env_fns]\n",
    "\n",
    "    def step(self, actions):\n",
    "        results = [env.step(a) for env, a in zip(self.envs, actions)]\n",
    "        obs_list, rews_list, dones_list, infos_list = zip(*results)\n",
    "        return list(obs_list), np.array(rews_list), np.array(dones_list), list(infos_list)\n",
    "\n",
    "    def reset(self):\n",
    "        return [env.reset() for env in self.envs]\n",
    "\n",
    "    def seed(self, seed):\n",
    "        for i, env in enumerate(self.envs):\n",
    "            if hasattr(env, 'seed'):\n",
    "                env.seed(seed + i)\n",
    "\n",
    "    def set_init_state(self, states):\n",
    "        return [env.set_init_state(s) for env, s in zip(self.envs, states)]\n",
    "\n",
    "    def close(self):\n",
    "        for env in self.envs:\n",
    "            env.close()\n",
    "\n",
    "def _stack_vector_obs(obs: Any) -> Dict[str, np.ndarray]:\n",
    "    if isinstance(obs, list):\n",
    "        keys = obs[0].keys()\n",
    "        return {k: np.stack([o[k] for o in obs], axis=0) for k in keys}\n",
    "    return obs\n",
    "\n",
    "def _select_camera_key(obs_batch: Dict[str, np.ndarray]) -> str:\n",
    "    for key in ('agentview_rgb', 'agentview_image', 'robot0_agentview_image'):\n",
    "        if key in obs_batch: return key\n",
    "    return list(obs_batch.keys())[0]\n",
    "\n",
    "def _prepare_policy_input(images: np.ndarray, device: torch.device) -> torch.Tensor:\n",
    "    imgs = torch.from_numpy(images).to(device=device, dtype=torch.float32) / 255.0\n",
    "    return imgs.permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "# --- 5. EVALUATION FUNCTION ---\n",
    "def evaluate_best_trial_26(\n",
    "    checkpoint_path: str = 'best_trial_26.pt',\n",
    "    action_stats_path: str = 'action_stats.json',\n",
    "    benchmark: str = 'libero_spatial',\n",
    "    task_id: int = 2,  # Fixed to 0 to match training task\n",
    "    env_num: int = 1,\n",
    "    max_steps: int = 300,\n",
    "    seed: int = 42,\n",
    "    save_videos: bool = True,\n",
    "    video_dir: str = 'evaluation_videos',\n",
    "    camera_height: int = 128,\n",
    "    camera_width: int = 128,\n",
    "    video_skip: int = 1\n",
    "):\n",
    "    print(f\"Starting evaluation on {benchmark} Task {task_id} (same as training)...\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load Model\n",
    "    ckpt = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    cfg_dict = ckpt.get('config', {}) or {}\n",
    "    base_cfg = TrainingConfig()\n",
    "    merged_cfg = base_cfg.to_dict()\n",
    "    merged_cfg.update(cfg_dict)\n",
    "    cfg = TrainingConfig(**merged_cfg)\n",
    "\n",
    "    policy = build_policy_from_config(cfg, obs_shape=(3, camera_height, camera_width)).to(device)\n",
    "    policy.load_state_dict(ckpt['model_state_dict'])\n",
    "    policy.eval()\n",
    "\n",
    "    # Load Stats\n",
    "    stats = json.load(open(action_stats_path))\n",
    "    action_mean = torch.tensor(stats['mean'], device=device).unsqueeze(0)\n",
    "    action_std = torch.tensor(stats['std'], device=device).unsqueeze(0)\n",
    "\n",
    "    # Setup Env\n",
    "    benchmark_map = {'libero_10': 'LIBERO_10', 'libero_spatial': 'LIBERO_SPATIAL'}\n",
    "    suite = get_benchmark(benchmark_map.get(benchmark, benchmark))(0)\n",
    "    task = suite.get_task(task_id)\n",
    "    \n",
    "    print(f\"Evaluating task: {task.name} (task_id={task_id})\")\n",
    "    \n",
    "    env_args = {\n",
    "        'bddl_file_name': str(Path(get_libero_path('bddl_files')) / task.problem_folder / task.bddl_file),\n",
    "        'camera_heights': camera_height,\n",
    "        'camera_widths': camera_width\n",
    "    }\n",
    "    \n",
    "    # Initialize SequentialVectorEnv (Mocked matplotlib allows this to load now)\n",
    "    env = SequentialVectorEnv([lambda: OffScreenRenderEnv(**env_args) for _ in range(env_num)])\n",
    "    \n",
    "    try:\n",
    "        init_states = torch.load(str(Path(get_libero_path('init_states')) / task.problem_folder / task.init_states_file), map_location='cpu', weights_only=False)\n",
    "        env.reset()\n",
    "        env.seed(seed)\n",
    "        env.set_init_state(init_states[0:env_num])\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(2): env.step(np.zeros((env_num, action_mean.shape[-1])))\n",
    "\n",
    "        with VideoWriter(video_dir, save_videos) as video_writer:\n",
    "            obs = env.reset()\n",
    "            obs = env.set_init_state(init_states[0:env_num])\n",
    "            dones = [False] * env_num\n",
    "            \n",
    "            for step in range(max_steps):\n",
    "                obs_batch = _stack_vector_obs(obs)\n",
    "                cam_key = _select_camera_key(obs_batch)\n",
    "                \n",
    "                # Inference\n",
    "                p_in = _prepare_policy_input(obs_batch[cam_key], device)\n",
    "                actions = policy(p_in) * action_std + action_mean\n",
    "                actions_np = actions.detach().cpu().numpy()\n",
    "\n",
    "                obs, reward, done, info = env.step(actions_np)\n",
    "\n",
    "                # --- PRINT DEBUG INFO ---\n",
    "                print(f\"\\n[STEP {step}]\")\n",
    "                for env_id, info in enumerate(info_list):\n",
    "                    if \"evaluation_info\" in info:\n",
    "                        eval_info = info[\"evaluation_info\"]\n",
    "                        print(f\"\\nEnv {env_id} â†’ BDDL evaluation:\")\n",
    "                        \n",
    "                        # Print full dict\n",
    "                        print(json.dumps(eval_info, indent=4))\n",
    "\n",
    "                        # Highlight failing conditions\n",
    "                        failed = [k for k, v in eval_info.items() if v is False]\n",
    "                        passed  = [k for k, v in eval_info.items() if v is True]\n",
    "\n",
    "                        print(\"\\n  âœ” Passed conditions:\")\n",
    "                        for k in passed:\n",
    "                            print(f\"     - {k}\")\n",
    "\n",
    "                        print(\"\\n  âŒ Failed conditions:\")\n",
    "                        for k in failed:\n",
    "                            print(f\"     - {k}\")\n",
    "\n",
    "                        # Success indicator\n",
    "                        if done_batch[env_id]:\n",
    "                            print(\"\\n  ğŸ‰ DONE FLAG = TRUE â†’ Task satisfied according to BDDL\")\n",
    "                        else:\n",
    "                            print(\"\\n  âš  DONE FLAG = FALSE â†’ Task NOT satisfied according to BDDL\")\n",
    "\n",
    "                    else:\n",
    "                        print(\"No evaluation_info present in info dict.\")\n",
    "\n",
    "                # Save video\n",
    "                if save_videos and step % video_skip == 0:\n",
    "                    video_writer.append_vector_obs(obs_batch, dones, camera_name=cam_key)\n",
    "\n",
    "                # Break if all done\n",
    "                if all(done_batch):\n",
    "                    print(\"\\nAll environments report done=True. Stopping evaluation.\")\n",
    "                    break\n",
    "\n",
    "                \n",
    "        print(\"Success! Videos saved.\")\n",
    "    finally:\n",
    "        env.close()\n",
    "\n",
    "# Esempio di chiamata:\n",
    "evaluate_trm_libero(\n",
    "    policy=policy,\n",
    "    benchmark=\"libero_spatial\",\n",
    "    task_id=0,\n",
    "    env_num=1,\n",
    "    max_steps=300,\n",
    "    seed=100,\n",
    "    save_videos=True,\n",
    "    video_dir=\"evaluation_videos\",\n",
    "    camera_height=128,\n",
    "    camera_width=128,\n",
    "    video_skip=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T16:54:11.423171Z",
     "iopub.status.busy": "2025-12-04T16:54:11.422876Z",
     "iopub.status.idle": "2025-12-04T17:11:46.302156Z",
     "shell.execute_reply": "2025-12-04T17:11:46.301244Z",
     "shell.execute_reply.started": "2025-12-04T16:54:11.423152Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- EVALUATING FULL SUITE: libero_spatial ---\n",
      "Loading model...\n",
      "[info] using task orders [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Found 10 tasks in benchmark 'LIBERO_SPATIAL'.\n",
      "\n",
      ">>> Starting Task 0: pick_up_the_black_bowl_between_the_plate_and_the_ramekin_and_place_it_on_the_plate\n",
      "Saved videos to evaluation_videos/task_0.\n",
      "    [Task 0] Finished. Success Rate: 0.00 - âŒ FAILURE\n",
      "\n",
      ">>> Starting Task 1: pick_up_the_black_bowl_next_to_the_ramekin_and_place_it_on_the_plate\n",
      "Saved videos to evaluation_videos/task_1.\n",
      "    [Task 1] Finished. Success Rate: 0.00 - âŒ FAILURE\n",
      "\n",
      ">>> Starting Task 2: pick_up_the_black_bowl_from_table_center_and_place_it_on_the_plate\n",
      "Saved videos to evaluation_videos/task_2.\n",
      "    [Task 2] Finished. Success Rate: 0.00 - âŒ FAILURE\n",
      "\n",
      ">>> Starting Task 3: pick_up_the_black_bowl_on_the_cookie_box_and_place_it_on_the_plate\n",
      "Saved videos to evaluation_videos/task_3.\n",
      "    [Task 3] Finished. Success Rate: 0.00 - âŒ FAILURE\n",
      "\n",
      ">>> Starting Task 4: pick_up_the_black_bowl_in_the_top_drawer_of_the_wooden_cabinet_and_place_it_on_the_plate\n",
      "Saved videos to evaluation_videos/task_4.\n",
      "    [Task 4] Finished. Success Rate: 0.00 - âŒ FAILURE\n",
      "\n",
      ">>> Starting Task 5: pick_up_the_black_bowl_on_the_ramekin_and_place_it_on_the_plate\n",
      "Saved videos to evaluation_videos/task_5.\n",
      "    [Task 5] Finished. Success Rate: 0.00 - âŒ FAILURE\n",
      "\n",
      ">>> Starting Task 6: pick_up_the_black_bowl_next_to_the_cookie_box_and_place_it_on_the_plate\n",
      "Saved videos to evaluation_videos/task_6.\n",
      "    [Task 6] Finished. Success Rate: 0.00 - âŒ FAILURE\n",
      "\n",
      ">>> Starting Task 7: pick_up_the_black_bowl_on_the_stove_and_place_it_on_the_plate\n",
      "Saved videos to evaluation_videos/task_7.\n",
      "    [Task 7] Finished. Success Rate: 0.00 - âŒ FAILURE\n",
      "\n",
      ">>> Starting Task 8: pick_up_the_black_bowl_next_to_the_plate_and_place_it_on_the_plate\n",
      "Saved videos to evaluation_videos/task_8.\n",
      "    [Task 8] Finished. Success Rate: 0.00 - âŒ FAILURE\n",
      "\n",
      ">>> Starting Task 9: pick_up_the_black_bowl_on_the_wooden_cabinet_and_place_it_on_the_plate\n",
      "Saved videos to evaluation_videos/task_9.\n",
      "    [Task 9] Finished. Success Rate: 0.00 - âŒ FAILURE\n",
      "\n",
      "==========================================\n",
      "       FINAL EVALUATION REPORT            \n",
      "==========================================\n",
      "Benchmark: libero_spatial\n",
      "Total Tasks: 10\n",
      "Average Success Rate: 0.00\n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "def evaluate_full_suite(\n",
    "    checkpoint_path: str = 'best_trial_26.pt',\n",
    "    action_stats_path: str = 'action_stats.json',\n",
    "    benchmark: str = 'libero_spatial',\n",
    "    env_num: int = 1,\n",
    "    max_steps: int = 300,\n",
    "    seed: int = 42,\n",
    "    save_videos: bool = True,\n",
    "    video_dir: str = 'evaluation_videos',\n",
    "    camera_height: int = 128,\n",
    "    camera_width: int = 128,\n",
    "    video_skip: int = 1\n",
    "):\n",
    "    print(f\"--- EVALUATING FULL SUITE: {benchmark} ---\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # 1. Load Model & Stats\n",
    "    print(\"Loading model...\")\n",
    "    repo_root = Path('/kaggle/working/LIBERO')\n",
    "    if repo_root.exists():\n",
    "        if str(repo_root) not in sys.path:\n",
    "            sys.path.insert(0, str(repo_root))\n",
    "    from libero.libero.benchmark import get_benchmark as _load_benchmark\n",
    "    ckpt = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    cfg_dict = ckpt.get('config', {}) or {}\n",
    "    base_cfg = TrainingConfig()\n",
    "    merged_cfg = base_cfg.to_dict()\n",
    "    merged_cfg.update(cfg_dict)\n",
    "    cfg = TrainingConfig(**merged_cfg)\n",
    "\n",
    "    policy = build_policy_from_config(cfg, obs_shape=(3, camera_height, camera_width)).to(device)\n",
    "    policy.load_state_dict(ckpt['model_state_dict'])\n",
    "    policy.eval()\n",
    "\n",
    "    stats = json.load(open(action_stats_path))\n",
    "    action_mean = torch.tensor(stats['mean'], device=device).unsqueeze(0)\n",
    "    action_std = torch.tensor(stats['std'], device=device).unsqueeze(0)\n",
    "\n",
    "    # 2. Get Benchmark Info\n",
    "    benchmark_map = {'libero_10': 'LIBERO_10', 'libero_spatial': 'LIBERO_SPATIAL'}\n",
    "    suite_name = benchmark_map.get(benchmark, benchmark)\n",
    "    suite = _load_benchmark(suite_name)(0)\n",
    "    num_tasks = suite.n_tasks\n",
    "    print(f\"Found {num_tasks} tasks in benchmark '{suite_name}'.\")\n",
    "    \n",
    "    total_success = 0.0\n",
    "    task_results = {}\n",
    "    \n",
    "    # 3. LOOP THROUGH ALL TASKS\n",
    "    for task_id in range(num_tasks):\n",
    "        task = suite.get_task(task_id)\n",
    "        print(f\"\\n>>> Starting Task {task_id}: {task.name}\")\n",
    "        \n",
    "        env_args = {\n",
    "            'bddl_file_name': str(Path(get_libero_path('bddl_files')) / task.problem_folder / task.bddl_file),\n",
    "            'camera_heights': camera_height,\n",
    "            'camera_widths': camera_width\n",
    "        }\n",
    "\n",
    "        # Initialize Env\n",
    "        env = SequentialVectorEnv([lambda: OffScreenRenderEnv(**env_args) for _ in range(env_num)])\n",
    "        \n",
    "        try:\n",
    "            # Load Init States\n",
    "            init_states_path = Path(get_libero_path('init_states')) / task.problem_folder / task.init_states_file\n",
    "            init_states = torch.load(str(init_states_path), map_location='cpu', weights_only=False)\n",
    "            \n",
    "            # Reset & Warmup\n",
    "            env.reset()\n",
    "            env.seed(seed)\n",
    "            indices = np.arange(env_num) % init_states.shape[0]\n",
    "            env.set_init_state(init_states[indices])\n",
    "            \n",
    "            for _ in range(5): \n",
    "                env.step(np.zeros((env_num, action_mean.shape[-1])))\n",
    "\n",
    "            # --- FIX: Create a unique subfolder for each task ---\n",
    "            # This prevents overwrites without breaking the camera key lookup\n",
    "            task_video_dir = os.path.join(video_dir, f\"task_{task_id}\")\n",
    "            os.makedirs(task_video_dir, exist_ok=True)\n",
    "            \n",
    "            with VideoWriter(task_video_dir, save_videos) as video_writer:\n",
    "                obs = env.reset()\n",
    "                obs = env.set_init_state(init_states[indices])\n",
    "                dones = [False] * env_num\n",
    "                successes = [False] * env_num\n",
    "                \n",
    "                for step in range(max_steps):\n",
    "                    obs_batch = _stack_vector_obs(obs)\n",
    "                    cam_key = _select_camera_key(obs_batch)\n",
    "                    \n",
    "                    # Inference\n",
    "                    p_in = _prepare_policy_input(obs_batch[cam_key], device)\n",
    "                    actions = policy(p_in) * action_std + action_mean\n",
    "                    actions_np = actions.detach().cpu().numpy()\n",
    "                    \n",
    "                    # Step\n",
    "                    obs, _, done_batch, info = env.step(actions_np)\n",
    "                    \n",
    "                    # Track Success\n",
    "                    for i in range(env_num):\n",
    "                        if info[i].get('success', False):\n",
    "                            successes[i] = True\n",
    "                    \n",
    "                    # Save Video\n",
    "                    if save_videos and step % video_skip == 0:\n",
    "                        obs_list = [{key: obs_batch[key][i] for key in obs_batch} for i in range(env_num)]\n",
    "                        video_writer.append_vector_obs(obs_list, dones, camera_name=cam_key)\n",
    "                    \n",
    "                \n",
    "            task_success_rate = np.mean(successes)\n",
    "            total_success += task_success_rate\n",
    "            status = \"âœ… SUCCESS\" if task_success_rate > 0 else \"âŒ FAILURE\"\n",
    "            print(f\"    [Task {task_id}] Finished. Success Rate: {task_success_rate:.2f} - {status}\")\n",
    "            task_results[f\"task_{task_id}\"] = {\n",
    "                \"name\": task.name,\n",
    "                \"success_rate\": float(task_success_rate)\n",
    "            }\n",
    "\n",
    "        finally:\n",
    "            env.close()\n",
    "\n",
    "    # 4. Final Summary\n",
    "    avg_success = total_success / num_tasks\n",
    "    print(f\"\\n==========================================\")\n",
    "    print(f\"       FINAL EVALUATION REPORT            \")\n",
    "    print(f\"==========================================\")\n",
    "    print(f\"Benchmark: {benchmark}\")\n",
    "    print(f\"Total Tasks: {num_tasks}\")\n",
    "    print(f\"Average Success Rate: {avg_success:.2f}\")\n",
    "    print(f\"==========================================\")\n",
    "\n",
    "    return {\n",
    "        \"per_task\": task_results,\n",
    "        \"average_success_rate\": float(avg_success)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run full-suite evaluation and display per-task success rates.\"\"\"\n",
    "results = evaluate_full_suite(\n",
    "    checkpoint_path='best_trial_26.pt',\n",
    "    action_stats_path='action_stats.json',\n",
    "    benchmark='libero_spatial',\n",
    "    env_num=1,\n",
    "    max_steps=300,\n",
    "    seed=42,\n",
    "    save_videos=False,\n",
    "    video_dir='evaluation_videos',\n",
    "    camera_height=128,\n",
    "    camera_width=128,\n",
    "    video_skip=1\n",
    ")\n",
    "\n",
    "print(\"\\nPer-task success rates:\")\n",
    "for task_id, info in results[\"per_task\"].items():\n",
    "    print(f\"- {task_id} ({info['name']}): {info['success_rate']:.2f}\")\n",
    "\n",
    "print(f\"\\nAverage success rate: {results['average_success_rate']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
